# 服务化

## 1.熔断

在进⾏服务化拆分之后，应⽤中原有的本地调⽤就会变成远程调⽤，这样就引⼊了更多的复杂性。⽐如说服务A依赖于服务B，这个过程中可能会出现⽹络抖动、⽹络异常，或者说服务B变得不可⽤或者不好⽤时，也会影响到A的服务性能，甚⾄可能会使得服务A占满整个线程池，导致这个应⽤上其它的服务也受影响，从⽽引发更严重的雪崩效应。

因此，服务之间有这样⼀种依赖关系之后，需要意识到服务的依赖其实是不稳定的。此时，需要通过采取⼀些服务治理的措施，例如熔断、降级、限流、隔离和超时等，来保障应⽤不被外部的异常拖垮。Dubbo提供了降级的特性，⽐如可以通过mock参数来配置⼀些服务的失败降级或者强制降级，但是Dubbo缺少⾃动熔断的特性，所以我们在Dubbo上引⼊了Hystrix。

消费者在进⾏服务调⽤的时候会经过熔断器，当服务提供者出现异常的时候，⽐如暂时性的不可⽤，熔断器就会打开，对消费端进⾏调⽤短路，此时，消费端就不会再发起远程调⽤，⽽是直接⾛向降级逻辑。与此同时，消费端会持续的探测服务的可⽤性，⼀旦服务恢复，熔断器就会关闭，重新恢复调⽤。在Dubbo的服务治理平台上，可以对Hystrix上运⾏的各种动态参数进⾏动态的配置，包括是否允许⾃动熔断，是否要强制熔断，熔断的失败率和时间窗⼝等等。

## 2.限流

当⽤⼾的请求量，调⽤超过系统可承受的并发时系统QPS会降低、出现不可⽤甚⾄存在宕机的⻛险。这就需要⼀个机制来保护我们的系统，当预期并发超过系统可承受的范围时，进⾏快速失败、直接返回，以保护系统。

Dubbo提供了⼀些基础的限流特性，例如可以通过信号量的配置来限制我们消费者的调⽤并发，或者限制提供者的执⾏并发。但是这些是远远不够的，考拉⾃研了限流框架NFC，并基于Dubbofilter的形式，实现了对Dubbo的⽀持，同时也⽀持对URL等其他资源的限流。通过配置中⼼动态获取流控规则，对于资源的请求，⽐如Dubbo调⽤会经过流控客⼾端，进⾏处理并判断是否触发限流，⼀旦请求超出定义的阈值，就会快速失败。

同时，这些限流的结果会上报到监控平台。我们可以对每⼀个资源（URL、Dubbo接⼝）进⾏⼀个阈值的配置，并对限流进⾏准实时监控，包括流控⽐率、限流次数和当前的QPS等。限流框架除了实现基本的并发限流之外，也基于令牌桶和漏桶算法实现了QPS限流，并基于Redis实现了集群级别的限流。这些措施保障系统在⾼流量的情况下不会被打垮。

## 3.监控

在服务化的过程中，系统变得越来越复杂，服务数量变得越来越多，此时需要引⼊更多维度的监控功能，帮助快速的去定位并解决系统中的各类问题。监控主要分为这四个⽅⾯，⽇志、Metrics、Trace和HealthCheck。

在应⽤程序、操作系统运⾏的时候，都会产⽣各种各样的⽇志，通过⽇志平台对这些⽇志进⾏采集、分析和展⽰，并⽀持查询和操作。Metrics反映的是系统运⾏的基本状态，包括瞬时值或者聚合值，例如系统的CPU使⽤率、磁盘使⽤率，以及服务调⽤过程中的平均延时等。Trace是对服务调⽤链的⼀个监控，例如调⽤过程中的耗时分析、瓶颈分析、依赖分析和异常分析等。Healthcheck可以探测应⽤是否准备就绪，是否健康，或者是否还存活。

### 服务监控

Dubbo提供了服务监控功能，⽀持定期上报服务监控数据，通过代码增强的⽅式，采集Dubbo调⽤数据，存储到时序数据库⾥⾯，将Dubbo的调⽤监控功能接⼊到考拉⾃⼰的监控平台。

包括对服务接⼝、源集群等不同维度的监控，除了全局的调⽤监控，还包括不同维度的监控，例如监控项⾥的调⽤次数。有时候我们更关⼼慢请求的情况，所以会将响应时间分为多个范围，⽐如说从0到10毫秒，或是从10到50毫秒等，这样就可以看到在各个范围内请求的数量，从⽽更好地了解服务质量。

同时，也可以通过各种报警规则，对报警进⾏定义，当服务调⽤出现异常时，通过邮件、短信和电话的形式通知相关⼈员。监控平台也会对异常堆栈进⾏采集，例如说这次服务调⽤的异常的原因，是超时还是线程满了的，可以在监控平台上直接看到。同时⽣成⼀些监控报表，帮助我们更好地了解服务的性能，推进开发去改进。

### Trace

我们参考了Dapper，⾃研了Trace平台，并通过代码增强的⽅式，实现了对Dubbo调⽤链路的采集。相关调⽤链参数如TarceID，SpanID等是通过Dubbo的隐式传参来传递的。Trace可以了解在服务调⽤链路中的⼀个耗时分析和瓶颈分析等。Trace平台上可以展⽰⼀次服务调⽤，经历了哪些节点，最耗时的那个节点是在哪⾥，从⽽可以有针对性的去进⾏性能优化。Trace还可以进⾏依赖分析，这些依赖是否合理，能否通过⼀些业务⼿段或者其它⼿段去减少⼀些不合理的依赖。

Trace对异常链路进⾏监控报警，及时的探测到系统异常并帮助我们快速的定位问题，同时和⽇志平台做了打通，通过TraceId可以很快的获取到关联的异常⽇志。

### 健康检查

健康检查也是监控中很重要的⼀个⽅⾯，以更优雅的⽅式上线应⽤实例。我们和⾃动部署平台结合，实现应⽤的健康检查。服务启动的时候可以通过Readiness接⼝判断应⽤依赖的各种资源，包括数据库、消息队列等等是否已经准备就绪。只有健康检查成功的时候才会触发出注册操作。同时Agent也会在程序运⾏的过程中定时的检查服务的运⾏状态。

同时，也通过这些接⼝实现更优雅的停机，仅依赖shutdownhook，在某些情况下不⼀定靠谱，⽐如会有shutdownhook执⾏先后顺序的问题。应⽤发布的时候，⾸先调⽤offline接⼝，将注册服务全部从注册中⼼反注册，这时不再有新的流量进来，等到⼀段时间后，再执⾏停机发布操作，可以实现更加优雅的停机。

## 4.服务测试

服务测试分为接⼝测试、单链路压测、全链路压测和异常测试四个维度。

### 接⼝测试

通过接⼝测试，可以来验证对外提供的Dubbo服务是否正确，因此我们也有接⼝测试平台，帮助QA更好的进⾏接⼝测试，包括对接⼝的编辑（⼊参、出参），⽤例的编辑和测试场景的执⾏等

### 单链路压测

单链路的压测，主要⾯对单个功能的压测，⽐如要上线⼀个重要功能或者⽐较重要的接⼝之前，必须通过性能测试的指标才可以上线。

### 全链路压测

考拉作为电商平台，在⼤促前都会做全链路压测，⽤以探测系统的性能瓶颈，和对系统容量的预估。例如，探测系统的各类服务的容量是否够，需要扩容多少，以及限流的阈值要定多少合适，都可以通过全链路压测来给出⼀些合理的值。

### 异常测试

对服务调⽤链路中的⼀些节点进⾏系统异常和服务异常的注⼊，也可以获取他们的强度依赖关系。⽐如⼀个⾮常重要的接⼝，可以从Trace获取的调⽤链路，然后对调⽤链的依赖的各个服务节点进⾏异常注⼊。通过接⼝的表现，系统就会判断这个接⼝的强度依赖关系，以改善这些不合理的强依赖关系。

## 5.API⽹关

随着服务化的发展，我们⾃研了API⽹关，API⽹关可以作为外部流量的统⼀接⼝，提供了包括路由转发、流控和⽇志监控等⼀些公共的功能。

是通过泛化调⽤的⽅式来调⽤后台Dubbo的服务的。Dubbo原⽣的泛化调⽤的性能⽐普通Api调⽤要差⼀些，所以我们也对泛化调⽤性能做了⼀些优化，也就是去掉了泛化调⽤在返回结果时的⼀次对象转换。最终压测的结果泛化的性能甚⾄⽐正常的调⽤性能还要好些。

## 6.多语⾔⽅⾯

在业务发展的过程中产⽣了不少多语⾔的需求，例如，我们的前端团队希望可以⽤Node应⽤调⽤Dubbo服务。对⽐了易⽤性，选⽤了开源的jsonrpc⽅案，然后在后端的Dubbo服务上暴露了双协议，包括Dubbo协议和jsonrpc协议。

但在实施的过程中，也遇到了⼀些⼩问题，⽐如说，对于Dubbo消费者来说，不管是什么样的协议提供者，都是invoker。通过⼀个负载均衡策略，选取⼀个invoker进⾏调⽤，这个时候就会导致原来的Java客⼾端选⽤⼀个jsonrpc协议的提供者。这样如果他们的API版本不⼀致，就有可能导致序列化异常，出现调⽤失败的情况。所以，我们对Dubbo的⼀些调⽤逻辑做了改造，例如在Java客⼾端的消费者进⾏调⽤的时候，除⾮显⽰的配置，否则默认只⽤Dubbo协议去调⽤。另外，考拉也为社区的jsonrpc扩展了隐式传参的功能，因为可以⽤Dubbo隐式传参的功能来传递⼀些全链路参数。

## 7.解决注册中⼼性能瓶颈

注册中⼼瓶颈可能是⼤部分电商企业都会遇到的问题，我们现在线上的Dubbo服务实例⼤概有4000多个，但是在ZooKeeper中注册的节点有⼀百多万个，包括服务注册的URL和消费者订阅的URL。

Dubbo应⽤发布时的惊群效应、重复通知和消费者拉取带来的瞬时流量⼀下就把ZooKeeper集群的⽹卡打满，ZooKeeper还有另外⼀个问题，他的强⼀致性模型导致CPU的利⽤率不⾼。

就算扩容，也解决不了ZooKeeper写性能的问题，ZooKeeper写是不可扩展的，并且应⽤发布时有⼤量的请求排队，从⽽使得接⼝性能急剧下降，表现出来的现象就是应⽤启动⼗分缓慢。

因此，在今年年初的时候就我们决定把ZooKeeper注册中⼼给替换掉，对⽐了现有的⼀些开源的注册中⼼，包括Consul、Eruka、etcd等，觉得他们并不适合Dubbo这种单进程多服务的注册模型，同时容量能否应对未来考拉的发展，也是⼀个问号。于是，我们决定⾃研注册中⼼，⽬前正在注册中⼼的迁移过程当中，采⽤的是双注册中⼼的迁移⽅案，即服务会同时注册ZooKeeper注册中⼼，还有新的注册中⼼，这样对原有的架构不会产⽣太⼤的影响。

考拉新的注册中⼼改造⽅案和现在社区的差不多，⽐如说也做了⼀个注册数据的拆分，往注册中⼼注册的数据只包含IP,Port等关键数据，其它的数据都写到了Redis⾥⾯，注册中⼼实现使⽤了去中⼼化的⼀个架构，包括使⽤最终⼀致性来换取我们接⼝性能的⼀个提升。后⾯如果接⼊Dubbo，会考虑使⽤Nacos⽽不是ZooKeeper作为注册中⼼