# 分布式系统

## 1.什么是 CAP 定理（CAP theorem）

在理论计算机科学中，CAP 定理（CAP theorem），又被称作布鲁尔定理（Brewer's theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点：

- 一致性（Consistency） （等同于所有节点访问同一份最新的数据副本）
- 可用性（Availability）（每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据）
- 分区容错性（Partition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在 C 和 A 之间做出选择。）

## 2.几个常用的 CAP 框架对比

| 框架      | 所属 |
| --------- | ---- |
| Eureka    | AP   |
| Zookeeper | CP   |
| Consul    | CP   |

Eureka

> Eureka 保证了可用性，实现最终一致性。

Eureka 所有节点都是平等的所有数据都是相同的，且 Eureka 可以相互交叉注册。 Eureka client 使用内置轮询负载均衡器去注册，有一个检测间隔时间，如果在一定时间内没有收到心跳，才会移除该节点注册信息；如果客户端发现当前 Eureka 不可用，会切换到其他的节点，如果所有的 Eureka 都跪了，Eureka client 会使用最后一次数据作为本地缓存；所以以上的每种设计都是他不具备`一致性`的特性。

注意：因为 EurekaAP 的特性和请求间隔同步机制，在服务更新时候一般会手动通过 Eureka 的 api 把当前服务状态设置为`offline`，并等待 2 个同步间隔后重新启动，这样就能保证服务更新节点对整体系统的影响

Zookeeper

> 强一致性

Zookeeper 在选举 leader 时会停止服务，只有成功选举 leader 成功后才能提供服务，选举时间较长；内部使用 paxos 选举投票机制，只有获取半数以上的投票才能成为 leader，否则重新投票，所以部署的时候最好集群节点不小于 3 的奇数个（但是谁能保证跪掉后节点也是基数个呢）；Zookeeper 健康检查一般是使用 tcp 长链接，在内部网络抖动时或者对应节点阻塞时候都会变成不可用，这里还是比较危险的；

Consul

和 Zookeeper 一样数据 CP

Consul 注册时候只有过半的节点都写入成功才认为注册成功；leader 挂掉时，重新选举期间整个 Consul 不可用,保证了强一致性但牺牲了可用性 有很多 blog 说 Consul 属于 ap，官方已经确认他为 CP 机制

## 3.**服务注册中心对比**

### (1)服务注册发现的原理

**Eureka，peer-to-pee**r，部署一个集群，**但是集群里每个机器的地位是对等的，各个服务可以向任何一个Eureka实例服务注册和服务发现，集群里任何一个Euerka实例接收到写请求之后，会自动同步给其他所有的Eureka实例** .

**ZooKeeper，服务注册和发现的原理，Leader + Follower两种角色，只有Leader可以负责写也就是服务注册，他可以把数据同步给Follower，读的时候leader/follower都可以读**.

### (2)一致性保障：CP or AP

**CAP，C是一致性，A是可用性，P是分区容错性**

**CP，AP**

**ZooKeeper是有一个leader节点会接收数据， 然后同步写其他节点，一旦leader挂了，要重新选举leader，这个过程里为了保证C，就牺牲了A，不可用一段时间，但是一个leader选举好了，那么就可以继续写数据了，保证一致性**.

Eureka是peer模式，可能还没同步数据过去，结果自己就死了，此时还是可以继续从别的机器上拉取注册表，但是看到的就不是最新的数据了，但是保证了可用性，强一致，最终一致性.

### (3)服务注册发现的时效性

zk，时效性更好，注册或者是挂了，一般秒级就能感知到.

eureka，默认配置非常糟糕，服务发现感知要到几十秒，甚至分钟级别，上线一个新的服务实例，到其他人可以发现他，极端情况下，可能要1分钟的时间，ribbon去获取每个服务上缓存的eureka的注册表进行负载均衡.

服务故障，隔60秒才去检查心跳，发现这个服务上一次心跳是在60秒之前，隔60秒去检查心跳，超过90秒没有心跳，才会认为他死了，2分钟都过去.30秒，才会更新缓存，30秒，其他服务才会来拉取最新的注册表.三分钟都过去了，如果你的服务实例挂掉了，此时别人感知到，可能要两三分钟的时间，一两分钟的时间，很漫长.

#### (4)容量

zk，不适合大规模的服务实例，因为服务上下线的时候，需要瞬间推送数据通知到所有的其他服务实例，所以一旦服务规模太大，到了几千个服务实例的时候，会导致网络带宽被大量占用.

eureka，也很难支撑大规模的服务实例，因为每个eureka实例都要接受所有的请求，实例多了压力太大，扛不住，也很难到几千服务实例.

## 4.服务发现过慢问题优化

### **zk**

**一般来说还好，服务注册和发现，都是很快的**

### **eureka**

**必须优化参数**,**服务发现的时效性变成秒级，几秒钟可以感知服务的上线和下线**.

#### EurekaServer修改如下配置：

```
eureka:
  server:
    responseCacheUpdateIntervalMs: 3000 #eureka server刷新readCacheMap的时间，注意，client读取的是readCacheMap，这个时间决定了多久会把readWriteCacheMap的缓存更新到readCacheMap上,默认30s
    enable-self-preservation: true # 测试时关闭自我保护机制，保证不可用服务及时踢出【生成环境不要关闭】
    eviction-interval-timer-in-ms: 3000       # 续期时间，即扫描失效服务的间隔时间，清理无效节点的时间间隔，默认60000毫秒，即60秒
```

#### Eureka服务提供方修改如下配置：

```
eureka:
    instance:
        lease-renewal-interval-in-seconds: 1  #每间隔1s，向服务端发送一次心跳，证明自己依然”存活“，默认30秒
        lease-expiration-duration-in-seconds: 2  #告诉服务端，如果我2s之内没有给你发心跳，就代表我“死”了，将我踢出掉。默认为90秒
```

#### Eureka服务调用方修改如下配置：

```
eureka:
    client:
        registryFetchIntervalSeconds: 3  #eureka client刷新本地缓存时间,默认30s
 
ribbon:
  eureka:
    enabled: true
  ReadTimeout: 1000
  ConnectTimeout: 1000
  MaxAutoRetries: 1
  MaxAutoRetriesNextServer: 1
  OkToRetryOnAllOperations: true
  ServerListRefreshInterval: 3000 #eureka客户端ribbon刷新时间,默认30s
```

## 5.网关的核心功能

(1)动态路由：新开发某个服务，动态把请求路径和服务的映射关系热加载到网关里去；服务增减机器，网关自动热感知

(2)灰度发布

(3)授权认证

(4)性能监控：每个API接口的耗时、成功率、QPS

(5)系统日志

(6)数据缓存

(7)限流熔断

## 6.网关技术选型

Zuul：基于Java开发，核心网关功能都比较简单，但是比如灰度发布、限流、动态路由之类的，很多都要自己做二次开发.高并发能力不强，部署到一些机器上去，还要基于Tomcat来部署，Spring Boot用Tomcat把网关系统跑起来；Java语言开发，可以直接把控源码，可以做二次开发封装各种需要的功能.

Kong：依托于Nginx实现，OpenResty，lua实现的模块，现成的一些插件，可以直接使用.

Nginx（Kong、Nginx+Lua）：Nginx抗高并发的能力很强，少数几台机器部署一下，就可以抗很高的并发，精通Nginx源码，很难，c语言，很难说从Nginx内核层面去做一些二次开发和源码定制.

## 7.什么是分布式链路追踪

其实核心架构就是做一个框架，然后每一次服务调用都要经过这个框架，框架采集调用链路的数据存储起来，然后有可视化界面展示出来每个调用链路，性能，故障，这些东西.

追踪了有什么用，调用链路，链路性能监控，链路故障排查.

