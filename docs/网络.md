# 网络

## 1.OSI七层网络模型

一个标准的网络模型出来，大家都按照这个来走，大家都要遵守统一的规范。这就是所谓OSI七层模型，他们分别是：应用层、表示层、会话层、传输层、网络层、数据链路层、物理层。那么在这个基础上，又简化出了TCP/IP四层模型，数据链路层、网络层、传输层、应用层。

1、物理层：物理设备，传输0/1电路信号。

2、数据链路层：解析/组包电路信号，走以太网协议，封装对应带有mac地址的数据包。

3、网络层：通过IP再次对底层网络进行抽象，将网络划分为局域网，广域网等等。

4、传输层：TCP/UDP等协议作用层。

5、应用层：HTTP/STMP等协议作用层。

6、交换机：作用于数据链路层，作用是广播以太网数据包。

7、路由器(网关)：作用于网络层，连接多个子网。

8、ARP协议：作用于数据链路层，主要是通过广播的方式，拿到目标IP的对应mac地址，并缓存一段时间。

9、DNS协议：作用于应用层，主要是通过UDP的方式拿到域名对应的IP地址。

9、整个过程：数据包由应用层对应的协议产生，然后向下逐一封装，TCP报文 -> IP报文 -> 以太网报文。最后由交换机广播在整个子网内，如果目标mac地址不在同一子网内，会由路由器去一次次转发，最后目标子网的网关接收到，发现目标电脑的mac地址在自己的子网内，于是再通过交换机在自己的子网内广播，最终目标电脑接收到报文。

前提：网络中主机与主机之间是通过Mac地址+IP地址+端口号来定位通信目标的

1.物理层：作用是把主机之间连接起来，传输0/1电信号

2.数据链路层：基于以太网协议，对物理层传输的电信号进行分组，每个数据包为一个帧，每个帧包含标头和数据，标头封装了要进行通信的主机的Mac地址。工作在这层的硬件为交换机，作用域在一个局域网内，基于Mac地址进行寻址

3.网络层：通过IP地址+子网掩码确定与目标主机是否在同一个局域网内：在同一个子网内，则通过广播+Mac地址来传输数据包进行网络通信；不在同一个子网内，则通过路到其他子网内继续判断与目标主机是否在同一个局域网 4.传输层：通过socket协议，将数据包加入端口号，实现主机的某个端口号与目标主机的某个端口号之间的点对点通信

5.应用层：HTTP协议、FTP协议等。解析tcp层传输的数据

## 2.浏览器请求[www.baidu.com](www.baidu.com)的全过程大概是怎么样的

1、DNS域名解析，通过UDP广播的方式拿到对应域名的IP。

2、拿到IP以后进行子网判断，如果不在同一个子网内，则向下进行组包。

3、应用层组http的请求报文，运输层组TCP报文，网络层组网络协议报文，数据链路层组以太网协议报文。

4、组好报文后由网关进行路由发出，经过层层的转发，最终到达目标服务器。

5、目标服务器收到对应的报文后，层层解析，最终交付应用层HTTP报文，由对应的服务处理后响应。

6、最终服务器返回的网络资源经过同样的过程到达用户电脑，并由浏览器展示。

发送请求过程：数据包的一层层封装过程

应用层：基于HTTP协议，将请求报文（请求行+请求头+请求体）打包成数据包传送给传输层

传输层：传输层基于tcp协议，将应用层数据包封装到tcp数据包中，加一个tcp头，头部信息存放发送者的端口号（随机选一个）以及接受者的端口号，将tcp数据包传递到网络层

网络层：网络层基于IP协议。将tcp头+tcp数据包封装到IP数据包,加一个IP头，IP头中包含本机和目标主机的IP地址，将IP数据包传递到数据链路层

数据链路层：基于以太网协议，把ip头和ip数据包封到以太网数据包里去，然后再加一个以太网数据包的头，头里放了本机网卡mac地址，和网关的mac地址。但是以太网数据包的限制是1500个字节，需将IP数据包分割成多个数据包，每个数据包包含了以太网头、ip头和切割后的ip数据包。通过ip协议+子网掩码，可以判断两个ip地址不是在一个子网内的，同一子网内则传送数据；不在一个子网内，只能将数据包先通过以太网协议广播到网关上去，通过网关再给他发送出去

返回数据过程：数据包的一层层拆封过程。

## 3.tcp三次握手过程

通过传输层的tcp协议建立网络连接的时候，其实走的是三次握手的过程 。

建立三次握手的时候，TCP报头用到了下面几个东西，ACK、SYN、FIN。

第一次握手，客户端发送连接请求报文，此时SYN=1、ACK=0，这就是说这是个连接请求，seq = x，接着客户端处于SYN_SENT状态，等待服务器响应。

第二次握手，服务端收到SYN=1的请求报文，需要返回一个确认报文，ack = x + 1，SYN=1，ACK = 1，seq = y，发送给客户端，自己处于SYN_RECV状态。

第三次握手，客户端收到了报文，将ack = y + 1，ACK = 1，seq = x + 1

其实三次握手说白了，就是来回来去三次请求，每次请求带上一堆TCP报文头，根据报文头是否正确，就是越好的协议来建立连接。简单说就是这样。

三次握手：保证client-server都可以进行可靠收发，第三次是为了确认服务端的可靠收发。还有一种说法就是如果两次就建立连接的话，会因为网络的原因，有些第一次握手的请求滞后了，导致服务端额外的开销，因此出现这种请求，就需要第三次握手去复位连接，释放掉服务端资源。

## 4.tcp断开连接的4次挥手

第一次挥手，客户端发送报文，FIN=1，seq=u，此时进入FIN-WAIT-1状态

第二次挥手，服务端收到报文，这时候进入CLOSE_WATI状态，返回一个报文，ACK=1，ack=u+1，seq=v。客户端收到这个报文之后，直接进入FIN-WAIT-2状态，此时客户端到服务端的连接就释放了。

第三次挥手，服务端发送连接释放报文，FIN=1，ack=u+1，seq=w，服务端进入LAST-ACK状态

第四次挥手，客户端收到连接释放报文之后，发应答报文，ACK=1，ack=w+1，seq=u+1，进入TIME_WAIT状态，等待一会儿客户端进入CLOSED状态，服务端收到报文之后就进入CLOSED状态。

四次挥手，主要是第二次挥手后服务端依旧可以发送数据，然后client在发出第四次挥手后需要等待2MSL的时间才会进入closed状态。具体原因也是出于可靠传输，一是为了防止第四次挥手的消息超时，2MSL的时间可以处理超时重发。 二是为了防止该client在服务端确认关闭前再次连接上该server，保证该连接被正确关闭。挥手过程耗时相对较长，所以说频繁的创建和关闭TCP连接的开销比较大。

## 5.聊聊HTTPS的工作原理？

https的工作原理大概是这样的：

（1）浏览器把自己支持的加密规则发送给网站 。

（2）网站从这套加密规则里选出来一套加密算法和hash算法，然后把自己的身份信息用证书的方式发回给浏览器，证书里有网站地址、加密公钥、证书颁发机构 。

（3）浏览器验证证书的合法性，然后浏览器地址栏上会出现一把小锁；浏览器接着生成一串随机数密码，然后用证书里的公钥进行加密，这块走的非对称加密；用约定好的hash算法生成握手消息的hash值，然后用密码对消息进行加密，然后把所有东西都发给网站，这块走的是对称加密 。

（4）网站，从消息里面可以取出来公钥加密后的随机密码，用本地的私钥对消息解密取出来密码，然后用密码解密浏览器发来的握手消息，计算消息的hash值，并验证与浏览器发送过来的hash值是否一致，最后用密码加密一段握手消息，发给浏览器。

（5）浏览器解密握手消息，然后计算消息的hash值，如果跟网站发来的hash一样，握手就结束，之后所有的数据都会由之前浏览器生成的随机密码，然后用对称加密来进行进行加密。

常用的非对称加密是RSA算法，对称加密是AES、RC4等，hash算法就是MD5 。

## 6.socket通信的原理？

大体来说这个步骤，就是我们搞一个ServerSocket无限等待别人来连接你，然后某个机器要跟你连接，就在本地创建一个socket去连接你，然后建立连接之后，在服务器上，ServerSocket也会创建出来一个socket的。通过客户端的socket跟服务端的socket进行通信，我给你写数据，你读数据，你给我写数据，我读数据，就这个过程。

当然这个底层，比如建立连接和释放连接，都是基于tcp三次握手和四次挥手的规范来搞的，包括基于tcp协议传输数据，其实就跟我们之前说的一样，都是封装个tcp数据包，里面有tcp报头，整了端口号啥的，然后封装在ip数据包里，最后封在以太网数据包里传递。

## 7.BIO

这个其实就是最传统的网络通信模型，就是BIO，同步阻塞式IO，简单来说大家如果参加过几个月的培训班儿应该都知道这种BIO网络通信方式。就是服务端创建一个ServerSocket，然后客户端用一个Socket去连接那个ServerSocket，然后ServerSocket接收到一个Socket的连接请求就创建一个Socket和一个线程去跟那个Socket进行通信。

然后客户端和服务端的socket，就进行同步阻塞式的通信，客户端socket发送一个请求，服务端socket进行处理后返回响应，响应必须是等处理完后才会返回，在这之前啥事儿也干不了，这可不就是同步么。

这种方式最大的坑在于，每次一个客户端接入，都是要在服务端创建一个线程来服务这个客户端的，这会导致大量的客户端的时候，服务端的线程数量可能达到几千甚至几万，几十万，这会导致服务器端程序的负载过高，最后崩溃死掉。

要么你就是搞一个线程池，固定线程数量来处理请求，但是高并发请求的时候，还是可能会导致各种排队和延时，因为没那么多线程来处理。

## 8.NIO

JDK 1.4中引入了NIO，这是一种同步非阻塞的IO，基于Reactor模型。

NIO中有一些概念：

比如Buffer，缓冲区的概念，一般都是将数据写入Buffer中，然后从Buffer中读取数据，有IntBuffer、LongBuffer、CharBuffer等很多种针对基础数据类型的Buffer。

还有Channel，NIO中都是通过Channel来进行数据读写的。

包括Selector，这是多路复用器，selector会不断轮询注册的channel，如果某个channel上发生了读写事件，selector就会将这些channel获取出来，我们通过SelectionKey获取有读写事件的channel，就可以进行IO操作。一个Selector就通过一个线程，就可以轮询成千上万的channel，这就意味着你的服务端可以接入成千上万的客户端。

这块其实相当于就是一个线程处理大量的客户端的请求，通过一个线程轮询大量的channel，每次就获取一批有事件的channel，然后对每个请求启动一个线程处理即可。

这里的核心就是非阻塞，就那个selector一个线程就可以不停轮询channel，所有客户端请求都不会阻塞，直接就会进来，大不了就是等待一下排着队而已。

这里的核心就是因为，一个客户端不是时时刻刻都要发送请求的，没必要死耗着一个线程不放吧，所以NIO的优化思想就是一个请求一个线程。只有某个客户端发送了一个请求的时候，才会启动一个线程来处理。

所以为啥是非阻塞呢？因为无论多少客户端都可以接入服务端，客户端接入并不会耗费一个线程，只会创建一个连接然后注册到selector上去罢了，一个selector线程不断的轮询所有的socket连接，发现有事件了就通知你，然后你就启动一个线程处理一个请求即可，但是这个处理的过程中，你还是要先读取数据，处理，再返回的，这是个同步的过程。

所以NIO是同步非阻塞的。

## 9.AIO

AIO是基于Proactor模型的，就是异步非阻塞模型。

每个连接发送过来的请求，都会绑定一个buffer，然后通知操作系统去异步完成读，此时你的程序是会去干别的事儿的，等操作系统完成数据读取之后，就会回调你的接口，给你操作系统异步读完的数据。

然后你对这个数据处理一下，接着将结果往回写。

写的时候也是给操作系统一个buffer，让操作系统自己获取数据去完成写操作，写完以后再回来通知你。

工作线程，读取数据的时候，是说，你提供给操作系统一个buffer，空的，然后你就可以干别的事儿了，你就把读数据的事儿，交给操作系统去干，操作系统内核，读数据将数据放入buffer中，完事儿了，来回调你的一个接口，告诉你说，ok，buffer交给你了，这个数据我给你读好了

写数据的时候也是一样的的，把放了数据的buffer交给操作系统的内核去处理，你就可以去干别的事儿了，操作系统完成了数据的写之后，级会来回调你，告诉你说，ok，哥儿们，你交给我的数据，我都给你写回到客户端去了

## 10.同步阻塞、同步非阻塞、异步非阻塞

为啥叫BIO是同步阻塞呢？这个其实不是针对网络编程模型来说的，是针对文件IO操作来说的，因为用BIO的流读写文件，是说你发起个IO请求直接hang死，必须等着搞完了这次IO才能返回 。

BIO的这个同步阻塞，不是完全针对的网络通信模型去说的，针对的是磁盘文件的IO读写，FileInputStream，BIO，卡在那儿，直到你读写完成了才可以。

NIO为啥是同步非阻塞？就是说通过NIO的FileChannel发起个文件IO操作，其实发起之后就返回了，你可以干别的事儿，这就是非阻塞，但是接下来你还得不断的去轮询操作系统，看IO操作完事儿了没有。

AIO为啥是异步非阻塞？就是说通过AIO发起个文件IO操作之后，你立马就返回可以干别的事儿了，接下来你也不用管了，操作系统自己干完了IO之后，告诉你说ok了。同步就是自己还得主动去轮询操作系统，异步就是操作系统反过来通知你。

## 11.http长连接是什么？

http本身没什么所谓的长连接短连接之说，其实说白了都是http下层的tcp连接是长连接还是短连接，tcp连接保持长连接，那么多个http请求和响应都可以通过一个链接来走。其实http 1.1之后，默认都是走长连接了，就是底层都是一个网页一个tcp连接，一个网页的所有图片、css、js的资源加载，都走底层一个tcp连接，来多次http请求即可。 

http 1.0的时候，底层的tcp是短连接，一个网页发起的请求，每个请求都是先tcp三次握手，然后发送请求，获取响应，然后tcp四次挥手断开连接；每个请求，都会先连接再断开。短连接，建立连接之后，发送个请求，直接连接就给断开了.

http 1.1，tcp长连接，tcp三次握手，建立了连接，无论有多少次请求都是走一个tcp连接的，走了n多次请求之后，然后tcp连接被释放掉了.

## 12.synchronized是如何使用内存屏障保证可见性和有序性的？

可见性保障来划分。

内存屏障可分为加载屏障（Load Barrier）和存储屏障（Store   Barrier）。加载屏障的作用是刷新处理器缓存，存储屏障的作用冲刷处理器缓存。Java虚拟机会在 MonitorExit （ 释放锁 ） 对应的机器码指令之后插入一个存储屏障，这就保障了写线程在释放锁之前在临界区中对共享变量所做的更新对读线程的执行处理器来说是可同步的。相应地，Java 虚拟机会在 MonitorEnter （ 申请锁 ） 对应的机器码指令之后临界区开始之前的地方插入一个加载屏障，这使得读线程的执行处理器能够将写线程对相应共享变量所做的更新从其他处理器同步到该处理器的高速缓存中。因此，可见性的保障是通过写线程和读线程成对地使用存储屏障和加载屏障实现的。 按照有序性保障来划分，内存屏障可以分为获取屏障（Acquire Barrier）和释放屏障 （ Release Barrier ）。获 取 屏 障 的 使 用 方 式 是 在 一 个 读 操 作 （ 包括 Read-Modify-Write 以及普通的读操作 ）之后插入该内存屏障，其作用是禁止该读操作与其后的任何读写操作之间进行重排序，这相当于在进行后续操作之前先要获得相应共享数据的所有权 （ 这也是该屏障的名称来源 ）。释放屏障的使用方式是在一个写操作之前插入该内存屏障，其作用是禁止该写操作与其前面的任何读写操作之间进行重排序。这相当于在对相应共享数据操作结束后释放所有权（ 这也是该屏障的名称来源 ）。 Java虚拟机会在 MonitorEnter（ 它包含了读操作 ） 对应的机器码指令之后临界区开始之前的地方插入一个获取屏障，并在临界区结束之后 MonitorExit （ 它包含了写操作 ） 对应的机器码指令之前的地方插入一个释放屏障。因此，这两种屏障就像是三明治的两层面包片把火腿夹住一样把临界区中的代码（指令序列）包括起来 由于获取屏障禁止了临界区中的任何读、写操作被重排序到临界区之前的可能性。而释放屏障又禁止了临界区中的任何读、写操作被重排序到临界区之后的可能性。因此临界区内的任何读、写操作都无法被重排序到临界区之外。在锁的排他性的作用下，这使得临界区中执行的操作序列具有原子性。因此，写线程在临界区中对各个共享变量所做的更新会同时对读线程可见，即在卖线程看来各个共享变量就像是“一下子”.

## 13.java内存模型优化

1)首先硬盘和内存的发展数据远不及cpu的发展速度，而要保证cpu的告诉运行就对cpu增加一个高速缓存。但是增加高速缓存以后这就导致了各个cpu之间内的高速缓存造成数据不一致的现象， 

2)为了解决这种数据不一致的现象就提出了MESI协议，通过修改各个高速缓存中数据的状态来保证数据一致性的问题。M是修改状态、E是独占状态也就是加锁、S是共享状态、I是无效状态。 

3)虽然通过MESI可以保证数据的一直性，但是却会大大的影响cpu的处理速度，因为cpu在修改一个处于S状态的数据时，首先会对总线发出一条invalid的通知，告诉所有其他的cpu(持有该数据)数据失效，然后等到接受到其他cpu的invalid ack消息才会对这条数据进行修改，等待ack这个时候其实是阻塞的，cpu只有等待所有ack返回之后才会执行其他的指令，而相对于cpu修改数据而言，等待ack消息的耗时是特别长的，这就体现出了cpu性能下降这个问题。 

4)为了解决这个问题，就又对cpu内增加写缓存和失效队列这两个概念，cpu要写一个数据的时候首先发送invalid指令，然后把接收ack这个工作交给写缓存器，然后cpu自己去执行其他的指令。其他的cpu收到invalid消息之后直接把invalid消息扔到失效队列中然后返回invalid ack消息，这样cpu就不用因为等待ack指令而降低处理速度。 

5)但是这种情况又会引发可见性和有序性的问题，被扔到写缓存里的数据不会保证什么时候完成，这就可能cpu顺序写入指令1、指令2，将他们扔到写缓存中，但是指令2先执行完成，而其他线程先看到该线程的执行顺序为指令2、指令1，这是有序性的问题。可见性也就不用说了，要修改的数据还在写缓存中等着没执行呢。 

6)为了解决这种可见性和有序性的问题就引入了内存屏障的概念，在修改一个数据之后强制cpu执行完写缓存中关于该变量的指令。在读取一个数据之前强制执行时效队列中对该数据时效的指令，然后从其他高速缓存或者主内存中读取最新数据。

## 14.Java虚拟机对锁的优化

1. 锁消除：JIT编译器通过逃逸分析等技术发现有些被加锁的代码不会出现线程安全问题，那么动态编译的时候就会消除掉这个加锁的操作。（一般是有些框架里面自己加的synchronized而我们作为程序员并不知道，主要是优化这个） 
2. 锁粗化：多个同步块合并在一起去执行。 
3. 偏向锁：偏向于第一个加锁的线程，下一次这个线程再来加锁就不用加锁了，提升性能。但是仅仅适用于非常低的并发场景，因为一旦有第二个线程去尝试加锁，原本偏向的那个线程会被挂起来释放锁，偏向锁也就失效了，升级为轻量级锁。 
4. 轻量级锁：主要是基于对象头里面的mark word进行cas，防止每一次加锁都用到os互斥量的重量级锁。这个也仅仅适用于只有少量并发的情况，因为一旦第二个线程加锁失败，进入自旋，仍然失败，就会升级到重量级锁。 
5. 自旋锁：为了尽量少使用os的互斥量所做的最后努力（如果重量级锁竞争失败了，会进入自适应自旋），如果自旋也失败了，就会被挂起导致上下文切换。 
6. 重量级锁：就是直接使用OS互斥量来进行加锁操作的一种锁，涉及到内核态和用户态的相互转换。

## 15.mmap

把一个磁盘文件映射到内存里来，然后把映射到内存里来的数据通过socket发送出去.

mmap技术，也就是内存映射，直接将磁盘文件数据映射到内核缓冲区，这个映射的过程是基于DMA引擎拷贝的，同时用户缓冲区是跟内核缓冲区共享一块映射数据的，建立共享映射之后，就不需要从内核缓冲区拷贝到用户缓冲区了.

光是这一点，就可以避免一次拷贝了，但是这个过程中还是会用户态切换到内核态去进行映射拷贝，接着再次从内核态切换到用户态，建立用户缓冲区和内核缓冲区的映射.

接着把数据通过Socket发送出去，还是要再次切换到内核态.

接着直接把内核缓冲区里的数据拷贝到Socket缓冲区里去，然后再拷贝到网络协议引擎里，发送出去就可以了，最后切换回用户态

减少一次拷贝，但是并不减少切换次数，一共是4次切换，3次拷贝

mmap技术是主要在RocketMQ里来使用的.

## 16.零拷贝技术到底是什么

linux提供了sendfile，也就是零拷贝技术.

这个零拷贝技术，就是先从用户态切换到内核态，在内核态的状态下，把磁盘上的数据拷贝到内核缓冲区，同时从内核缓冲区拷贝一些offset和length到Socket缓冲区；接着从内核态切换到用户态，从内核缓冲区直接把数据拷贝到网络协议引擎里去.

同时从Socket缓冲区里拷贝一些offset和length到网络协议引擎里去，但是这个offset和length的量很少，几乎可以忽略.

只要2次切换，2次拷贝，就可以了.

kafka、tomcat，都是用的零拷贝技术，rocketmq用的是mmap技术，mmap还是要多2次切换和1次拷贝的.

 

