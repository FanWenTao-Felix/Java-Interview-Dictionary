# 分布式算法

## 1.一致性Hash算法

一致性哈希算法(Consistent Hashing Algorithm)是一种分布式算法，常用于负载均衡。Memcached client 也选择这种算法，解决将key-value 均匀分配到众多Memcached server 上的问题。它可以取代传统的取模操作，解决了取模操作无法应对增删Memcached Server 的问题(增删server 会导致同一个key,在get 操作时分配不到数据真正存储的server，命中率会急剧下降)。

### 一致性 Hash 特性

- 平衡性(Balance)：平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。
- 单调性(Monotonicity)：单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中。哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。容易看到，上面的简单求余算法hash(object)%N 难以满足单调性要求。
- 平滑性(Smoothness)：平滑性是指缓存服务器的数目平滑改变和缓存对象的平滑改变是一致的。

### 一致性Hash的原理

一致性Hash 的原理是将整个Hash 空间虚拟成一个0 ～ 232-1 的Hash 环，然后将数据和服务器分别映射到Hash 环上来实现数据在各个服务器上的Hash 分布。。

具体过程如下

( 1 ）构建环形Hash空间：一致性Hash 将整个Has h 空间组成一个0 ～ 232-1 的虚拟圆环。

( 2 ）将服务器节点映射到Hash环：使用Hash函数将服务器映射到虚拟的Hash环（空间）上，一般可以使用服务器节点机器的IP地址或者机器名作为Hash函数的计算值。

( 3 ）将数据映射到Hash环：使用相同的Hash函数计算需要存储的数据的Hash值，并将数据映射到Hash环上。

( 4 ）将对象映射到服务节点：首先找到对象的Hash值在Hash环上的位置；然后从该位置开始沿Hash环顺时针寻找，遇到的第一台服务器就是该对象的存储节点服务器；最后将该对象映射到该服务器上。

移除 cache：考虑假设 cache B 挂掉了，根据上面讲到的映射方法，这时受影响的将仅是那些沿 cache B 逆时针遍历直到下一个 cache （ cache C ）之间的对象。

添加 cache：再考虑添加一台新的 cache D 的情况，这时受影响的将仅是那些沿 cacheD 逆时针遍历直到下一个 cache 之间的对象，将这些对象重新映射到 cache D 上即可。

### 一致性Hash的节点变动

传统的通过计算Hash值然后除以服务器个数求余的方法带来的最大问题在于不能满足单调性，即当Server有变动（增加节点或移除节点）时，整个系统的Hash值都会失效（因为服务器个数发生了变化，即被除数发生了变化），从而需要重新计算Hash值，并进行Hash映射和数据分布。而一致性Hash在服务器发生变动时，由于对象的数据分布只与顺时针方向的下一个服务器相关，因此只会影响变化节点的下一个节点的数据分布。

( 1 ）移除节点：假设其中某台服务器宕机，受影响的对象仅仅是那些原本映射到服务器上的对象，根据一致性Hash顺时针数据映射的原则，只需要将原本映射到该服务器上的对象重新映射到下一个正常的服务器即可。

( 2 ）添加节点：在添加节点时，受影响的数据将仅是那些沿着逆时针方向从新加入的节点到上一个服务器之间的对象，将这些对象重新映射到新加入的节点即可。

### 虚拟节点（down机多节点托管）

一致Hash算法不能保证数据的绝对平衡，在集群对象数据较少的情况下，对象并不能被均匀地映射到各个Server上。为了解决数据分布不均的问题，一致性Hash引入了“虚拟节点”的概念。虚拟节点（VirtualNode）是实际节点在Hash空间的副本，一个实际节点对应若干虚拟节点，对应个数也被称为副本个数，虚拟节点在Hash空间中以Hash值排列。

在引入虚拟节点后，映射关系就从对象到节点转换为从对象到虚拟节点。

### 一致性Hash的特点

( 1 ）平衡性（Balance）：平衡性指Hash后的结果能够将数据尽可能平均地分配到每一个节点上。

( 2 ）单调性（Monotonicity）：单调性指如果有新的节点加入，不会影响其他节点上原有数据的分配。简单求余算法hash(object)%N难以满足单调性的要求。

( 3 ）平滑性（Smoothness）：平滑性指节点数的平滑改变和缓存对象的平滑改变保持步调一致。

## 2.Paxos

Paxos 算法解决的问题是一个分布式系统如何就某个值（决议）达成一致。一个典型的场景是，在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点执行相同的操作序列，那么他们最后能得到一个一致的状态。为保证每个节点执行相同的命令序列，需要在每一条指令上执行一个“一致性算法”以保证每个节点看到的指令一致。zookeeper 使用的zab 算法是该算法的一个实现。 在Paxos 算法中，有三种角色：Proposer，Acceptor，Learners

Paxos 三种角色:Proposer,Acceptor,Learners

Proposer

只要Proposer 发的提案被半数以上Acceptor 接受，Proposer 就认为该提案里的value 被选定了。

Acceptor

只要Acceptor 接受了某个提案，Acceptor 就认为该提案里的value 被选定了。

Learner

Acceptor 告诉Learner 哪个value 被选定，Learner 就认为那个value 被选定。

Paxos 算法

具体分为两个阶段，具体如下：

阶段一（准leader 确定）

(a) Proposer 选择一个提案编号N，然后向半数以上的Acceptor 发送编号为N的Prepare 请求。 (b) 如果一个Acceptor 收到一个编号为N 的Prepare 请求，且N 大于该Acceptor 已经响应过的所有Prepare 请求的编号，那么它就会将它已经接受过的编号最大的提案（如果有的话）作为响应反馈给Proposer，同时该Acceptor 承诺不再接受任何编号小于N 的提案。

阶段二（leader 确认）

(a) 如果Proposer 收到半数以上Acceptor 对其发出的编号为N 的Prepare 请求的响应，那么它就会发送一个针对[N,V]提案的Accept 请求给半数以上的Acceptor。注意：V 就是收到的响应中编号最大的提案的value，如果响应中不包含任何提案，那么V 就由Proposer 自己决定。 (b) 如果Acceptor 收到一个针对编号为N 的提案的Accept 请求，只要该Acceptor 没有对编号大于N 的Prepare 请求做出过响应，它就接受该提案。

## 3.Raft

与Paxos 不同Raft 强调的是易懂（Understandability），Raft 和Paxos 一样只要保证n/2+1 节点正常就能够提供服务；raft 把算法流程分为三个子问题：选举（Leader election）、日志复制（Log replication）、安全性（Safety）三个子问题。

### 角色

Raft 把集群中的节点分为三种状态：Leader、 Follower 、Candidate，理所当然每种状态负责的任务也是不一样的，Raft 运行时提供服务的时候只存在Leader 与Follower 两种状态；

Leader（领导者-日志管理）

负责日志的同步管理，处理来自客户端的请求，与Follower 保持这heartBeat 的联系；

Follower（追随者-日志同步）

刚启动时所有节点为Follower 状态，响应Leader 的日志同步请求，响应Candidate 的请求，把请求到Follower 的事务转发给Leader；

Candidate（候选者-负责投票）

负责选举投票，Raft 刚启动时由一个节点从Follower 转为Candidate 发起选举，选举出Leader 后从Candidate 转为Leader 状态；

### Term（任期）

在Raft 中使用了一个可以理解为周期（第几届、任期）的概念，用Term 作为一个周期，每个Term 都是一个连续递增的编号，每一轮选举都是一个Term 周期，在一个Term 中只能产生一个Leader；当某节点收到的请求中Term 比当前Term 小时则拒绝该请求。

### 选举（Election）

选举定时器

Raft 的选举由定时器来触发，每个节点的选举定时器时间都是不一样的，开始时状态都为Follower 某个节点定时器触发选举后Term 递增，状态由Follower 转为Candidate，向其他节点发起RequestVote RPC 请求，这时候有三种可能的情况发生： 1：该RequestVote 请求接收到n/2+1（过半数）个节点的投票，从Candidate 转为Leader，向其他节点发送heartBeat 以保持Leader 的正常运转。 2：在此期间如果收到其他节点发送过来的AppendEntries RPC 请求，如该节点的Term 大则当前节点转为Follower，否则保持Candidate 拒绝该请求。 3：Election timeout 发生则Term 递增，重新发起选举

在一个Term 期间每个节点只能投票一次，所以当有多个Candidate 存在时就会出现每个Candidate 发起的选举都存在接收到的投票数都不过半的问题，这时每个Candidate 都将Term递增、重启定时器并重新发起选举，由于每个节点中定时器的时间都是随机的，所以就不会多次存在有多个Candidate 同时发起投票的问题。 在Raft 中当接收到客户端的日志（事务请求）后先把该日志追加到本地的Log 中，然后通过heartbeat 把该Entry 同步给其他Follower，Follower 接收到日志后记录日志然后向Leader 发送ACK，当Leader 收到大多数（n/2+1）Follower 的ACK 信息后将该日志设置为已提交并追加到本地磁盘中，通知客户端并在下个heartbeat 中Leader 将通知所有的Follower 将该日志存储在自己的本地磁盘中。

## 4.raft 协议和 zab 协议区别

### 相同点

- 采用quorum 来确定整个系统的一致性,这个quorum 一般实现是集群中半数以上的服务器,
- zookeeper 里还提供了带权重的quorum 实现.
- 都由leader 来发起写操作.
- 都采用心跳检测存活性
- leader election 都采用先到先得的投票方式

### 不同点

- zab 用的是epoch 和count 的组合来唯一表示一个值, 而raft 用的是term 和index
- zab 的follower 在投票给一个leader 之前必须和leader 的日志达成一致,而raft 的follower则简单地说是谁的term 高就投票给谁
- raft 协议的心跳是从leader 到follower, 而zab 协议则相反
- raft 协议数据只有单向地从leader 到follower(成为leader 的条件之一就是拥有最新的log),而zab 协议在discovery 阶段, 一个prospective leader 需要将自己的log 更新为quorum 里面最新的log,然后才好在synchronization 阶段将quorum 里的其他机器的log 都同步到一致.