# 电商详情页系统缓存架构

### 1.真正能支撑高并发以及高可用的复杂系统中的缓存架构有哪些东西？

（1）如何让redis集群支撑几十万QPS高并发+99.99%高可用+TB级海量数据+企业级数据备份与恢复？：redis企业级集群架构

（2）如何支撑高性能以及高并发到极致？同时给缓存架构最后的安全保护层？：(nginx+lua)+redis+ehcache的三级缓存架构

（3）高并发场景下，如何解决数据库与缓存双写的时候数据不一致的情况？：企业级的完美的数据库+缓存双写一致性解决方案

（4）如何解决大value缓存的全量更新效率低下问题？：缓存维度化拆分解决方案

（5）如何将缓存命中率提升到极致？：双层nginx部署架构，以及lua脚本实现的一致性hash流量分发策略

（6）如何解决高并发场景下，缓存重建时的分布式并发重建的冲突问题？：基于zookeeper分布式锁的缓存并发重建解决方案

（7）如何解决高并发场景下，缓存冷启动MySQL瞬间被打死的问题？：基于storm实时统计热数据的分布式快速缓存预热解决方案

（8）如何解决热点缓存导致单机器负载瞬间超高？：基于storm的实时热点发现，以及毫秒级的实时热点缓存负载均衡降级

（9）如何解决分布式系统中的服务高可用问题？避免多层服务依赖因为少量故障导致系统崩溃？：基于hystrix的高可用缓存服务，资源隔离+限流+降级+熔断+超时控制

（10）如何应用分布式系统中的高可用服务的高阶技术？：基于hystrix的容错+多级降级+手动降级+生产环境参数优化经验+可视化运维与监控

（11）如何解决恐怖的缓存雪崩问题？避免给公司带来巨大的经济损失？：独家的事前+事中+事后三层次完美解决方案

（12）如何解决高并发场景下的缓存穿透问题？避免给MySQL带来过大的压力？：缓存穿透解决方案

（13）如何解决高并发场景下的缓存失效问题？避免给redis集群带来过大的压力？：缓存失效解决方案

### 2.企业级的持久化的配置策略

在企业中，RDB的生成策略，用默认的也差不多

save 60 10000：如果你希望尽可能确保说，RDB最多丢1分钟的数据，那么尽量就是每隔1分钟都生成一个快照，低峰期，数据量很少，也没必要

10000->生成RDB，1000->RDB，这个根据你自己的应用和业务的数据量，你自己去决定

AOF一定要打开，fsync，everysec

auto-aof-rewrite-percentage 100: 就是当前AOF大小膨胀到超过上次100%，上次的两倍 auto-aof-rewrite-min-size 64mb: 根据你的数据量来定，16mb，32mb

### 3.redis优化

1、fork耗时导致高并发请求延时

RDB和AOF的时候，其实会有生成RDB快照，AOF rewrite，耗费磁盘IO的过程，主进程fork子进程。

fork的时候，子进程是需要拷贝父进程的空间内存页表的，也是会耗费一定的时间的。

一般来说，如果父进程内存有1个G的数据，那么fork可能会耗费在20ms左右，如果是10G~30G，那么就会耗费20 * 10，甚至20 * 30，也就是几百毫秒的时间。

info stats中的latest_fork_usec，可以看到最近一次form的时长。

redis单机QPS一般在几万，fork可能一下子就会拖慢几万条操作的请求时长，从几毫秒变成1秒。

优化思路

fork耗时跟redis主进程的内存有关系，一般控制redis的内存在10GB以内，slave -> master，全量复制。

2、AOF的阻塞问题

redis将数据写入AOF缓冲区，单独开一个现场做fsync操作，每秒一次。

但是redis主线程会检查两次fsync的时间，如果距离上次fsync时间超过了2秒，那么写请求就会阻塞。

everysec，最多丢失2秒的数据。

一旦fsync超过2秒的延时，整个redis就被拖慢。

优化思路

优化硬盘写入速度，建议采用SSD，不要用普通的机械硬盘，SSD，大幅度提升磁盘读写的速度。

3、主从复制延迟问题

主从复制可能会超时严重，这个时候需要良好的监控和报警机制。

在info replication中，可以看到master和slave复制的offset，做一个差值就可以看到对应的延迟量。

如果延迟过多，那么就进行报警。

4、主从复制风暴问题

如果一下子让多个slave从master去执行全量复制，一份大的rdb同时发送到多个slave，会导致网络带宽被严重占用。

如果一个master真的要挂载多个slave，那尽量用树状结构，不要用星型结构。

5、vm.overcommit_memory

0: 检查有没有足够内存，没有的话申请内存失败 1: 允许使用内存直到用完为止 2: 内存地址空间不能超过swap + 50%

如果是0的话，可能导致类似fork等操作执行失败，申请不到足够的内存空间。

cat /proc/sys/vm/overcommit_memory echo "vm.overcommit_memory=1" >> /etc/sysctl.conf sysctl vm.overcommit_memory=1

6、swapiness

cat /proc/version，查看linux内核版本。

如果linux内核版本<3.5，那么swapiness设置为0，这样系统宁愿swap也不会oom killer（杀掉进程）。 如果linux内核版本>=3.5，那么swapiness设置为1，这样系统宁愿swap也不会oom killer。

保证redis不会被杀掉。

echo 0 > /proc/sys/vm/swappiness echo vm.swapiness=0 >> /etc/sysctl.conf

7、最大打开文件句柄

ulimit -n 10032 10032

不同的操作系统，版本，设置的方式都不太一样

8、tcp backlog

cat /proc/sys/net/core/somaxconn echo 511 > /proc/sys/net/core/somaxconn

### 4.企业架构

持久化：高可用的一部分，在发生redis集群灾难的情况下（比如说部分master+slave全部死掉了），如何快速进行数据恢复，快速实现服务可用，才能实现整个系统的高可用。

复制：主从架构，master -> slave 复制，读写分离的架构，写master，读slave，横向扩容slave支撑更高的读吞吐，读高并发，10万，20万，30万，上百万，QPS，横向扩容。

哨兵：高可用，主从架构，在master故障的时候，快速将slave切换成master，实现快速的灾难恢复，实现高可用性。

redis cluster：多master读写，数据分布式的存储，横向扩容，水平扩容，快速支撑高达的数据量+更高的读写QPS，自动进行master -> slave的主备切换，高可用。

让底层的缓存系统，redis，实现能够任意水平扩容，支撑海量数据（1T+，几十T，10G * 600 redis = 6T），支撑很高的读写QPS（redis单机在几万QPS，10台，几十万QPS），高可用性（给我们每个redis实例都做好AOF+RDB的备份策略+容灾策略，slave -> master主备切换）。

redis的第一套企业级的架构

如果你的数据量不大，单master就可以容纳，一般来说你的缓存的总量在10G以内就可以，那么建议按照以下架构去部署redis。

redis持久化+备份方案+容灾方案+replication（主从+读写分离）+sentinal（哨兵集群，3个节点，高可用性）。

可以支撑的数据量在10G以内，可以支撑的写QPS在几万左右，可以支撑的读QPS可以上10万以上（随你的需求，水平扩容slave节点就可以），可用性在99.99%。

redis的第二套企业级架构

如果你的数据量很大。

redis cluster

多master分布式存储数据，水平扩容。

支撑更多的数据量，1T+以上没问题，只要扩容master即可。

读写QPS分别都达到几十万都没问题，只要扩容master即可，redis cluster，读写分离，支持不太好，readonly才能去slave上读。

支撑99.99%可用性，也没问题，slave -> master的主备切换，冗余slave去进一步提升可用性的方案（每个master挂一个slave，但是整个集群再加个3个slave冗余一下）。

### 5.架构设计难题及解决方案

1、亿级流量电商网站的商品详情页系统架构

面临难题：对于每天上亿流量，拥有上亿页面的大型电商网站来说，能够支撑高并发访问，同时能够秒级让最新模板生效的商品详情页系统的架构是如何设计的？

解决方案：异步多级缓存架构+nginx本地化缓存+动态模板渲染的架构

2、redis企业级集群架构

面临难题：如何让redis集群支撑几十万QPS高并发+99.99%高可用+TB级海量数据+企业级数据备份与恢复？

解决方案：redis的企业级备份恢复方案+复制架构+读写分离+哨兵架构+redis cluster集群部署

3、多级缓存架构设计

面临难题：如何将缓存架构设计的能够支撑高性能以及高并发到极致？同时还要给缓存架构最后的一个安全保护层？

解决方案：nginx抗热点数据+redis抗大规模离线请求+ehcache抗redis崩溃的三级缓存架构

4、数据库+缓存双写一致性解决方案

面临难题：高并发场景下，如何解决数据库与缓存双写的时候数据不一致的情况？

解决方案：异步队列串行化的数据库+缓存双写一致性解决方案

5、缓存维度化拆分解决方案

面临难题：如何解决大value缓存的全量更新效率低下问题？

解决方案：商品缓存数据的维度化拆分解决方案

6、缓存命中率提升解决方案

面临难题：如何将缓存命中率提升到极致？

解决方案：双层nginx部署架构+lua脚本实现一致性hash流量分发策略

7、缓存并发重建冲突解决方案

面临难题：如何解决高并发场景下，缓存重建时的分布式并发重建的冲突问题？

解决方案：基于zookeeper分布式锁的缓存并发重建冲突解决方案

8、缓存预热解决方案

面临难题：如何解决高并发场景下，缓存冷启动导致MySQL负载过高，甚至瞬间被打死的问题？

解决方案：基于storm实时统计热数据的分布式快速缓存预热解决方案

9、热点缓存自动降级方案

面临难题：如何解决热点缓存导致单机器负载瞬间超高？

解决方案：基于storm的实时热点发现+毫秒级的实时热点缓存负载均衡降级

10、高可用分布式系统架构设计

面临难题：如何解决分布式系统中的服务高可用问题？避免多层服务依赖因为少量故障导致系统崩溃？

解决方案：基于hystrix的高可用缓存服务，资源隔离+限流+降级+熔断+超时控制

11、复杂的高可用分布式系统架构设计

面临难题：如何针对复杂的分布式系统将其中的服务设计为高可用架构？

解决方案：基于hystrix的容错+多级降级+手动降级+生产环境参数优化经验+可视化运维与监控

12、缓存雪崩解决方案

面临难题：如何解决恐怖的缓存雪崩问题？避免给公司带来巨大的经济损失？

解决方案：全网独家的事前+事中+事后三层次完美缓存雪崩解决方案

13、缓存穿透解决方案

面临难题：如何解决高并发场景下的缓存穿透问题？避免给MySQL带来过大的压力？

解决方案：缓存穿透解决方案

14、缓存失效解决方案

面临难题：如何解决高并发场景下的缓存失效问题？避免给redis集群带来过大的压力？

解决方案：基于随机过期时间的缓存失效解决方案

硬件规划

每日上亿流量，高峰QPS过1万

nginx部署，负载很重，16核32G，建议给3~5台以上，就非常充裕了，每台抗个几千QPS

缓存服务部署，4核8G，按照每台QPS支撑500，部署个10~20台

redis部署，每台给8核16G，根据数据量以及并发读写能力来看，部署5~10个master，每个master挂一个slave，主要是为了支撑更多数据量，1万并发读写肯定没问题了

### 6.大型电商网站的商品详情页系统架构演进

第一个版本

架构设计：

- J2EE+Tomcat+MySQL。
- 动态页面，每次请求都要调用多个依赖服务的接口，从数据库里查询数据，然后通过类似JSP的技术渲染到HTML模板中，返回最终HTML页面。

架构缺陷：

- 每次请求都是要访问数据库的，性能肯定很差。
- 每次请求都要调用大量的依赖服务，依赖服务不稳定导致商品详情页展示的性能经常抖动。

第二个版本

架构设计：

- 页面静态化技术。
- 通过MQ得到商品详情页涉及到的数据的变更消息。
- 通过Java Worker服务全量调用所有的依赖服务的接口，查询数据库，获取到构成一个商品详情页的完整数据，并通过velocity等模板技术生成静态HTML。
- 将静态HTML页面通过rsync工具直接推送到多台nginx服务器上，每台nginx服务器上都有全量的HTML静态页面。
- nginx对商品详情页的访问请求直接返回本地的静态HTML页面。
- 在nginx服务器前加一层负载均衡设备，请求打到任何一台应用nginx服务器上，都有全量的HTML静态页面可以返回。

架构缺陷：

- 全量更新问题，如果某一个商品分类、商家等信息变更了，那么那个分类、店铺、商家下面所有的商品详情页都需要重新生成静态HTML页面。
- 更新速度过慢问题，分类、店铺、商家、商品越来越多，重新生成HTML的负载越来越高，rsync全量同步所有nginx的负载也越来越高，从数据变更到生成静态HTML，再到全量同步到所有nginx，时间越来越慢。
- 扩容问题，因为每个商品详情页都要全量同步到所有的nginx上，导致系统无法扩容，无法增加系统容量。

架构优化：

- 解决全量更新问题，每次Java Worker收到某个维度的变更消息，不是拉去全量维度并生成完整HTML，而是按照维度拆分，生成一个变化维度的HTML片段， nginx对多个HTML片段通过SSI合并html片段然后输出一个完整的html。
- 解决扩容问题，每个商品详情页不是全量同步到所有的nginx，而是根据商品id路由到某一台nginx上，同时接入层nginx按照相同的逻辑路由请求。
- 更新速度过慢问题，增加更多机器资源，多机房部署，每个机房部署一套Java Worker+应用Nginx，所有机房用一套负载均衡设备，在每个机房内部完成全流程，不跨机房。

架构优化后的缺陷：

- 更新速度还是不够快的问题，商品的每个维度都有一个HTML片段，rsync推送大量的HTML片段，负载太高，性能较差，Nginx基于机械硬盘进行SSI合并，性能太差。
- 还是存在全量更新的问题，虽然解决了分类、商家、店铺维度的变更，只要增量重新生产较小的HTML片段即可，不用全量重新生成关联的所有商品详情页的HTML，但是如果某个页面模板变更，或者新加入一个页面模板，还是会导致几亿个商品的HTML片段都要重新生成和rsync，要几天时间才能完成，无法响应需求。
- 还是存在容量问题，nginx存储有限，不能无限存储几亿，以及增长的商品详情页的HTML文件，如果nginx存储达到极限，需要删除部分商品详情页的HTML文件，改成nginx找不到HTML，则调用后端接口，回到动态页面的架构，动态页面架构在高并发访问的情况下，会对依赖系统造成过大的压力，几乎扛不住。

第三个版本

需要支持的需求：

- 迅速响应各种页面模板的改版和个性化需求的新模板的加入
- 页面模块化，页面中的某个区域变化，只要更新这个区域中的数据即可
- 支持高性能访问
- 支持水平扩容的伸缩性架构

架构设计：

- 依赖服务有数据变更发送消息到MQ
- 数据异构Worker服务监听MQ中的变更消息，调用依赖服务的接口，仅仅拉取有变更的数据即可，然后将数据存储到redis中
- 数据异构Worker存储到redis中的，都是原子未加工数据，包括商品基本信息、商品扩展属性、商品其他信息、商品规格参数、商品分类、商家信息
- 数据异构Worker发送消息到MQ，数据聚合Worker监听到MQ消息
- 数据聚合Worker将原子数据从redis中取出，按照维度聚合后存储到redis中，包括三个维度，基本信息维度：基本信息、扩展属性，商品介绍：PC版、移动版 ，其他信息：商品分类、商家信息，nginx+lua，lua从redis读取商品各个维度的数据，通过nginx动态渲染到html模板中，然后输出最终的html。

如何解决所有的问题：

- 更新问题：不再是生成和推送html片段了，不再需要合成html，直接数据更新到redis，然后走动态渲染，性能大大提升。
- 全量更新问题：数据和模板分离，数据更新呢就更新数据，模板更新直接推送模板到nginx，不需要重新生成所有html，直接走动态渲染。
- 容量问题：不需要依赖nginx所在机器的磁盘空间存储大量的html，将数据放redis，html就存放模板，大大减少空间占用，而且redis集群可扩容。

### 7.亿级流量大型电商网站的商品详情页系统架构的整体设计

动态渲染系统 将页面中静的数据，直接在变更的时候推送到缓存，然后每次请求页面动态渲染新数据 商品详情页系统（负责静的部分）：被动接收数据，存储redis，nginx+lua动态渲染 商品详情页动态服务系统（对外提供数据接口） 提供各种数据接口 动态调用依赖服务的接口，产生数据并且返回响应 从商品详情页系统处理出来的redis中，获取数据，并返回响应 OneService系统 动的部分，都是走ajax异步请求的，不是走动态渲染的 商品详情页统一服务系统（负责动的部分） 前端页面 静的部分，直接被动态渲染系统渲染进去了 动的部分，html一到浏览器，直接走js脚本，ajax异步加载 商品详情页，分段存储，ajax异步分屏加载 工程运维 限流，压测，灰度发布

### 8.商品详情页动态渲染系统：大型网站的多机房4级缓存架构设计

本地缓存 使用nginx shared dict作为local cache，http-lua-module的shared dict可以作为缓存，而且reload nginx不会丢失 也可以使用nginx proxy cache做local cache 双层nginx部署，一层接入，一层应用，接入层用hash路由策略提升缓存命中率 比如库存缓存数据的TP99为5s，本地缓存命中率25%，redis命中率28%，回源命中率47% 一次普通秒杀活动的命中率，本地缓存55%，分布式redis命中率15%，回源命中率27% 最高可以提升命中率达到10% 全缓存链路维度化存储，如果有3个维度的数据，只有其中1个过期了，那么只要获取那1个过期的数据即可 nginx local cache的过期时间一般设置为30min，到后端的流量会减少至少3倍 4级多级缓存 nginx本地缓存，抗热点数据，小内存缓存访问最频繁的数据 各个机房本地的redis从集群的数据，抗大量离线数据，采用一致性hash策略构建分布式redis缓存集群 tomcat中的动态服务的本地jvm堆缓存 支持在一个请求中多次读取一个数据，或者与该数据相关的数据 作为redis崩溃的备用防线 固定缓存一些较少访问频繁的数据，比如分类，品牌等数据 堆缓存过期时间为redis过期时间的一半 主redis集群 命中率非常低，小于5% 防止主从同步延迟导致的数据读取miss 防止各个机房的从redis集群崩溃之后，全量走依赖服务会导致雪崩，主redis集群是后备防线 主redis集群，采取多机房一主三从的高可用部署架构 redis集群部署采取双机房一主三活的架构，机房A部署主集群+一个从集群，机房B部署一个从集群（从机房A主集群）+一个从集群（从机房B从集群） 双机房一主三活的架构，保证了机房A彻底故障的时候，机房B还有一套备用的集群，可以升级为一主一从 如果采取机房A部署一主一从，机房B一从，那么机房A故障时，机房B的一从承载所有读写压力，压力过大，很难承受

### 9.商品详情页动态渲染系统：架构整体设计

（1）依赖服务 -> MQ -> 动态渲染服务 -> 多级缓存 （2）负载均衡 -> 分发层nginx -> 应用层nginx -> 多级缓存 （3）多级缓存 -> 数据直连服务

动态渲染系统

数据闭环 数据闭环架构 依赖服务：商品基本信息，规格参数，商家/店铺，热力图，商品介绍，商品维度，品牌，分类，其他 发送数据变更消息到MQ 数据异构Worker集群，监听MQ，将原子数据存储到redis，发送消息到MQ 数据聚合Worker集群，监听MQ，将原子数据按维度聚合后存储到redis，三个维度（商品基本信息、商品介绍、其他信息） 数据闭环，就是数据的自我管理，所有数据原样同步后，根据自己的逻辑进行后续的数据加工，走系统流程，以及展示k 数据形成闭环之后，依赖服务的抖动或者维护，不会影响到整个商品详情页系统的运行 数据闭环的流程：数据异构（多种异构数据源拉取），数据原子化，数据聚合（按照维度将原子数据进行聚合），数据存储（Redis） 数据维度化 商品基本信息：标题、扩展属性、特殊属性、图片、颜色尺码、规格参数 商品介绍 非商品维度其他信息：分类，商家，店铺，品牌 商品维度其他信息：采用ajax异步加载，价格，促销，配送至，广告，推荐，最佳组合，等等 采取ssdb，这种基于磁盘的大容量/高性能的kv存储，保存商品维度、主商品维度、商品维度其他信息，数据量大，不能光靠内存去支撑 采取redis，纯内存的kv存储，保存少量的数据，比如非商品维度的其他数据，商家数据，分类数据，品牌数据 一个完整的数据，拆分成多个维度，每个维度独立存储，就避免了一个维度的数据变更就要全量更新所有数据的问题 不同维度的数据，因为数据量的不一样，可以采取不同的存储策略 系统拆分 系统拆分更加细：依赖服务、MQ、数据异构Worker、数据同步Worker、Redis、Nginx+Lua 每个部分的工作专注，影响少，适合团队多人协作 异构Worker的原子数据，基于原子数据提供的服务更加灵活 聚合Worker将数据聚合后，减少redis读取次数，提升性能 前端展示分离为商品详情页前端展示系统和商品介绍前端展示系统，不同特点，分离部署，不同逻辑，互相不影响 异步化 异步化，提升并发能力，流量削峰 消息异步化，让各个系统解耦合，如果使用依赖服务调用商品详情页系统接口同步推送，那么就是耦合的 缓存数据更新异步化，数据异构Worker同步调用依赖服务接口，但是异步更新redis 动态化 数据获取动态化：nginx+lua获取商品详情页数据的时候，按照维度获取，比如商品基本数据、其他数据（分类、商家） 模板渲染实时化：支持模板页面随时变化，因为采用的是每次从nginx+redis+ehcache缓存获取数据，渲染到模板的方式，因此模板变更不用重新静态化HTML 重启应用秒级化：nginx+lua架构，重启在秒级 需求上线快速化：使用nginx+lua架构开发商品详情页的业务逻辑，非常快速 多机房多活 Worker无状态，同时部署在各自的机房时采取不同机房的配置，来读取各自机房内部部署的数据集群（redis、mysql等） 将数据异构Worker和数据聚合Worker设计为无状态化，可以任意水平扩展 Worker无状态化，但是配置文件有状态，不同的机房有一套自己的配置文件，只读取自己机房的redis、ssdb、mysql等数据 每个机房配置全链路：接入nginx、商品详情页nginx+商品基本信息redis集群+其他信息redis集群、商品介绍nginx+商品介绍redis集群 部署统一的CDN以及LVS+KeepAlived负载均衡设备

### 10.商品详情页动态渲染系统：复杂的消息队列架构设计

队列化 任务等待队列 任务排重队列（异构Worker对一个时间段内的变更消息做排重） 失败任务队列（失败重试机制） 优先级队列，刷数据队列（依赖服务洗数据）、高优先级队列（活动商品优先级高）

### 11.商品详情页动态渲染系统：使用多线程并发提升系统吞吐量的设计

并发化 数据同步服务做并发化+合并，将多个变更消息合并在一起，调用依赖服务一次接口获取多个数据，采用多线程并发调用 数据聚合服务做并发化，每次重新聚合数据的时候，对多个原子数据用多线程并发从redis查询

### 12.商品详情页动态渲染系统：redis批量查询性能优化设计

mget，把要查询的数据一次性全部拿到，在不同的redis实例上，可能就要走不同的实例，性能并不是太好。

对某一个商品的数据，使用hash tag功能，做路由，一个商品的数据全部路由到同一个redis实例上去。然后一次性全部取出来。

### 13.商品详情页动态渲染系统：全链路高可用架构设计

高可用设计 读链路多级降级：本机房从集群 -> 主集群 -> 直连 全链路隔离 基于hystrix的依赖调用隔离，限流，熔断，降级 普通服务的多机房容灾冗余部署以及隔离

1.如果缓存从集群挂掉了，怎么降级？应用nginx会做一个计数，如果访问缓存从集群最近连续几次都失败，那么就认为挂掉，然后设置一个标志位以及一个时间周期，在那个时间周期内，请求会发现标志位是降级标志位，然后就不走本机房的缓存从集群，直接走数据直连服务。

如果超出一段时候之后，就会自动将标志位修改，再次尝试访问缓存从集群。

2.如果数据直连服务，宕机了，怎么办？应用nginx一样，记录好直连服务，如果最近几次http请求都失败，那么也是修改标志位，设置降级时间，然后每次请求直接从应用nginx访问到缓存主集群。

3.如果缓存主机群也宕机了，那么此时再次降级，修改标志位，直接应用nginx，跨机房，去直接调用依赖服务的接口，依赖服务是直接查询mysql的，所以这里的话，其实每一个依赖服务都要基于hystrix做限流，熔断等措施，避免自己被打垮。

4.读链路多级降级的策略，控制权掌握在应用层nginx手中，它自己会去检测问题，自动进行整个读链路的自动降级。

### 14.商品详情页动态渲染系统：微服务架构设计

1、领域驱动设计：我们需要对这个系统涉及到的领域模型机进行分析，然后进行领域建模，最后的话，设计出我们对应的微服务的模型 2、spring cloud：微服务的基础技术架构，我们用spring cloud来做 3、持续交付流水线，jenkins+git+自动化持续集成+自动化测试+自动化部署 4、docker：大量的微服务的部署与管理

LVS+KeepAlived，负载均衡

MySQL+Atlas，分库分表

微服务架构的几大特征：

（1）足够单一的职责与功能 （2）非常的微型 （3）面向服务的思想 （4）独立开发：团队，技术选型，前后端分离，存储分离，独立部署 （5）自动化开发流程：编码，自动化测试，持续集成，自动化部署

微服务的强大作用：

（1）迭代速度：你只要管好自己的服务就行了，跟别人没关系，随便你这么玩儿，修改代码，测试，部署，都是你自己的事情，不用考虑其他人，没有任何耦合 （2）复用性：拆分成一个一个服务之后，就不需要写任何重复的代码了，有一个功能别人做好了，暴露了接口出来，直接调用不就ok了么 （3）扩展性：独立，扩展，升级版本，重构，更换技术 （4）完全克服了传统单块应用的缺点

微服务的缺点

（1）服务太多，难以管理 （2）微服务 = 分布式系统，你本来是一个系统，现在拆分成多块，部署在不同的服务器上，一个请求要经过不同的服务器上不同的代码逻辑处理，才能完成，这不就是分布式系统 （3）分布式一致性，分布式事务，故障+容错

### 15.商品详情页动态渲染系统：机房与机器的规划

负载均衡：2台机器，lvs+keepalived，双机高可用

两个机房，每个机房给1台机器，总共就是2台机器，分发层nginx+应用层nginx+缓存从集群

缓存主集群：模拟跟上面的两个机房部署在一起，在实际生产环境中，的确可能是在相同的机房，但是肯定在不同的机器上

缓存集群分片中间件，跟缓存集群部署在一起

rabbitmq和mysql：1台机器

### 16.商品详情页动态渲染系统：为什么是twemproxy+redis而不是redis cluster？

twemproxy+redis去做集群，redis部署多个主实例，每个主实例可以挂载一些redis从实例，如果将不同的数据分片，写入不同的redis主实例中，twemproxy这么一个缓存集群的中间件。

redis cluster

（1）不好做读写分离，读写请求全部落到主实例上的，如果要扩展写QPS，或者是扩展读QPS，都是需要扩展主实例的数量，从实例就是一个用做热备+高可用。 （2）不好跟nginx+lua直接整合，lua->redis的client api，但是不太支持redis cluster，中间就要走一个中转的java服务。 （3）不好做树状集群结构，比如redis主集群一主三从双机房架构，redis cluster不太好做成那种树状结构。 （4）方便，相当于是上下线节点，集群扩容，运维工作，高可用自动切换，比较方便。

twemproxy+redis

（1）上线下线节点，有一些手工维护集群的成本。 （2）支持redis集群+读写分离，就是最基本的多个redis主实例，twemproxy这个中间件来决定的，java/nginx+lua客户端，是连接twemproxy中间件的。每个redis主实例就挂载了多个redis从实例，高可用->哨兵，redis cluster读写都要落到主实例的限制，你自己可以决定写主，读从，等等。 （3）支持redis cli协议，可以直接跟nginx+lua整合。 （4）可以搭建树状集群结构。

twemproxy讲解

```html
eshop-detail-test:  
  listen: 127.0.0.1:1111  
  hash: fnv1a_64  
  distribution: ketama  
  timeout:1000  
  redis: true  
  servers:  
   - 127.0.0.1:6379:1 test-redis-01 
   - 127.0.0.1:6380:1 test-redis-02
```

eshop-detail-test: redis集群的逻辑名称 listen：twemproxy监听的端口号 hash：hash散列算法 distribution：分片算法，一致性hash，取模，等等 timeout：跟redis连接的超时时长 redis：是否是redis，false的话是memcached servers：redis实例列表，一定要加别名，否则默认使用ip:port:weight来计算分片，如果宕机后更换机器，那么分片就不一样了，因此加了别名后，可以确保分片一定是准确的

你的客户端，java/nginx+lua，连接twemproxy，写数据的时候，twemproxy负责将数据分片，写入不同的redis实例

如果某个redis机器宕机，需要自动从一致性hash环上摘掉，等恢复后自动上线

auto_eject_hosts: true，自动摘除故障节点 server_retry_timeout: 30000，每隔30秒判断故障节点是否正常，如果正常则放回一致性hash环 server_failure_limit: 2，多少次无响应，就从一致性hash环中摘除

### 17.商品详情页动态渲染系统：部署双机房一主三从架构的redis主集群

在第一台虚拟机上，部署两个redis主实例+两个redis从实例，模拟一个机房的情况 在第二台虚拟机上，部署两个redis从实例，挂载到第一台虚拟机的redis从实例上; 再部署两个redis从实例，挂载到第二台虚拟机的从实例上

给每个机房部署一个redis从集群

为redis主集群部署twemproxy中间件

### 18.微服务的技术栈

（1）领域驱动设计：微服务建模

你的任何业务系统都有自己独特的复杂的业务，但是这个时候就是有一个问题，怎么拆分服务？拆成哪些服务？拆成多大？每个服务负责哪些功能？

微服务的建模，模型怎么设计

领域驱动的设计思想，可以去分析系统，完成建模的设计

（2）Spring Cloud：基础技术架构

各个服务之间怎么知道对方在哪里 -> 服务的注册和发现

服务之间的调用怎么处理，rpc，负载均衡

服务故障的容错

服务调用链条的追踪怎么做

多个服务依赖的统一的配置如何管理

（3）DevOps：自动化+持续集成+持续交付+自动化流水线，将迭代速度提升到极致

如果要将微服务的开发效率提升到最高，DevOps，全流程标准化，自动化，大幅度提升你的开发效率

（4）Docker：容器管理大量服务

微服务，一个大型的系统，可以涉及到几十个，甚至是上百个服务，比较坑，怎么部署，机器怎么管理，怎么运维

### 19.Spring Boot

1、Spring Boot的特点

（1）快速开发spring应用的框架

spring mvc+spring+mybatis，首先配置一大堆xml配置文件，其次部署和安装tomcat，jetty等容器，跟java web打交道

跟servlet，listener，filter，打交道

手工部署到tomcat或者jetty等容器中，发布一个web应用

spring boot，简单来说，就是看中了这种java web应用繁琐而且重复的开发流程，采用了spring之上封装的一套框架，spring boot，简化整个这个流程

尽可能提升我们的开发效率，让我们专注于自己的业务逻辑即可

（2）内嵌tomcat和jetty容器，不需要单独安装容器，jar包直接发布一个web应用 （3）简化maven配置，parent这种方式，一站式引入需要的各种依赖 （4）基于注解的零配置思想 （5）和各种流行框架，spring web mvc，mybatis，spring cloud无缝整合

2、Spring Boot和微服务

（1）spring boot不是微服务技术 （2）spring boot只是一个用于加速开发spring应用的基础框架，简化工作，开发单块应用很适合 （3）如果要直接基于spring boot做微服务，相当于需要自己开发很多微服务的基础设施，比如基于zookeeper来实现服务注册和发现 （4）spring cloud才是微服务技术

### 20.商品详情页动态渲染系统：Spring Cloud之Eureka注册中心

1、什么是注册中心

（1）就是首先有一个eureka server，服务的注册与发现的中心 （2）你如果写好了一个服务，就可以将其注册到eureka server上去 （3）然后别人的服务如果要调用你的服务，就可以从eureka server上查找你的服务所在的地址，然后调用

2、Eureka基本原理

（1）服务都会注册到eureka的注册表 （2）eureka有心跳机制，自动检测服务，故障时自动从注册表中摘除 （3）每个服务也会缓存euraka的注册表，即使eureka server挂掉，每个服务也可以基于本地注册表缓存，与其他服务进行通信 （4）只不过是如果eureka server挂掉了，那么无法发布新的服务了

3、eureka server

（1）pom.xml

（2）Application

```html
@EnableEurekaServer
@SpringBootApplication
public class EurekaServerApplication {

    public static void main(String[] args) {
        SpringApplication.run(EurekaServerApplication.class, args);
    }
}
```

（3）application.yml

```html
server:
  port: 8761

eureka:
  instance:
    hostname: localhost
  client:
    registerWithEureka: false
    fetchRegistry: false
    serviceUrl:
      defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/
```

（4）访问8761端口

4、eureka client

（1）pom.xml

（2）Application

```html
@SpringBootApplication
@EnableEurekaClient
@RestController
public class SayHelloServiceApplication {

    public static void main(String[] args) {
        SpringApplication.run(SayHelloServiceApplication.class, args);
    }

    @Value("${server.port}")
    String port;
    @RequestMapping("/sayHello")
    public String home(@RequestParam String name) {
        return "hello, " + name + " from port:" +port;
    }
}
```

（3）application.yml

```html
eureka:
  client:
    serviceUrl:
      defaultZone: http://localhost:8761/eureka/
server:
  port: 8762
spring:
  application:
    name: service-say-hello
```

（4）查看eureka server界面

（5）http://localhost:8762/sayHello?name=leo

### 21.商品详情页动态渲染系统：Spring Cloud之Ribbon+Rest调用负载均衡

通过eureka server发现其他服务，并且调用其他服务，通过ribbon+rest，RestTemplate调用rest服务接口，ribbon多个服务实例的负载均衡

1、将say-hello-service的port修改为8673，再启动一个实例

在生产环境中，肯定是一个服务会发布在多台机器上，每个机器上发布的服务，就是一个服务实例，多个服务实例实际上就组成了一个集群

2、创建一个新的工程，叫做greeting-service

（1）pom.xml

（2）application.yml

```html
eureka:
  client:
    serviceUrl:
      defaultZone: http://localhost:8761/eureka/
server:
  port: 8764
spring:
  application:
    name: greeting-service
```

（3）Application

```html
// @EnableDiscoveryClient，向eureka注册自己作为一个服务的调用client
// 之前的服务，EnableEurekaClient，代表的是向eureka注册自己，将自己作为一个服务
@SpringBootApplication
@EnableDiscoveryClient
public class GreetingServiceApplication {

    public static void main(String[] args) {
        SpringApplication.run(GreetingServiceApplication.class, args);
    }
    
    // 在spring容器中注入一个bean，RestTemplate，作为rest服务接口调用的客户端
    // @LoadBalanced标注，代表对服务多个实例调用时开启负载均衡
    @Bean
    @LoadBalanced
    public RestTemplate restTemplate() {
        return new RestTemplate();
    }

}
```

（4）调用say-hello-service

```html
// 写一个服务，注入RestTemplate服务调用客户端
@Service
public class GreetingService {

    @Autowired
    private RestTemplate restTemplate;

    // 用SAY-HELLO-SERVICE这个服务名替代实际的ip地址
    // ribbon负载在多个服务实例之间负载均衡，每次调用随机挑选一个实例，然后替换服务名
    public String greeting(String name) {
        return restTemplate.getForObject("http://SAY-HELLO-SERVICE/sayHello?name="+name, String.class);
    }
}
```

（5）controller

```html
@RestController
public class GreetingControler {

    @Autowired
    private GreetingService greetingService;
	
    @RequestMapping(value = "/greeting")
    public String greeting(@RequestParam String name){
        return greetingService.greeting(name);
    }
}
```

（6）多次访问http://localhost:8764/greeting?name=leo，发现每次访问的端口都不一样，在多个服务实例之间负载均衡了

### 22.商品详情页动态渲染系统：Spring Cloud之Fegion声明式服务调用

ribbon+rest是比较底层的调用方式，其实一般不常用

fegion，声明式的服务调用，类似于rpc风格的服务调用，默认集成了ribbon做负载均衡，集成eureka做服务发现

用fegion来重构greeting-service

1、pom.xml

2、application.yml

```html
eureka:
  client:
    serviceUrl:
      defaultZone: http://localhost:8761/eureka/
server:
  port: 8764
spring:
  application:
    name: greeting-service
```

3、Application

```html
@SpringBootApplication
@EnableDiscoveryClient
@EnableFeignClients
public class GreetingServiceApplication {

    public static void main(String[] args) {
        SpringApplication.run(GreetingServiceApplication.class, args);
    }
}
```

4、声明远程服务接口

```html
@FeignClient(value = "say-hello-service")
public interface SayHelloService {
    @RequestMapping(value = "/sayHello",method = RequestMethod.GET)
    String sayHello(@RequestParam(value = "name") String name);
}
```

5、controller

```html
@RestController
public class GreetingController {
    @Autowired
    private SayHelloService sayHelloService;
	
    @RequestMapping(value = "/greeting", method = RequestMethod.GET)
    public String greeting(@RequestParam String name){
        return sayHelloService.sayHello(name);
    }	
}
```

### 23.商品详情页动态渲染系统：Spring Cloud之Hystrix熔断降级

微服务架构，很重要的就是多个服务之间互相调用，很可能某个服务就死了，然后依赖它的其他服务调用大量超时，最后耗尽资源，继续死，最终导致整个系统崩盘

hystrix去做资源隔离，限流，熔断，降级

1、让greeting-service支持hystrix

（1）pom.xml

（2）application.xml

```html
feign.hystrix.enabled=true
```

（3）SayHelloService

```html
@FeignClient(value = "say-hello-service", fallback = SayHelloServiceFallback.class)  
public interface SayHelloService {

	@RequestMapping(value = "/sayHello", method = RequestMethod.GET)
	public String sayHello(@RequestParam(value = "name") String name);

}
```

（4）SayHelloServiceFallback

```html
@Component
public class SayHelloServiceFallback implements SayHelloService {

    @Override
    public String sayHello(String name) {
        return "error, " + name;
    }

}
```

（5）先保持SayHelloService启动，可以访问; 关闭SayHelloSerivce，再访问，调用失败直接走降级

包括限流，自动熔断，调用失败，都会走降级

2、hystrix dashboard

（1）pom.xml

（2）Application

```html
@EnableHystrixDashboard
@EnableCircuitBreaker
```

（3）http://localhost:8764/hystrix

输入http://localhost:8764/hystrix.stream和title

访问接口，会在hystrix dashboard看到访问请求

3、改造say-hello-service支持hystrix

将一个服务多个实例的指标聚合起来看，改造say-hello-service

（1）pom.xml

（2）Application

```html
@SpringBootApplication
@EnableEurekaClient
@RestController
@EnableHystrix
@EnableHystrixDashboard
@EnableCircuitBreaker
public class SayHelloServiceApplication {

	public static void main(String[] args) {
		SpringApplication.run(SayHelloServiceApplication.class, args); 
	}
	
	@Value("${server.port}")
	private String port;
	
	@RequestMapping("/sayHello")
	@HystrixCommand(fallbackMethod = "sayHelloFallback")
	public String sayHello(String name) {
		return "hello, " + name + " from port: " + port;
	}
	
	public String sayHelloFallback(String name) {
		return "error, " + name
	}

}
```

（3）locahost:8762/hystrix

输入locahost:8762/hystrix.stream，2000，title

访问这个接口

4、创建turbin工程，hystrix-turbine-server

（1）pom.xml

（2）Application

```html
@SpringBootApplication
@EnableTurbine
public class HystrixTurbineServer {

    public static void main(String[] args) {
        new SpringApplicationBuilder(HystrixTurbineServer.class).web(true).run(args);
    }

}
```

（3）application.yml

```html
spring:
  application.name: hystrix-terbine-server
server:
  port: 8765
security.basic.enabled: false
turbine:
  aggregator:
    clusterConfig: default   
  appConfig: say-hello-service
  clusterNameExpression: new String("default")
eureka:
  client:
    serviceUrl:
      defaultZone: http://localhost:8761/eureka/
```

（4）对say-hello-service每个服务实例都访问几次

http://localhost:8762/hystrix

stream输入：http://localhost:8765/turbine.stream

在dashboard可以看到两个服务实例聚合起来的指标

### 24.商品详情页动态渲染系统：Spring Cloud之Zuul网关路由

常规的spring cloud的微服务架构下

前端请求先通过nginx走到zuul网关服务，zuul负责路由转发、请求过滤等网关接入层的功能，默认和ribbon整合实现了负载均衡

比如说你有20个服务，暴露出去，你的调用方，如果要跟20个服务打交道，是不是很麻烦

所以比较好的一个方式，就是开发一个通用的zuul路由转发的服务，根据请求api模式，动态将请求路由转发到对应的服务

你的前端，主要考虑跟一个服务打交道就可以了

1、创建zuul-server工程

2、pom.xml

3、Application

```html
@EnableZuulProxy
@EnableEurekaClient
@SpringBootApplication
public class ServiceZuulApplication {

    public static void main(String[] args) {
        SpringApplication.run(ServiceZuulApplication.class, args);
    }

}
```

4、application.yml

```html
eureka:
  client:
    serviceUrl:
      defaultZone: http://localhost:8761/eureka/
server:
  port: 8766
spring:
  application:
    name: zuul-server
zuul:
  routes:
    say-hello:
      path: /say/hello/**
      serviceId: say-hello-service
    greeting:
      path: /greeting/**
      serviceId: greeting-service
```

5、修改代码

在greeting-service中的返回值加入自己的标识

6、运行，依次走两种不同的api接口，zuul会路由到不同的服务上去

7、请求过滤

```html
@Component
public class UserLoginFilter extends ZuulFilter {

    private static Logger logger = LoggerFactory.getLogger(UserLoginFilter.class);
    
    // pre，routing，post，error
    @Override
    public String filterType() {
        return "pre";
    }
    
    // 顺序
    @Override
    public int filterOrder() {
        return 0;
    }
    
    // 根据逻辑判断是否要过滤
    @Override
    public boolean shouldFilter() {
        return true;
    }
    
    @Override
    public Object run() {
        RequestContext ctx = RequestContext.getCurrentContext();
        HttpServletRequest request = ctx.getRequest();
        log.info(String.format("%s >>> %s", request.getMethod(), request.getRequestURL().toString()));
    	
        Object userId = request.getParameter("userId");
    	
        if(userId == null) {
            log.warn("userId is empty");
            ctx.setSendZuulResponse(false);
            ctx.setResponseStatusCode(401);
            try {
                ctx.getResponse().getWriter().write("userId is empty");
            }catch (Exception e){}
    
            return null;
        }
    	
        log.info("ok");
    	
        return null;
    }

}
```

### 25.商品详情页动态渲染系统：Spring Cloud之Config统一配置中心

多个服务共享相同的配置，举个例子，数据库连接，redis连接，还有别的一些东西，包括一些降级开关，等等

用config统一配置中心

1、创建工程config-server

2、pom.xml

3、Application

```html
@SpringBootApplication
@EnableConfigServer
public class ConfigServerApplication {

    public static void main(String[] args) {
        SpringApplication.run(ConfigServerApplication.class, args);
    }

}
```

4、公开的git仓库

spring cloud config，配置文件，用的是properties的格式，基于git去做

```html
账号：roncoo-eshop
密码：roncoo123456
仓库地址：https://github.com/roncoo-eshop/roncoo-eshop-config
```

5、application.properties

```html
spring.application.name=config-server
server.port=8767

spring.cloud.config.server.git.uri=https://github.com/roncoo-eshop/roncoo-eshop-config
spring.cloud.config.server.git.searchPaths=config-file
spring.cloud.config.label=master
spring.cloud.config.server.git.username=roncoo-eshop
spring.cloud.config.server.git.password=roncoo123456
```

6、访问http://localhost:8767/name/dev

7、重构greeting-service

配置一个默认的name，如果前端没有传递name参数，直接取用默认的name

（1）pom.xml

（2）application.yml -> bootstrap.properties

```html
spring.application.name=config-client
spring.cloud.config.label=master
spring.cloud.config.profile=dev
spring.cloud.config.uri= http://localhost:8767/
server.port=8764
eureka.client.serviceUrl.defaultZone=http://localhost:8761/eureka/
feign.hystrix.enabled=true
```

（3）controller

```html
@Value("${defaultName}")
private String defaultName;
```

没有传递name的时候，默认用spring cloud config中配置的name

### 26.商品详情页动态渲染系统：Spring Cloud之Sleuth调用链路追踪

在一个微服务系统中，一个请求过来，可能会经过一个很复杂的调用链路，经过多个服务的依次处理，才能完成

在这个调用链路过程中，可能任何一个环节都会出问题，所以如果要进行一些问题的定位，那么就要对每个调用链路进行追踪

sleuth

1、搭建sleuth server

（1）创建工程：sleuth-server

（2）pom.xml

（3）Application

```html
@SpringBootApplication
@EnableZipkinServer
public class SleuthServer {

    public static void main(String[] args) {
        SpringApplication.run(SleuthServer.class, args);
    }

}
```

（4）application.yml

```html
server.port=9411
```

2、在say-hello-service和greeting-service中加入sleuth支持

（1）pom.xml

```html
<dependency>
	<groupId>org.springframework.cloud</groupId>
	<artifactId>spring-cloud-starter-zipkin</artifactId>
</dependency>
```

（2）application.yml

```html
spring.zipkin.base-url=http://localhost:9411
```

3、调用接口，查看http://localhost:9411

### 27.商品详情页动态渲染系统：Spring Cloud之Eureka Server安全认证

1、pom.xml

```html
<dependency>  
    <groupId>org.springframework.boot</groupId>  
    <artifactId>spring-boot-starter-security</artifactId>  
</dependency>  
```

2、application.yml

```html
security:  
  basic:  
    enabled: true  
  user:  
    name: admin  
    password: 123456
```

3、访问http://localhost:8761/，需要输入用户名和密码

### 28.商品详情页动态渲染系统：完成Spring Boot+Spring Cloud+MyBatis整合

spring boot+spring cloud，本身自带了spring web mvc的支持，mybatis整合起来，可以操作数据库

1、pom.xml

2、application.yml

```html
spring: 
  datasource: 
    type: com.alibaba.druid.pool.DruidDataSource
    platform: mysql
    url: jdbc:mysql://192.168.31.223:3306/eshop
    username: root
    password: root
```

3、User

```html
public class User {	

	private Long id;
	private String name;
	private Integer age;

}
```

4、UserMapper

```html
@Mapper
public interface UserMapper {	

	@Select("select * from users")
	List<User> findAllUsers();

}
```

5、UserService

```html
@Service
public class UserService {	

	@Autowired
	private UserMapper userMapper;
	
	public List<User> findAllUsers() {
		return userMapper.findAllUsers();
	}

}
```

6、UserController

```html
@RestController
public class UserController {	

	@Autowrite
	private UserService userService;
	
	@RequestMapping("/findAllUsers")
	@ResponseEntity
	public List<User> findAllUsers() {
		return userService.findAllUsers();
	}

}
```

跟mybatis整合成功，可以操作数据库

### 29.基于spring cloud搭建

spring boot + spring cloud + spring mvc + spring + mybatis

### 30.商品详情页动态渲染系统：windows部署rabbitmq作为开发测试环境

1、安装erlang

下载erlang：http://www.erlang.org/downloads，otp_win64_20.0.exe，直接安装

在windows设置环境变量

```html
ERLANG_HOME=/usr/local/erlang
PATH=$ERLANG_HOME/bin:$PATH
```

erl

3、安装rabbitmq

http://www.rabbitmq.com/download.html

```html
cd rabbitmq-3.6.1/sbin/
rabbitmq-plugins.bat enable rabbitmq_management
```

后台启动rabbitmq server

添加管理员账号

```html
./rabbitmqctl add_user rabbitadmin 123456
./rabbitmqctl set_user_tags rabbitadmin administrator
```

进入管理页面

15672端口号，输入用户名和密码

### 31.商品详情页动态渲染系统：windows部署redis作为开发测试环境

在windows上部署一个最基本的单实例的redis，可以用就行了，可以写kv存储就可以了

上生产环境，也是直接连接twemproxy，连接twemproxy中间件+redis集群，跟本地连接单个的redis实例，是一个意思

原生的redis是不支持windows的，但是微软搞了一个可以在windows上部署的redis版本，供我们学习、开发以及测试来使用的

### 32.商品详情页动态渲染系统：依赖服务将数据变更消息写入rabbitmq或双写redis

1、基于spring boot整合rabbitmq的发送与消费

（1）pom.xml

```html
<dependency>  
    <groupId>org.springframework.boot</groupId>  
    <artifactId>spring-boot-starter-amqp</artifactId>  
</dependency>
```

（2）application.yml

```html
spring: 
  rabbitmq:
    host: localhost
    port: 15672  
    username: rabbitadmin  
    password: 123456
```

（3）生产者

```html
@Component  
public class RabbitMQSender {  

    @Autowired  
    private AmqpTemplate rabbitTemplate;  
       
    public void send(String message) {  
        this.rabbitTemplate.convertAndSend("my-queue", message);  
    }  

}  
```

（4）消费者

```html
@Component  
@RabbitListener(queues = "my-queue")  
public class RabbitMQReceiver {  

    @RabbitHandler  
    public void process(String message) {  
        System.out.println("从my-queue队列接收到一条消息：" + message);  
    }  

}  
```

（5）在rabbitmq管理界面中创建队列

```html
rabbitmqctl set_permissions -p / rabbitadmin '.*' '.*' '.*' 
```

在web界面中，到admin下，点击一下那个用户，set一下permission，就可以创建queue了

2、spring boot整合redis

（1）pom.xml

```html
<dependency>
	<groupId>com.alibaba</groupId>
	<artifactId>fastjson</artifactId>
	<version>1.1.43</version>
</dependency>
<dependency>
	<groupId>redis.clients</groupId>
	<artifactId>jedis</artifactId>
</dependency>
```

（2）Application

```html
@Bean
public Jedis jedis() {
	JedisPoolConfig config = new JedisPoolConfig();
	config.setMaxActive(100);
	config.setMaxIdle(5);
	config.setMaxWait(1000 * 100);
	config.setTestOnBorrow(true);
	pool = new JedisPool(config, "localhost", 6379);
}
```

3、商品服务数据变更，将消息写入rabbitmq

时效性比较低的数据，走rabbitmq，然后后面就接着整套动态渲染系统去玩儿

4、价格服务和库存服务，数据变更，直接将数据双写到redis中

时效性比较高的数据，直接mysql+redis双写，不走冬天渲染系统，写到redis之后，后面走OneService服务提供页面的ajax调用

### 33.商品详情页动态渲染系统：基于Spring Cloud开发数据同步服务

1、基于spring boot整合rabbitmq的发送与消费

（1）pom.xml

```html
<dependency>  
    <groupId>org.springframework.boot</groupId>  
    <artifactId>spring-boot-starter-amqp</artifactId>  
</dependency>
```

（2）application.yml

```html
spring: 
  rabbitmq:
    host: localhost
    port: 15672  
    username: rabbitadmin  
    password: 123456
```

（3）生产者

```html
@Component  
public class RabbitMQSender {  

    @Autowired  
    private AmqpTemplate rabbitTemplate;  
       
    public void send(String message) {  
        this.rabbitTemplate.convertAndSend("my-queue", message);  
    }  

}  
```

（4）消费者

```html
@Component  
@RabbitListener(queues = "my-queue")  
public class RabbitMQReceiver {  

    @RabbitHandler  
    public void process(String message) {  
        System.out.println("从my-queue队列接收到一条消息：" + message);  
    }  

}  
```

（5）在rabbitmq管理界面中创建队列

2、spring boot整合redis

（1）pom.xml

```html
<dependency>
	<groupId>com.alibaba</groupId>
	<artifactId>fastjson</artifactId>
	<version>1.1.43</version>
</dependency>
<dependency>
	<groupId>redis.clients</groupId>
	<artifactId>jedis</artifactId>
</dependency>
```

（2）Application

```html
@Bean
public Jedis jedis() {
	return new Jedis("127.0.0.1", 6379);
}
```

3、基于spring cloud开发数据同步服务

（1）接收到增删改消息

（2）直接基于Fegion调用依赖服务接口，拉取数据，对redis原子数据进行增删改操作

（3）再将数据变更消息按照维度发送到rabbitmq

4、基于spring cloud开发数据聚合服务

（1）接收到数据变更消息

（2）按照维度从redis中获取数据，聚合成一个维度数据，写入redis中维度聚合数据

### 34.商品详情页动态渲染系统：基于Spring Cloud开发数据聚合服务

是这样，数据同步服务，就是去接收各个依赖服务发送过来的某个原子数据的变更消息，然后将原子数据通过fegion调用依赖服务的接口拉取过来

然后写入redis中

接着再将某个维度数据的变更消息发送到另外一个queue中

数据聚合服务，就是去监听另外一个queue，得到某个维度变化的消息之后，就从redis中将这个维度的数据全部读取出来，然后拼成一个大的聚合json串

1、基于spring boot整合rabbitmq的发送与消费

（1）pom.xml

```html
<dependency>  
    <groupId>org.springframework.boot</groupId>  
    <artifactId>spring-boot-starter-amqp</artifactId>  
</dependency>
```

（2）application.yml

```html
spring: 
  rabbitmq:
    host: localhost
    port: 15672  
    username: rabbitadmin  
    password: 123456
```

（3）生产者

```html
@Component  
public class RabbitMQSender {  

    @Autowired  
    private AmqpTemplate rabbitTemplate;  
       
    public void send(String message) {  
        this.rabbitTemplate.convertAndSend("my-queue", message);  
    }  

}  
```

（4）消费者

```html
@Component  
@RabbitListener(queues = "my-queue")  
public class RabbitMQReceiver {  

    @RabbitHandler  
    public void process(String message) {  
        System.out.println("从my-queue队列接收到一条消息：" + message);  
    }  

}  
```

（5）在rabbitmq管理界面中创建队列

2、spring boot整合redis

（1）pom.xml

```html
<dependency>
	<groupId>com.alibaba</groupId>
	<artifactId>fastjson</artifactId>
	<version>1.1.43</version>
</dependency>
<dependency>
	<groupId>redis.clients</groupId>
	<artifactId>jedis</artifactId>
</dependency>
```

（2）Application

```html
@Bean
public Jedis jedis() {
	return new Jedis("127.0.0.1", 6379);
}
```

3、基于spring cloud开发数据同步服务

（1）接收到增删改消息

（2）直接基于Fegion调用依赖服务接口，拉取数据，对redis原子数据进行增删改操作

（3）再将数据变更消息按照维度发送到rabbitmq

4、基于spring cloud开发数据聚合服务

（1）接收到数据变更消息

（2）按照维度从redis中获取数据，聚合成一个维度数据，写入redis中维度聚合数据

### 35.商品详情页动态渲染系统：消息队列架构升级之去重队列

依赖服务，一个商品服务走动态渲染系统，另外两个价格服务和库存服务走mysql+redis双写+OneService系统+页面Ajax

商品服务（增删改查各种数据） -> 发送数据变更消息到queue -> 数据同步服务+原子数据更新到redis中 -> 发送维度数据变更消息到queue -> 数据聚合服务+将原子数据从redis中查询出来按照维度聚合后写入redis

对这个里面的一些细节做一些架构上的优化和升级

消息队列，rabbitmq，去重队列

动态渲染系统，就说明了，数据更新之后，要反馈到页面中，时效性并不是太高，可以接受几秒钟甚至是几分钟的延迟

在这里为了减少后面的系统，比如说数据聚合服务的压力，可以做一些优化，去重队列

数据同步服务里面，完全可以不用每次立即发送维度数据变更消息，可以将维度数据变更消息采用set的方式，在内存中先进行去重

开启一条后台线程，每隔5分钟，每隔1分钟，每隔5秒钟，将set中的数据拿出来，发送到下一个queue中，然后set中的数据清除掉

比如某一个维度数据，product，商品属性，商品规格，商品本身，短时间内变更了，可能就会发送3条一模一样的维度数据变更消息到下一个queue

数据聚合服务短时间内要执行3次聚合操作，压力比较大， 给redis带来的压力也比较大

如果做了去重之后，3个维度数据变更消息会在数据同步服务的内存中先去重，然后几分钟之后才会发送一条消息到下一个queue，数据聚合服务执行一次聚合操作即可

### 36.商品详情页动态渲染系统：消息队列架构升级之刷数据与高优先级队列

刷数据的问题

业务系统，特别是在快速迭代过程中，可能会因为一些代码的bug，或者要上线一些新功能，导致需要对全量的数据刷一遍数据

本来有个字段，status，0，1，2这样的值，但是现在要将所有数据，status这个字段的值刷为OPEN，CLOSED，SEND之类的状态

刷数据，一般是在晚上凌晨的时候执行的，依赖服务会大量的更新数据，大量的刷数据的请求会到我们的消息队列中

此时我们的系统压力会非常的大

甚至可能会影响夜间一些正常用户的购买行为，等等

所以一般对这个问题是这样的，我们会针对刷数据的问题，单独开出来队列，专门处理刷数据的请求，对这些队列的消费，通常来说，只会在凌晨0点之后才开始执行

这样的话呢，好处在于，正常的消息不会跟刷数据的消息混杂在一个队列中，可以拆分到不同的队列中

高优先级问题

大家可以这么想，有些数据，比如说，一些特别紧急的活动对应的数据，需要尽快的反应到页面中，那么此时，一个道理，如果这种高优先级的数据

跟其他的消息混杂的一个队列中，势必需要去等待队列中其他的普通消息先处理完了，才能轮到自己

所以一般来说，也会单开高优先级的队列，然后如果业务系统有高优先级的消息，直接写到高优先的队列中，这样的话呢，后续流程全部单独处理

### 37.商品详情页动态渲染系统：吞吐量优化之批量调用依赖服务接口

### 38.商品详情页动态渲染系统：吞吐量优化之redis mget批量查询数据

### 39.商品详情页动态渲染系统：在分发层nginx部署流量分发的lua脚本

### 40.商品详情页动态渲染系统：完成应用层nginx的lua脚本的编写与部署

### 41.商品详情页动态渲染系统：基于Spring Cloud开发数据直连服务

如果nginx本地，走nginx local cache没有，在本机房的通过twemproxy读本机房的从集群，如果还是没有，则发送http请求给数据直连服务

数据直连服务，先在自己本地读取ehcache（有的时候也可以忽略，我这里就不做了，因为之前已经做过了），读redis主集群，通过fegion拉取依赖服务的接口

将数据写入主集群中，主集群会同步到各个机房的从集群，同时数据直连服务将获取到的数据返回给nginx，nginx会写入自己本地的local cache

### 42.商品详情页动态渲染系统：商品介绍分段存储以及分段加载的介绍

商品介绍，product_intro，里面可能包含大段的文字，还有大量的图片

存储的时候，完全可以将大段的东西，分段来存储，因为一般最好不要将一个特别大的value存储到redis中

商品介绍，实际上，很多商品，考虑用户的实际访问情况，很多用户可能误点击，进入第一屏看下就走了，或者进入第一屏一看，不感兴趣，就跳出去了，还有一种，就是只看了第二屏，或者第三屏，但是后面的没有看完，所以说对这个大段的商品介绍，可以采取这种分段加载的方式，来做，按需加载。

第一屏，都没有商品介绍。

滑动到第二屏的时候，js脚本来控制，先发送ajax请求到nginx，要求显示第一个分段。

滚动到第三屏的时候，再次ajax请求，分段加载。

### 43.商品详情页动态渲染系统：高可用架构优化之读链路多级降级思路

读链路：nginx local cache -> 本机房redis从集群 -> 数据直连服务的jvm堆缓存（之前讲解，这次没做） -> 其他机房redis主集群 -> 依赖服务

读链路的降级

本机房redis从集群可能会挂掉，可能性会大一些：降级为直接连数据直连服务

数据直连服务也可能会挂掉：降级为跨机房直接连redis主集群

### 44.商品详情页动态渲染系统：高可用架构优化之hystrix隔离与降级

在spring cloud里面，fegion去掉用其他服务接口的操作，在这里肯定是要将hystrix跟fegion整合起来的

这样的话，对后端的依赖服务的接口才能做资源隔离，不至于说某一个依赖服务故障，拖垮整个服务，有一个服务故障的时候，可以自动降级

hystrix，还可以自动做限流

依赖服务故障过多，限流，熔断，降级

```html
hystrix.command.default.execution.isolation.thread.timeoutInMilliseconds: 5000
```

### 45.商品详情页动态渲染系统：在CentOS 6安装和部署Docker

1、初步安装和启动docker

```html
yum update -y

yum install -y yum-utils

yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

yum -y install docker-ce

systemctl start docker
```

2、设置镜像

```html
vi /etc/docker/daemon.json

{
  "registry-mirrors": ["https://aj2rgad5.mirror.aliyuncs.com"]
}
```

3、开放管理端口映射

```html
vi /lib/systemd/system/docker.service
```

将第11行的ExecStart=/usr/bin/dockerd，替换为：

```html
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock -H tcp://0.0.0.0:7654
```

2375是管理端口，7654是备用端口

在~/.bashrc中写入docker管理端口

```html
export DOCKER_HOST=tcp://0.0.0.0:2375

source ~/.bashrc
```

4、重启docker

```html
systemctl daemon-reload && service docker restart
```

5、测试docker是否正常安装和运行

```html
docker run hello-world

Hello from Docker!
This message shows that your installation appears to be working correctly.
```

### 46.商品详情页动态渲染系统：在CentOS 6安装maven、git以及推送github

1、安装maven

```html
wget http://mirror.bit.edu.cn/apache/maven/maven-3/3.5.0/binaries/apache-maven-3.5.0-bin.tar.gz

tar -zxvf apache-maven-3.5.0-bin.tar.gz

vi ~/.bashrc

export  MAVEN_HOME=/usr/local/apache-maven-3.5.0
export PATH=$PATH:$MAVEN_HOME/bin

source ~/.bashrc

mvn -version

cd /usr/local/apache-maven-3.5.0/conf

vi settings.xml

<mirror>
	<id>nexus-aliyun</id>
	<mirrorOf>*</mirrorOf>
	<name>Nexus aliyun</name>
	<url>http://maven.aliyun.com/nexus/content/groups/public</url>
</mirror>
```

2、安装git

```html
yum install -y git

git --version
```

3、修改课程中的所有服务的配置为基于虚拟机中的生产环境

4、将代码推送到github中

### 47.商品详情页动态渲染系统：通过jenkins+docker部署eureka服务

1、新建一个任务

2、构建一个自由风格的软件项目

3、配置Github，包括github地址，用户名和密码，分支

4、配置构建环境

```html
增加构建步骤 -> invoker top-level-Maven targets

Maven version: maven3.5.0
Goals: clean package
POM: pom.xml
```

5、增加构建步骤 -> execute shell

```html
#!/bin/bash
REGISTRY_URL=192.168.189.54:2375
WORK_DIR=/root/work_build
PROJECT_NAME=eureka-server
PROJECT_VERSION=0.0.1-SNAPSHOT
if [ ! -e ${WORK_DIR}/${PROJECT_NAME} ] && [ ! -d ${WORK_DIR}/${PROJECT_NAME} ]; then
mkdir -p ${WORK_DIR}/${PROJECT_NAME}
echo "Create Dir: ${WORK_DIR}/${PROJECT_NAME}"
fi
if [ -e ${WORK_DIR}/${PROJECT_NAME}/Dockerfile ]; then
rm -rf ${WORK_DIR}/${PROJECT_NAME}/Dockerfile
echo "Remove File: ${WORK_DIR}/${PROJECT_NAME}/Dockerfile"
fi
cp /root/.jenkins/workspace/eureka-server/Dockerfile ${WORK_DIR}/${PROJECT_NAME}/
cp /root/.jenkins/workspace/eureka-server/target/*.jar ${WORK_DIR}/${PROJECT_NAME}/
cd ${WORK_DIR}/${PROJECT_NAME}/
docker build -t ${REGISTRY_URL}/eshop-detail/${PROJECT_NAME}:${PROJECT_VERSION} .
docker push ${REGISTRY_URL}/eshop-detail/${PROJECT_NAME}:${PROJECT_VERSION}
if docker ps -a | grep ${PROJECT_NAME}; then
docker rm -f ${PROJECT_NAME}
echo "Remove Docker Container: ${PROJECT_NAME}"
fi
docker run -d -p 8761:8761 --name ${PROJECT_NAME} ${REGISTRY_URL}/eshop-detail/${PROJECT_NAME}:${PROJECT_VERSION}
```

6、执行一些修改

（1）修改1：在系统配置中设置maven，然后在配置中选择自己配置的maven版本，解决cannot run mvn program的错误

（2）修改2：编写DockerFile

```html
FROM frolvlad/alpine-oraclejdk8:slim
VOLUME /tmp
ADD eureka-server-0.0.1-SNAPSHOT.jar app.jar
#RUN bash -c 'touch /app.jar'
ENTRYPOINT ["java","-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar"]
EXPOSE 8761
```

7、执行构建

8、访问http://192.168.31.253:8761/，可以看到eureka-server的页面

### 48.商品详情页动态渲染系统：twemproxy hash tag+mget优化思路介绍

我们其实twemproxy就是将Redis集群做成了很多个分片，相当于是部署了很多个redis主实例

然后通过twemproxy中间件，将数据散列存储到多个redis实例中去，每个redis实例中存储一部分的数据

但是还记得，我们用了mget那个优化操作么

```html
product_1
product_property_1
product_specification_1
```

这是3个key，可能会散列到不同的redis实例上去

此时直接用mget很坑，可能要走多次网络请求，连接多个redis实例才能获取到所有数据

所以这里的优化思路，就是用hash tag这个功能

通过hash tag，将一个商品关联的数据（可能会被mget一下子一起批量获取的数据），全部路由到同一个redis实例上去

这样的话呢，后面在获取数据的时候，直接就会路由到同一个redis实例，一下子mget出来多个数据

就不需要连接多次到不同的redis实例上去

1、twemproxy中配置一个东西，hash_tag

```html
server1:  
  listen: 127.0.0.1:1111  
  hash: fnv1a_64  
  distribution: ketama  
  redis: true  
  hash_tag: "::"  
  servers:  

   - 127.0.0.1:6660:1 server1  
   - 127.0.0.1:6661:1 server2  
```

2、写和读，都按照hash_tag格式来

```html
product:1:
product_property:1:
product_specification:1:
```

写的时候，不是按照product_1这种完整的串来计算hash值和路由的

是按照两个冒号中间的值来计算hash值和路由的

比如上面3个值，两个冒号中间都是1，那么计算出来的hash值一定是一样的，然后三个key-value对一定会路由到同一个redis实例上去

在读的时候

```html
mget product:1: product_property:1: product_specification:1:
```

读的时候，同样是根据两个冒号中间的值来路由和读取，这样的话，3个key会路由到同一个redis实例上去，一次性全部读出来

### 49.商品详情页动态渲染系统：所有服务最终修改以及jenkins+docker部署

把所有服务全部修改为生产环境的配置，然后全部代码推送带github，全部在jenkins中配置基于docker的自动化部署

整个流程串起来搞一遍看看

1、新建一个任务

2、构建一个自由风格的软件项目

3、配置Github，包括github地址，用户名和密码，分支

4、配置构建环境

增加构建步骤 -> invoker top-level-Maven targets

```html
Maven version: maven3.5.0
Goals: clean package
POM: pom.xml
```

5、增加构建步骤 -> execute shell

```html
#!/bin/bash
REGISTRY_URL=192.168.189.54:2375
WORK_DIR=/root/work_build
PROJECT_NAME=eshop-datalink-service
PROJECT_VERSION=0.0.1-SNAPSHOT
if [ ! -e ${WORK_DIR}/${PROJECT_NAME} ] && [ ! -d ${WORK_DIR}/${PROJECT_NAME} ]; then
mkdir -p ${WORK_DIR}/${PROJECT_NAME}
echo "Create Dir: ${WORK_DIR}/${PROJECT_NAME}"
fi
if [ -e ${WORK_DIR}/${PROJECT_NAME}/Dockerfile ]; then
rm -rf ${WORK_DIR}/${PROJECT_NAME}/Dockerfile
echo "Remove File: ${WORK_DIR}/${PROJECT_NAME}/Dockerfile"
fi
cp /root/.jenkins/workspace/eshop-datalink-service/Dockerfile ${WORK_DIR}/${PROJECT_NAME}/
cp /root/.jenkins/workspace/eshop-datalink-service/target/*.jar ${WORK_DIR}/${PROJECT_NAME}/
cd ${WORK_DIR}/${PROJECT_NAME}/
docker build -t ${REGISTRY_URL}/eshop-detail/${PROJECT_NAME}:${PROJECT_VERSION} .
docker push ${REGISTRY_URL}/eshop-detail/${PROJECT_NAME}:${PROJECT_VERSION}
if docker ps -a | grep ${PROJECT_NAME}; then
docker rm -f ${PROJECT_NAME}
echo "Remove Docker Container: ${PROJECT_NAME}"
fi
docker run -d -p 8767:8767 --network="host" --name ${PROJECT_NAME} ${REGISTRY_URL}/eshop-detail/${PROJECT_NAME}:${PROJECT_VERSION}
```

6、执行一些修改

（1）修改1：在系统配置中设置maven，然后在配置中选择自己配置的maven版本，解决cannot run mvn program的错误

（2）修改2：编写DockerFile

```html
FROM frolvlad/alpine-oraclejdk8:slim
VOLUME /tmp
ADD eureka-server-0.0.1-SNAPSHOT.jar app.jar
#RUN bash -c 'touch /app.jar'
ENTRYPOINT ["java","-Djava.security.egd=file:/dev/./urandom","-jar","/app.jar"]
EXPOSE 8761
```

hystrix有熔断的作用，所以一开始调用失败后，后面几次可能经常会失败，所以大家可以跟我一样，在datasync-service里加一个测试的接口，手动去调用一下product-service的接口，这样让接口调通了以后，就ok了

7、执行构建

8、访问http://192.168.31.253:8761/，可以看到eureka-server的页面

```html
GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;
```

### 50.商品详情页OneService系统：整体架构设计

OneService系统 商品详情页依赖的服务达到数十个，甚至是上百个 需要给一个统一的入口，打造服务闭环 请求预处理 合并接口调用，减少ajax异步加载次数 统一监控 统一降级

### 51.商品详情页OneService系统：库存服务与价格服务的代理接口开发

统一入口，方便用户调用

### 52.商品详情页OneService系统：请求预处理功能设计介绍

请求预处理 比如库存状态（有货/无货）的转换，第三方运费的处理，第三方配送时效（多少天发货）的处理 处理主商品与配件关系，比如说iphone可能就搭载着耳机，充电器，等等 商家运费动态计算 请求预处理，先做一些简单的，薄薄的一层的封装和代理，先做点业务逻辑的判断和处理

这样的话，就可以给后端的服务传递更多的参数，或者简化后端服务的计算逻辑

### 53.商品详情页OneService系统：多服务接口合并设计介绍

可能页面中需要相关联的几份数据，就不用一次又一次的发送ajax请求来获取多份数据

直接就是一次请求发给一个one service系统的大接口，然后那个接口统一调用各个后端服务的接口就可以

减少浏览器和后端系统之间的交互次数

将哪些接口合并为一个接口呢？如何来设计接口的合并呢？

请求合并 促销和广告，合并成一个接口，一次性发送请求过来，然后调用促销服务和广告服务，获取两份数据 库存服务，配送服务，合并成一个接口，一次性过来，获取当前有多少库存，如何配送 组合推荐服务+配件推荐服务+套装推荐服务，三个服务并发调用，合并结果

### 54.商品详情页OneService系统：基于hystrix进行接口统一降级

1、让greeting-service支持hystrix

（1）pom.xml

```html
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-hystrix</artifactId>
</dependency>
```

（2）application.xml

```html
feign.hystrix.enabled=true
```

（3）SayHelloService

```html
@FeignClient(value = "say-hello-service", fallback = SayHelloServiceFallback.class)  
public interface SayHelloService {

	@RequestMapping(value = "/sayHello", method = RequestMethod.GET)
	public String sayHello(@RequestParam(value = "name") String name);

}
```

（4）SayHelloServiceFallback

```html
@Component
public class SayHelloServiceFallback implements SayHelloService {

    @Override
    public String sayHello(String name) {
        return "error, " + name;
    }

}
```

（5）先保持SayHelloService启动，可以访问; 关闭SayHelloSerivce，再访问，调用失败直接走降级

包括限流，自动熔断，调用失败，都会走降级

### 55.商品详情页OneService系统：基于hystrix dashboard进行统一监控

2、hystrix dashboard

（1）pom.xml

```html
<dependency>
	<groupId>org.springframework.boot</groupId>
	<artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
<dependency>
	<groupId>org.springframework.cloud</groupId>
	<artifactId>spring-cloud-starter-hystrix-dashboard</artifactId>
</dependency>
```

（2）Application

```html
@EnableHystrixDashboard
@EnableCircuitBreaker
```

（3）http://localhost:8764/hystrix

输入http://localhost:8764/hystrix.stream和title

访问接口，会在hystrix dashboard看到访问请求

3、改造say-hello-service支持hystrix

将一个服务多个实例的指标聚合起来看，改造say-hello-service

（1）pom.xml

```html
<dependency>
	<groupId>org.springframework.boot</groupId>
	<artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
<dependency>
	<groupId>org.springframework.cloud</groupId>
	<artifactId>spring-cloud-starter-hystrix-dashboard</artifactId>
</dependency>
<dependency>
	<groupId>org.springframework.cloud</groupId>
	<artifactId>spring-cloud-starter-hystrix</artifactId>
</dependency>
```

（2）Application

```html
@SpringBootApplication
@EnableEurekaClient
@RestController
@EnableHystrix
@EnableHystrixDashboard
@EnableCircuitBreaker
public class SayHelloServiceApplication {

	public static void main(String[] args) {
		SpringApplication.run(SayHelloServiceApplication.class, args); 
	}
	
	@Value("${server.port}")
	private String port;
	
	@RequestMapping("/sayHello")
	@HystrixCommand(fallbackMethod = "sayHelloFallback")
	public String sayHello(String name) {
		return "hello, " + name + " from port: " + port;
	}
	
	public String sayHelloFallback(String name) {
		return "error, " + name
	}

}
```

（3）locahost:8762/hystrix

输入locahost:8762/hystrix.stream，2000，title

访问这个接口

4、创建turbin工程，hystrix-turbine-server

（1）pom.xml

（2）Application

```html
@SpringBootApplication
@EnableTurbine
public class HystrixTurbineServer {

    public static void main(String[] args) {
        new SpringApplicationBuilder(HystrixTurbineServer.class).web(true).run(args);
    }

}
```

（3）application.yml

```html
spring:
  application.name: hystrix-terbine-server
server:
  port: 8765
security.basic.enabled: false
turbine:
  aggregator:
    clusterConfig: default   
  appConfig: say-hello-service
  clusterNameExpression: new String("default")
eureka:
  client:
    serviceUrl:
      defaultZone: http://localhost:8761/eureka/
```

（4）对say-hello-service每个服务实例都访问几次

http://localhost:8762/hystrix

stream输入：http://localhost:8765/turbine.stream

在dashboard可以看到两个服务实例聚合起来的指标

### 56.商品详情页OneService系统：基于jenkins+docker部署OneService服务

### 57.商品详情页OneService系统：基于jenkins+docker部署hystrix terbine服务

### 58.商品详情页系统

有两套系统

第一套：商品服务+动态渲染系统

第二套：库存/价格服务+OneService系统

第三部分：前端页面

（1）时效性比较低的数据

更新的时候发送消息到mq，专门有一套数据同步服务+数据聚合服务来进行数据的加工和处理

前端页面，请求商品详情页的时候，nginx会走多级缓存策略（nginx local cache -> 本机房redis集群 -> 数据直连服务 -> 本地jvm cache -> redis主集群 -> 依赖服务），将时效性比较低的数据，全部加载到内存中，然后动态渲染到html中

前端html展示出来的时候，上来就有一些动态渲染出来的数据

（2）时效性比较高的数据

依赖服务每次更新数据库的时候，直接就更新redis缓存了，mysql+redis双写

前端html在展示出来以后，立即会对时效性要求比较高的数据，比如库存，价格，促销，推荐，广告，发送ajax请求到后盾

后端nginx接收到请求之后，就会将请求转发给one service系统，one service系统代理了所有几十个服务的接口，统一代理，统一降级，预处理，合并接口，统一监控

由one service系统发送请求给后端的一些服务，那些服务优先读redis，如果没有则读mysql，然后再重新刷入redis

（3）商品介绍

写的时候，采取的是分段存储策略

读的时候，也是在用户滚屏的时候，动态的异步ajax加载，分段加载商品介绍，不要一次性将所有的商品介绍都加载出来