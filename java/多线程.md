### 1.分布式服务接口的幂等性如何设计（比如不能重复扣款）？

所谓幂等性，就是说一个接口，多次发起同一个请求，你这个接口得保证结果是准确的，比如不能多扣款，不能多插入一条数据，不能将统计值多加了1。

其实保证幂等性主要是三点：

（1）对于每个请求必须有一个唯一的标识，举个例子：订单支付请求，肯定得包含订单id，一个订单id最多支付一次，对吧

（2）每次处理完请求之后，必须有一个记录标识这个请求处理过了，比如说常见的方案是在mysql中记录个状态啥的，比如支付之前记录一条这个订单的支付流水，而且支付流水

（3）每次接收请求需要进行判断之前是否处理过的逻辑处理，比如说，如果有一个订单已经支付了，就已经有了一条支付流水，那么如果重复发送这个请求，则此时先插入支付流水，orderId已经存在了，唯一键约束生效，报错插入不进去的。然后你就不用再扣款了。

（4）上面只是给大家举个例子，实际运作过程中，你要结合自己的业务来，比如说用redis用orderId作为唯一键。只有成功插入这个支付流水，才可以执行实际的支付扣款。

### 2.保证分布式系统的接口幂等性的几种常见方案

**1、业务表内唯一索引**

好比说电商系统里面，不是说有那个销售出库单表，wms中心

如果你要对创建销售出库单的接口保证幂等性，也就是说人家网络超时，重复调用的时候，保证一个订单只能有一个对应的销售出库单

销售出库单表里面，其实都是有这个unique key唯一索引的，比如说可以针对销售出库单的表的订单id，创建一个唯一索引，你如果接口被重试，同一个订单创建一个销售出库单的话，一定会违反唯一索引，那么此时会报错

保证说你在数据库里，一个订单只能有一个销售出单

常见于插入操作，创建一些关键的数据的时候，而且这个数据在库里只能有一条的时候

**2、业务表内状态机**

修改订单状态，比如说将订单状态修改为待发货的时候

update order set status = “待发货” where status = “待付款” and id = 1，订单的状态其实就变为了“待发货”。假如说id = 1的订单接口重复调用，又要执行一次这个操作的话，就不会生效了，就不会再次修改了

在订单模块里，我们不是用了状态模式，在进行状态流转的时候，其实都会去判断一下的，当前是否处于某个状态，然后才能流转到下一个状态。

**3、基于版本号的更新**

id name age version

1 张三 20 1

如果要调用人家的这个接口，更新他的这个年龄，先得查一下他的版本号是多少

version = 1

调用人家的接口修改他的年龄，要changeAge(1, 21, 1)

在你的接口里为了保证分布式接口的幂等性

update user set age=21, version=version+1 where id=1 and version=1

如果这条SQL执行成功过后

id name age version

1 张三 21 2

如果changeAge(1, 21, 1)，被重复调用了，此时会如何？

update user set age=21, version=version+1 where id=1 and version=1

我们一般不常用，对于接口调用方来说，要多做一些事情，他要先查出来数举的version，调用修改接口的时候，传过去这个version

**4、基于mysql的去重表 / 基于redis的去重**

就是说这个方案是很常见的一个方案

比如说你的接口的入参，参数，是changeAge(1, 21, 1)

将所有的参数拼接成一个字符串，或者是从这些入参里选择一些参数，可以唯一标识这一次请求的一些参数id和version，id和version每次请求都不一样的，应该是可以唯一的标识这一次请求

1_21_1，这样的一个字符串

如果基于mysql，单独搞一个表出来，就一个字段，建一个唯一索引，插入这个1_21_1到表里去。如果这个接口被重复调用的话，1_21_1，再次插入一个表的话，唯一索引会报一个冲突出来，这次插入就会失败

这个方案还是不错的，尤其是并发不是特别高的话，接口被调用的并发不是特别高的话，每秒的并发请求量在1000左右，1000以内的话，用mysql的去重表也没什么问题

但是如果接口调用量很大，并发很高，一秒请求量达到了几十万，选择使用redis，拼接一个串出来，直接set设置到redis里去，如果下一次人家请求再过来了，此时会发现这个key已经存在了，那么这个时候就不能执行了，因为已经出现重复调用了

### 3.一般实现分布式锁都有哪些方式？使用redis如何设计分布式锁？使用zk来设计分布式锁可以吗？这两种分布式锁的实现方式哪种效率比较高？

（1）redis分布式锁

官方叫做RedLock算法，是redis官方支持的分布式锁算法。

这个分布式锁有3个重要的考量点，互斥（只能有一个客户端获取锁），不能死锁，容错（大部分redis节点或者这个锁就可以加可以释放）

第一个最普通的实现方式，如果就是在redis里创建一个key算加锁

SET my:lock 随机值 NX PX 30000，这个命令就ok，这个的NX的意思就是只有key不存在的时候才会设置成功，PX 30000的意思是30秒后锁自动释放。别人创建的时候如果发现已经有了就不能加锁了。

释放锁就是删除key，但是一般可以用lua脚本删除，判断value一样才删除：

```html
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

为啥要用随机值呢？因为如果某个客户端获取到了锁，但是阻塞了很长时间才执行完，此时可能已经自动释放锁了，此时可能别的客户端已经获取到了这个锁，要是你这个时候直接删除key的话会有问题，所以得用随机值加上面的lua脚本来释放锁。

但是这样是肯定不行的。因为如果是普通的redis单实例，那就是单点故障。或者是redis普通主从，那redis主从异步复制，如果主节点挂了，key还没同步到从节点，此时从节点切换为主节点，别人就会拿到锁。

第二个问题，RedLock算法

这个场景是假设有一个redis cluster，有5个redis master实例。然后执行如下步骤获取一把锁：

1）获取当前时间戳，单位是毫秒

2）跟上面类似，轮流尝试在每个master节点上创建锁，过期时间较短，一般就几十毫秒

3）尝试在大多数节点上建立一个锁，比如5个节点就要求是3个节点（n / 2 +1）

4）客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了

5）要是锁建立失败了，那么就依次删除这个锁

6）只要别人建立了一把分布式锁，你就得不断轮询去尝试获取锁

（2）zk分布式锁

zk分布式锁，其实可以做的比较简单，就是某个节点尝试创建临时znode，此时创建成功了就获取了这个锁；这个时候别的客户端来创建锁会失败，只能注册个监听器监听这个锁。释放锁就是删除这个znode，一旦释放掉就会通知客户端，然后有一个等待着的客户端就可以再次重新枷锁。

（3）redis分布式锁和zk分布式锁的对比

redis分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能

zk分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小

另外一点就是，如果是redis获取锁的那个客户端bug了或者挂了，那么只能等待超时时间之后才能释放锁；而zk的话，因为创建的是临时znode，只要客户端挂了，znode就没了，此时就自动释放锁

redis分布式锁大家每发现好麻烦吗？遍历上锁，计算时间等等。。。zk的分布式锁语义清晰实现简单

所以先不分析太多的东西，就说这两点，我个人实践认为zk的分布式锁比redis的分布式锁牢靠、而且模型简单易用

如果有一把锁，被多个人给竞争，此时多个人会排队，第一个拿到锁的人会执行，然后释放锁，后面的每个人都会去监听排在自己前面的那个人创建的`node`上，一旦某个人释放了锁，排在自己后面的人就会被`zookeeper`给通知，一旦被通知了之后，就`ok`了，自己就获取到了锁，就可以执行代码了

### 4.redis分布式锁的问题

第一种方案，基于redis单实例，不靠谱，因为redis单点故障，会导致系统全盘崩溃，做不到高可用

第二种方案，基于redis主从架构+哨兵，保证高可用，master宕机，slave接替，但是有隐患，master宕机的一瞬间，还没异步复制锁到slave，导致重复加锁的问题，高可用是高可用了，但是锁的实现有漏洞，可能导致系统异常

客户端刚在master实例加了一个锁，但是master->slave的复制数据（锁复制过去）是异步的，导致master突然宕机，此时锁还没复制到slave，然后master->slave主备切换（哨兵），客户端B此时也对同一个key上锁，此时就会成功的在切换为master的slave实例上加锁，客户端A和客户端B同时对一个key完成了上锁

只要多个客户端同时对某个key上锁，就会导致数据肯定会出错

第三种方案，基于redis多master集群（redis-cluster，或者redis部署多机器，twitter开源的twemproxy做客户端集群分片，豌豆荚开源的codis做集群分片，都行），redlock算法，个人不推荐，实现过程太复杂繁琐，很脆弱，多节点同时设置分布式锁，但是失效时间都不一样，随着不同的linux机器的时间不同步，以及各种你无法考虑到的问题，很可能出现重复加锁

举个例子，给5个redis master都设置了一把锁key，失效时间是10s，但是因为设置key的时间可能因为网络等各种情况不同，会导致，也许客户端A还加着锁呢，因为处理耗时特别长，然后结果过了几秒钟，其中3台机器的锁key都过期失效了，因为这3台机器加锁的时间都比较靠前，已经10秒过期了，此时分布式锁自然失效，客户端B成功加锁，出现两个客户端同时加锁的问题

所以redlock算法，有两个问题，第一是实现过程和步骤太复杂，上锁的过程和机制很重，很复杂，导致很脆弱，各种意想不到的情况都可能发生；第二是并不健壮，不一定能完全实现健壮的分布式锁的语义

综上所述，redis分布式锁实际上目前没有100%完美的方案，或多或少有点问题，实际生产系统中，我设计的系统有时候用zk分布式锁，但是有的时候也会用redis分布式锁。

redis锁又有一个优点，redisson，知名的、优秀的开源的redis客户端类库，他封装了大量的基于redis的复杂的一些操作，数据集合（map、set、list）的分布式的存储，还有多种复杂的分布式锁，分布式执行操作以及对象，甚至就是说基于redis+redisson，甚至都可以将redis作为一个轻量级的NoSQL数据库、数据存储来使用

redisson：redis分布式锁的支持，非常不错，可重入锁、读写锁、公平锁、信号量、CountDownLatch，很多种复杂的锁的语义，可以支持我们将分布式锁玩儿的非常的好

但是zk分布式锁呢？其实有优点也有缺点，优点是锁模型健壮、稳定、可用性高，缺点是目前没有太好的开源的zk分布式锁的类库，封装多种不同的锁类型，因为有可重入锁、读写锁、公平锁等多种锁类型，但是zk的话，目前常见的方案，就是自己动手封装一个基于顺序节点的普通的悲观锁，没有复杂的锁机制的支持

但是如果有zookeeper的环境，而且你的分布式锁的需求很简单，就是普通的悲观锁模型就可以了，不涉及到什么公平锁、读写锁之类的东西，那么可以考虑基于zk的分布式锁模型，健壮，稳定，zk的临时顺序节点实现的分布式锁，其实那套模型挺健壮的，zk本身就是集群多节点分布式高可用，分布式系统协调、处理的，支持分布式锁的时候，基于zk的一些语义，监听节点的变化

zk锁，优点就是健壮、稳定、简单、易懂；缺点就是没有太好的开源的客户端类库，对zk分布式锁的封装和支持，可以支持那么多种不同的高阶的锁类型，公平锁、可重入锁、读写锁、信号量、CountDownLatch，Curator客户端主要还是针对zk的一些基础的语义做的一些封装，redis的Jedis + Redisson结合起来，Jedis封装redis的一些基础的语义，一些操作，都是不错的

### 5.redisson

（1）建一个普通的工程

（2）在pom.xml里引入依赖

```html
<dependency>
   <groupId>org.redisson</groupId>
   <artifactId>redisson</artifactId>
   <version>3.8.1</version>
</dependency>  
```

（3）参照官网构建RedissonClient，同时看看对应的配置

```html
Config config = new Config();
config.useClusterServers()
    .addNodeAddress("redis://192.168.31.114:7001")
    .addNodeAddress("redis://192.168.31.114:7002")
    .addNodeAddress("redis://192.168.31.114:7003")
    .addNodeAddress("redis://192.168.31.184:7001")
    .addNodeAddress("redis://192.168.31.184:7002")
    .addNodeAddress("redis://192.168.31.184:7003");
RedissonClient redisson = Redisson.create(config);
```

（4）简单用一下分布式锁的功能

```html
RLock lock = redisson.getLock("anyLock");
lock.lock();
lock.unlock();

RMap<String, Object> map = redisson.getMap("anyMap");
map.put("foo", "bar");  
        
map = redisson.getMap("anyMap");
System.out.println(map.get("foo"));
```

### 6.redis的分布式锁的方案

（1）加锁：在redis里设置hash数据结构，生存周期是30000毫秒

（2）维持加锁：代码里一直加锁，redis里的key会一直保持存活，后台每隔10秒的定时任务（watchdog）不断的检查，只要客户端还在加锁，就刷新key的生存周期为30000毫秒

（3）可重入锁：同一个线程可以多次加锁，就是在hash数据结构中将加锁次数累加1

（4）锁互斥：不同客户端，或者不同线程，尝试加锁陷入死循环等待

（5）手动释放锁：可重入锁自动递减加锁次数，全部释放锁之后删除锁key

（6）宕机自动释放锁：如果持有锁的客户端宕机了，那么此时后台的watchdog定时调度任务也没了，不会刷新锁key的生存周期，此时redis里的锁key会自动释放

（7）尝试加锁超时：在指定时间内没有成功加锁就自动退出死循环，标识本次尝试加锁失败

（8）超时锁自动释放：获取锁之后，在一定时间内没有手动释放锁，则redis里的key自动过期，自动释放锁

这8大机制，组合在一起，才是构成了一个企业级的基于redis的分布式锁的方案

### 7.公平锁

大致可以理解为，你的某个客户端不是在不断的重试尝试加锁么？每次重试尝试加锁的时候，就判定为是这个客户端一次新的尝试加锁的行为，此时会将这个客户端对应的timeout时间往后延长，10:00:25，这次他重试加锁之后，timeout时间算出来就变成10:00:30，把这个客户端的timeout时间延长了，有序集合中的分数变大了

zadd指令的返回值，如果是第一次往里面怼入一个元素，返回值是什么？如果是第二次刷新这个元素的分数，返回值是什么？如果是第一次插入一个元素在有序集合中，此时就会返回值是1，同时会入队；但是后续不断的尝试加锁的时候，其实是会不断的刷新这个有序集合中的元素的分数，越来越大，但是每次刷新分数，返回值是0

所以不会重复的往队列中插入这个元素的

客户端B的分数会刷大，客户端C的分数其实也在不断的刷大，所以在有序集合中的分数的大小的顺序，基本上还是按照的是最初他们是如何排队的，此时也会如何排队

获取一把分布式锁：

1）获取当前时间戳，单位是毫秒

2）跟上面类似，轮流尝试在每个master节点上创建锁，过期时间较短，一般就几十毫秒，在每个节点上创建锁的过程中，需要加一个超时时间，一般来说比如几十毫秒如果没有获取到锁就超时了，标识为获取锁失败

3）尝试在大多数节点上建立一个锁，比如3个节点就要求是2个节点（n / 2 +1）

4）客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了

5）要是锁建立失败了，那么就依次删除已经创建的锁

6）只要别人创建了一把分布式锁，你就得不断轮询去尝试获取锁

## 集合

### 1.ArrayList

默认的构造函数，直接初始化一个ArrayList实例的话，会将内部的数组做成一个默认的空数组，{}，Object[]，他有一个默认的初始化的数组的大小的数值，是10，也就是我们可以认为他默认的数组初始化的大小就是只有10个元素

基本上最好是给ArrayList构造的时候，给一个比较靠谱的初始化的数组的大小，比如说，100个数据，1000,10000，避免数组太小，往里面塞入数据的时候，导致数据不断的扩容，不断的搞新的数组

ensureCapacityInternal(size + 1); // Increments modCount!!

你每次往ArrayList中塞入数据的时候，人家都会判断一下，当前数组的元素是否塞满了，如果塞满的话，此时就会扩容这个数组，然后将老数组中的元素拷贝到新数组中去，确保说数组一定是可以承受足够多的元素的

数组扩容

```html
int newCapacity = oldCapacity + (oldCapacity >> 1);

oldCapacity = 10
oldCapacity >> 1 = oldCapacity / 2 = 5
newCapacity = 15

elementData = Arrays.copyOf(elementData, newCapacity);
```

数组扩容的时候，他是怎么扩的，老的大小 + 老的大小 >> 1（相当于除以2），实现了一个数组的拷贝

### 2.LinkedList

底层是基于链表来实现的

LinkedList的优点，就是非常适合各种元素频繁的插入里面去

LinkedList的缺点，不太适合在随机的位置，获取某个随机的位置的元素，比如LinkedList.get(10)，这种操作，性能就很低，因为他需要遍历这个链表，从头开始遍历这个链表，直到找到index = 10的这个元素为止

### 3.Vector和Stack

Stack代表了一个栈这种数据结构，他是继承自Vector来实现的，Vector是一种类似于ArrayList（基于数组来实现的）数据结构，有序的集合，Stack是一种基于数组来实现的栈数据结构

栈，先进后出

栈的压入栈底的时候，push()方法，几乎就跟ArrayList.add()方法的实现源码是一样的，就是放在数组的按顺序排列的位置上

ArrayList每次扩容是1.5倍，capacity + (capacity >> 1) = 1.5

Vector每次扩容默认是2倍，默认情况下是直接扩容两倍，2倍

pop()方法，从栈顶弹出来一个元素，最后一个压入栈的元素，会通过pop()方法弹出来，先是使用elementData[size - 1]获取最后一个元素，返回给用户，removeElementAt(size - 1)删除了最后一个元素

### 4.hashmap

数组+链表+红黑树的数据结构

红黑树

（1）红黑树是二叉查找树，左小右大，根据这个规则可以快速查找某个值

（2）但是普通的二叉查找树，是有可能出现瘸子的情况，只有一条腿，不平衡了，导致查询性能变成O(n)，线性查询了

（3）红黑树，红色和黑色两种节点，有一大堆的条件限制，尽可能保证树是平衡的，不会出现瘸腿的情况

（4）如果插入节点的时候破坏了红黑树的规则和平衡，会自动重新平衡，变色（红 <-> 黑），旋转，左旋转，右旋转

JDK 1.8，在链表长度为8以后，要链表 -> 红黑树，链表的遍历性能，时间复杂度是O(n)，红黑树是O(logn)，所以如果出现了大量的hash冲突以后，红黑树的性能比链表高得多，几倍到几十倍

JDK 1.8以后，hashmap的数据结构是，数组 + 链表 + 红黑树

核心成员变量的作用分析

static final int DEFAULT_INITIAL_CAPACITY = 1 << 4; // aka 16

应该是数组的默认的初始大小，是16，这个跟ArrayList是不一样的，初始的默认大小是10

static final float DEFAULT_LOAD_FACTOR = 0.75f;

这个参数，默认的负载因子，如果你在数组里的元素的个数达到了数组大小（16） * 负载因子（0.75f），默认是达到12个元素，就会进行数组的扩容

```html
static class Node<K,V> implements Map.Entry<K,V> {
        final int hash;
        final K key;
        V value;
        Node<K,V> next;
}
```

这是一个很关键的内部类，他其实是代表了一个key-value对，里面包含了key的hash值，key，value，还有就是可以有一个next的指针，指向下一个Node，也就是指向单向链表中的下一个节点

通过这个next指针，就可以形成一个链表

transient Node<K,V>[] table;

Node<K, V>[]，这个数组就是所谓的map里的核心数据结构的数组，数组的元素就可以看到是Node类型的，天然就可以挂成一个链表，单向链表，Node里面只有一个next指针

transient int size;

这个size代表的是就是当前hashmap中有多少个key-value对，如果这个数量达到了指定大小 * 负载因子，那么就会进行数组的扩容

final float loadFactor;，默认就是负载因子，默认的值是0.75f，你也可以自己指定，如果你指定的越大，一般就越是拖慢扩容的速度，一般不要修改

优化后的降低冲突概率的hash算法

map.put(key, value) -> 对key进行hash算法，通过hash获取到对应的数组中的index位置

在hashmap里面是优化过后的hash算法，高性能的

```html
public V put(K key, V value) {
        return putVal(hash(key), key, value, false, true);
}
```

hash(key)，对key进行hash获取一个对应的hash值，key、value传入到putVal()方法里面去，将key-value对儿根据其hash值找到对应的数组位置

hash(key)方法

```html
static final int hash(Object key) {
    int h;
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

将他的高16位和低16位进行一个异或运算

一般那个后面的位运算，一般都是用低16位在进行运算，所以说如果你不把hash值的高16位和低16位进行运算的话，那么就会导致你后面在通过hash值找到数组index的时候，只有hash值的低16位参与了运算，提前在hash()函数里面，就会把高16位和低16位进行一下异或运算，就可以保证说，在hash值的低16位里面，可以同时保留他的高16位和低16位的特征

这么做有什么好处呢？为什么要保证同时将高16位和低16位的特征同时纳入运算，考虑到数组index的定位中去呢？因为这样子可以保证降低hash冲突的概率，如果说直接用hash值的低16位去运算定位数组index的话，可能会导致一定的hash冲突。

很多key，可能值不同，但是hash值可能是相同的，如果key不同，但是hash值相同，或者是hash值不同，但是到数组的index相同，那么都会出现hash冲突，通过上面的这个操作，计算出来的hash值可以降低hash冲突概率

put操作原理以及hash寻址算法

数组刚开始的初始值，以及未来每次扩容的值，都是2的n次方

数组的大小就是2的n次方，只要保证数组的大小是2的n次方，就可以保证说，(n - 1) & hash，可以保证就是hash % 数组.length取模的一样的效果，也就是说通过(n - 1) & hash，就可以将任意一个hash值定位到数组的某个index里去

因为他不想用取模，取模的性能相对较低，这个是他的一个提升性能的优化点

i = (n - 1) & hash，i就是最后寻址算法获取到的那个hash值对应的数组的index

tab[i]不就是直接定位到了数组的那个位置了么？

刚开始肯定是空，就直接创建一个Node出来，代表了一个key-value对，放在数组的那个位置就可以了

hash冲突时的链表处理

两个key的hash值不一样，但是通过寻址算法，定位到了数组的同一个key上去，此时就会出现典型的hash冲突，默认情况下，会用单向链表来处理

if ((p = tab[i = (n - 1) & hash]) == null)

这个分支，他的意思是说tab[i]，i就是hash定位到的数组index，tab[i]如果为空，也就是hash定位到的这个位置是空的，之前没有任何人在这里，此时直接是放一个Node在数组的这个位置即可

如果进入else，就说明通过hash定位到的数组位置，是已经有了Node了

key不一样，出现了hash冲突，然后此时还不是红黑树的数据结构，还是链表的数据结构，在这里，就会通过链表来处理

```html
if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
    reeifyBin(tab, hash);
break;
```

这串东西的意思，就是说如果当前链表的长度（binCount），大于等于TREEIFY_THRESHOLD - 1的话，如果链表的长度大于等于8的话，链表的总长度达到8的话，那么此时就需要将这个链表转换为一个红黑树的数据结构

JDK 1.8引入红黑树优化hash冲突

如果链表长度太长的话，会导致有一些get()操作的时间复杂度就是O(n)，正常来说，table[i]数组索引直接定位的方式的话，O(1)

JDK 1.8以后人家优化了这块东西，会判断，如果链表的长度达到8的时候，那么就会将链表转换为红黑树，如果用红黑树的话，get()操作，即使对一个很大的红黑树进行二叉查找，那么时间复杂度会变成O(logn)，性能会比链表的O(n)得到大幅度的提升

当你遍历到第8个节点，此时binCount是7，同时你挂上了第9个节点，然后就会发现binCount >= 7，达到了临界值，也就是说，当你的链表节点的数量超过8的时候，此时就会将链表转换为红黑树

通过红黑树来解决hash冲突

如果此时在那个地方再次出现一个hash冲突的话，怎么办呢？此时就应该是在红黑树里插入一个节点了，不是说挂链表了，红黑树是一个平衡的二叉查找树，平衡的，插入的时候还挺复杂的，变色、旋转

e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);

hashmap扩容的原理

2倍扩容 + rehash，每个key-value对，都会基于key的hash值重新寻址找到新数组的新的位置

本来那个数组的长度假设是16，现在的话新数组的长度是32

本来所有的key的hash，对16取模是一个位置，比如说是index = 5；但是如果对32取模，可能就是index = 11,，位置可能变化

问你，hashmap扩容的原理，数组，一次扩容多大，2倍，rehash过程，基于key的hash值重新在新的数组里找到新的位置，很多key在新数组的位置都不一样了，如果是之前冲突的这个key可能就会在新的数组里分布在不同的位置上了

这个原理大体上是JDK 1.7以前的原理，现在的话呢，JDK 1.8以后，都是数组大小是2的n次方扩容，用的是与操作符来实现hash寻址的算法，来进行扩容的时候，rehash

JDK 1.8的高性能rehash算法

JDK 1.8以后，为了提升rehash这个过程的性能，不是说简单的用key的hash值对新数组.length取模，取模给大家讲过，性能较低，所以JDK 1.8以后hash寻址这块，统一都是用的这个位操作

如果数组的长度扩容之后 = 32，重新对每个hash值进行寻址，也就是用每个hash值跟新数组的length - 1进行与操作

JDK 1.8，扩容一定是2的倍数，从16到32到64到128，就可以保证说，每次扩容之后，你的每个hash值要么是停留在原来的那个index的地方，要么是变成了原来的index（5） + oldCap（16） = 21

扩容机制：数组2倍扩容，重新寻址（rehash），hash & n - 1，判断二进制结果中是否多出一个bit的1，如果没多，那么就是原来的index，如果多了出来，那么就是index + oldCap，通过这个方式，就避免了rehash的时候，用每个hash对新数组.length取模，取模性能不高，位运算的性能比较高

通过这个方式的话，可以有效的将原来冲突在一个位置的多个key，给分散到新数组的不同的位置去

### 5.LinkedHashMap

LinkedHashMap，他会记录你插入key-value的顺序， 如果你在遍历的时候，他是按照插入key-value对的顺序给你遍历出来的

LinkedHashMap是HashMap的一个子类，LinkedHashMap和TreeMap的区别，他们都可以维持key的顺序，只是LinkedHashMap底层是基于链表来实现的，TreeMap是基于红黑树来实现顺序的

LinkedHashMap其实原则上来说一些基本的原理和操作跟HashMap是差不多的，唯一主要的区别就是你在插入、覆盖、删除，他会记录一下key-value对的顺序，用一个链表来记录，在遍历的时候，就可以按照这个顺序来遍历

在调用LinkedHashMap的put()方法的时候，一定会调用到HashMap的put()方法里面去，调用完put()方法，插入一个key-value对之后，其实就会调用afterNodeInsertion(evict);，这个方法就会去回调LinkedHahsMap里面的子类的实现

如果我们是做key值的覆盖，可以看到，你多次覆盖一个值，不会改变他的顺序，LinkedHashMap有一个参数的，你可以在构造的时候传入进去，accessOrder，默认他是false，如果是默认为false的话，那么你比如说你get一个key，或者是覆盖这个key的值，都不会改变他在链表里的顺序

但是如果accessOrder是true的话，那么如果你get一个key，或者是覆盖这个key的值，就会导致个key-value对顺序会在链表里改变，他会被挪动到链表的尾部去，如果你把accessOrder指定为true，你每次修改一个key的值，或者是get访问一下这个key，都会导致这个key挪动到链表的尾部去

你删除某个元素的时候，就会将那个元素从链表里给摘除

在迭代的时候，LinkedHashMap里面会从链表的头部开始迭代，这样通过这个链表就可以维持他的一个顺序

### 6.TreeMap源码剖析：自定义排序规则的红黑树map数据结构

TreeMap，顾名思义，底层是基于红黑树做的数据结构，不是传统意义上的那红HashMap，他天然就可以按照你的key的自然顺序来排序，既然人家是按照key的大小来进行排序和迭代输出的

人家肯定是用了一个按照key大小排序的数据结构，我们可以想到的就是红黑树，所以他底层是基于红黑树来实现的按key排序就可以了

```html
static final class Entry<K,V> implements Map.Entry<K,V> {
        K key;
        V value;
        Entry<K,V> left;
        Entry<K,V> right;
        Entry<K,V> parent;
        boolean color = BLACK;
}
```

### 7.HashSet、LinkedHashSet、TreeSet的源码剖析

HashSet就是基于HashMap来实现的

HashMap是不允许key重复的，他底层是一个数组，如果你的key重复了，你会hash寻址到数组的同一个位置去，然后覆盖原来的值

HashSet，他其实就是说一个集合，里面的元素是无序的，他里面的元素是没有重复的，HashMap的key是无顺序的，你插入进去的顺序，跟你迭代遍历的顺序是不一样的，而且HashMap的key是没有重复的，HashSet是不是直接就可以基于HashMap来实现啊

你不断的往HashSet里放入一些元素，人家底层就是不断的put到HashMap里去就ok了，如果你是从HashSet里进行遍历，人家就是直接遍历HashMap的key就可以了

LinkedHashSet，他是有顺序的set，也就是维持了插入set的这个顺序，你迭代LinkedHashSet的顺序跟你插入的顺序是一样的，底层是不是直接就可以基于LinkedHashMap来实现的

TreeSet，默认是根据你插入进去的元素的值来排序的，而且可以定制Comparator，自己决定排序的算法和逻辑，他底层是不是可以基于TreeMap来实现

Set底层的Map，只有key是有值的，value都是null值，都是空的

HashSet底层是基于HashMap来实现的，所以底层也是有数组的，扩容的问题，你可以在构造HashSet的时候就传入数组的大小

面试的时候，可能会有人问到你Set底层的实现原理是什么呢？Map来实现的，其实就是在map的key里放置，set的源码没多少行代码，非常的简单的，value都是一个空的对象

LinkedHashSet.add()方法，底层会调用LinkedHashMap.put()方法，此时在这个方法里就会记住加入元素的顺序，在一个链表中，后面你遍历的时候，是从LinkedHashMap里遍历元素，人家是直接遍历维护好的链表的

### 8.Iterator迭代器应对多线程并发修改的fail fast机制

java集合中，迭代器在迭代的时候，他的fail-fast机制

ConcurrentModificationException，并发修改的异常，这个机制就叫做fail fast

modCount就是用来实现fail fast机制的，各个集合里面其实都有这个modCount的概念，只要这个集合被修改了，那么就会对modCount++

modificationCount，修改次数，只要你修改一次，就会更新这个，add、remove、set

比如说在迭代一个ArrayList之前，已经插入了4个元素，此时modCount = 4，在你获取和初始化一个迭代器的时候，里面的expectedModCount就会被初始化为modCount

throw new ConcurrentModificationException();，并发修改冲突异常

java集合包下的类，都是非线程安全的，所以说里面都设计了针对并发修改集合的问题，有fail fast机制，modCount

## 并发编程

### 1.进程

jvm进程里，是有很多线程的，首先第一个线程，你能看到的线程就是main线程

如果main线程执行完了以后，jvm进程默认就会直接退出

多线程并发运行的时候，本质是CPU在执行各个线程的代码，一个CPU会有一个时间片算法，他一会儿执行main线程，一会儿执行Thread线程，看起来两个线程好像是在同时运行一样

只不过CPU执行每个线程的时间特别短，可能执行一次就几毫秒，几微妙，你是感觉不出来的，看起来好像是多个线程并发在运行一样

### 2.多线程并发编程是个什么意思呢？

一个jvm进程里，你除了人家默认给你开启的这个main线程，你还可以在main线程里开启别的线程，比如说上面，你可以通过Thread类开启别的线程，别的线程是跟main线程同时在运行的

1、控制多线程实现系统功能

2、Java内存模型以及volatile关键字

3、线程同步以及通信

4、锁优化

5、并发编程设计模式：基于多线程实现复杂系统架构

6、并发包以及线程池

### 3.创建线程

1.Thread

```html
public class MyThread extends Thread {
    public void run() {
    }
}
new MyThread().start();
```

2.Runnable

```html
public class MyRunnable implements Runnable {
    public void run() {
    }
}
new Thread(new MyRunnable()).start();
```

### 4.daemon线程

默认创建的线程就是非daemon的，我们称之为工作线程，还有一个daemon线程的概念，这个意思是说，如果jvm里的工作线程都停止了，比如main线程之类的都执行完了

一般会将后台运行的线程设置为daemon线程，如果jvm里只剩下了daemon线程，那么就会进程退出，所有daemon线程一起销毁了，不会阻止jvm进程退出。所以我们应该将微服务存活状态监控的线程，设置为daemon线程，这样如果工作线程都死了，那么jvm也就退出了，daemon线程也销毁了

### 5.ThreadGroup

ThreadGroup就是线程组，其实意思就是你可以把一堆线程加入一个线程组里，那关键这个玩意儿有啥好处？好处大概就是，你可以将一堆线程作为一个整体，统一的管理和设置

实际上在java里，每个线程都有一个父线程的概念，就是在哪个线程里创建这个线程，那么他的父线程就是谁。举例来说，java都是通过main启动的，那么有一个主要的线程就是mian线程。在main线程里启动的线程，父线程就是main线程，就这么简单。

然后每个线程都必然属于一个线程组，默认情况下，你要是创建一个线程没指定线程组，那么就会属于父线程的线程组了，main线程的线程组就是main ThreadGroup。

线程组其实也有父线程组的概念，我们创建线程组的时候，如果没有手动指定他的父线程组，那么其实默认的父线程组就是main线程的线程组

默认线程会加入父线程的ThreadGroup，或者你自己手动创建ThreadGroup，ThreadGroup也有父ThreadGroup，ThreadGroup可以包裹一大堆的线程，然后统一做一些操作，比如统一复制、停止、销毁，等等

### 6.线程优先级

设置线程优先级，理论上可以让优先级高的线程先尽量多执行，但是其实一般实践中很少弄这个东西，因为这是理论上的，可能你设置了优先级，人家cpu结果也还是没按照这个优先级来执行线程

这个优先级一般是在1~10之间

而且ThreadGroup也可以指定优先级，线程优先级不能大于ThreadGroup的优先级

但是一般就是用默认的优先级就ok了，默认他会用父线程的优先级，就是5

### 7.Thread类的源码

Thread初始化

init(null, null, "Thread-" + nextThreadNum(), 0);

默认情况下，如果你不指定线程的名称，那么自动给你生成的线程名称就是，Thread-0，Thread-1，以此类推的一大堆的线程

Thread parent = currentThread();

你创建线程的时候，获取到的是currentThread()，是当前创建你的那个线程，比如说一般来说就是那个main线程，main线程在创建ServiceAliveMonitor线程，所以说此时创建线程的过程中，获取到的currentThread()就是main线程

threadGroup是不指定的，他就会自动给你处理一下，给你分配一个线程组，每个线程必须属于一个ThreadGroup的。如果你没有指定线程组，那么你默认的线程组就是父线程的线程组，如果你的父线程是main线程的话，那么你的线程组就是main线程的线程组（main线程组）

默认情况下，如果你没有指定你是否为daemon的话，那么你的daemon的状态是由父线程决定的，就是说如果你的父线程是daemon线程，那么你也是daemon线程；同理，你的优先级如果没有指定的话，那么就跟父线程的优先级保持一致

总结

（1）创建你的线程，就是你的父线程

（2）如果你没有指定ThreadGroup，你的ThreadGroup就是父线程的ThreadGroup

（3）你的daemon状态默认是父线程的daemon状态

（4）你的优先级默认是父线程的优先级

（5）如果你没有指定线程的名称，那么默认就是Thread-0格式的名称

（6）你的线程id是全局递增的，从1开始

线程启动过程

永远都不能对一个线程多次调用和执行start()方法，这个是不对的

if (threadStatus != 0)

throw new IllegalThreadStateException();

如果你的线程一旦执行过一次以后，那么他的threadStatus就一定会变为非0的一个状态，如果threadStatus是非0的状态，说明他之前已经被执行过了，所以这里会有一个判断，如果你对一个线程多次执行start()方法，人家会抛出一个异常，IllegalThreadStateException，非法的线程状态的异常。

group.add(this);

group就是之前给分配的，如果你自己指定了那么就是你自己创建的那个ThreadGroup，否则的话就是你的父线程的threadGroup，这行代码，其实就是将当前线程加入了他属于的那个线程组

private native void start0();

会结合底层的一些代码和机制，实际的启动一个线程

一旦是start0()成功的启动之后，他就会去执行我们覆盖掉的那个run()方法，或者是如果你传入进去的是那个Runnalbe对象，人家就会执行那个Runnable对象的方法

```html
@Override
public void run() {
    if (target != null) {
        target.run();
    }
}
```

如果你是

```html
new Thread(new Runnable() {
    public void run() {
    }
}).start();
```

传递进去了一个Runnable对象，就是在thread类里是target的东西，会判断一下，如果target不为null的话，那么此时就会执行target的run方法。反之，如果你是直接自己用Thread类继承了一个子类的话，那么你会重写这个run()方法，start0()启动线程之后，就会来执行你的run()方法

需要注意的几个点：

（1）一旦启动了线程之后，就不能再重新启动了，多次调用start()方法，因为启动之后，threadStatus就是非0的状态了，此时就不能重新调用了

（2）你启动线程之后，这个线程就会加入之前处理好的那个线程组中

（3）启动一个线程实际上走的是native方法，start0()，会实际的启动一个线程

（4）一个线程启动之后就会执行run()方法

### 8.join

join，概念

main线程里面，如果开启了一个其他线程，这个时候只要你一旦开启了其他线程之后，那么main线程就会跟其他线程开始并发的运行，一会执行main线程的代码，一会儿会执行其他线程的代码

main线程里面开启了一个线程，main线程如果对那个线程调用了join的方法，那么就会导致main线程会阻塞住，他会等待其他线程的代码逻辑执行结束，那个线程执行完毕，然后main线程才会继续往下走

### 9.interrupt

interrupt打断一个线程，其实是在修改那个线程里的一个interrupt的标志位，打断他以后，interrupt标志位就会变成true，所以在线程内部，可以根据这个标志位，isInterrupted这个标志位来判断，是否要继续运行

并不是说，直接interrupt一下某个线程，直接就不让他运行了

### 10.volatile

只要开了多个线程，一定会有一些这种问题，某个线程修改一个变量值，其他线程要立刻感知到这个变量值的变化，但是如果你不用volatile，会有问题

有线程修改了一个变量的值，结果其他的线程感知不到这个变量值的修改

volatile关键字，尽量让你修改了一个变量之后，其他的线程可以立即看到这个变量的最新的值

cpu缓存模型 -> java内存模型 -> 原子性、可见性、有序性 -> volatile的作用 -> volatile的底层原理 -> volatile实战

cpu缓存模型

cpu在他和主内存之间加了一层缓存，主内存的数据会被加载到cpu本地缓存里去，cpu后面会读写自己的缓存

cpu缓存模型，其实默认情况下是有问题的，特别是多线程并发运行的时候，导致说各个cpu的本地缓存，跟主内存，没有同步，一个数据，在各个地方，可能都不一样，就会导致数据的不一致

总线加锁机制

某个cpu如果要读一个数据，会通过一个总线，对这个数据加一个锁，其他的cpu就没法去读和写这个数据了，只有当这个cpu修改完了以后，其他cpu可以读到最新的数据

这个总线加锁机制，效率太差了，一旦说多个线程出现了对某个共享变量的访问之后，就会导致说，可能串行化的问题，多个cpu多线程并发运行的时候，效率很差

MESI协议

MESI协议，缓存一致性协议

修改本地缓存，立马刷主存，其他cpu本地缓存立马工期，重新从主存加载

:lock前缀指令 -> 内存屏障

内存模型

Java内存模型是跟cpu缓存模型是类似的，基于cpu缓存模型来建立的java内存模型，只不过java内存模型是标准化的，屏蔽掉底层不同的计算机的区别

线程的工作内存和主内存

read（从主存读取），load（将主存读取到的值写入工作内存），use（从工作内存读取数据来计算），assign（将计算好的值重新赋值到工作内存中），store（将工作内存数据写入主存），write（将store过去的变量值赋值给主存中的变量）

并发编程的三大特性

**（1）可见性**

**（2）原子性**

对于一个i++的操作，只要是多个线程并发运行来执行这行代码，其实的话，他都是不保证原子性的，如果保证原子性的，第一个线程i++，i = 1；第二个线程，i++，i = 2

volatile，就是保证线程之间可见性的一个关键字

**（3）有序性**

编译器和指令器，有的时候为了提高代码执行效率，会将指令重排序

volatile保证多线程的可见性

只要flag变成了1，然后线程不是要将flag = 1写回工作内存吗？assign操作，此时如果这个flag变量是加了volatile关键字的话，那么此时会这样子，就是说一定会强制保证说assign之后，就立马执行store + write，刷回到主内存里去

保证只要工作内存一旦变为flag = 1，主内存立马变成flag = 1

此外，如果这个变量是加了volatile关键字的话，此时他就会让其他线程的工作内存中的这个flag变量的缓存，会过期

线程2如果再从工作内存里读取flag变量的值，发现他已经过期了，此时就会重新从主内存里来加载这个flag = 1的值

通过volatile关键字，可以实现的一个效果就是说，有一个线程修改了值，其他线程可以立马感知到这个值

volatile无法保证原子性

volatile保证有序性

java中有一个happens-before原则：

编译器、指令器可能对代码重排序，乱排，要守一定的规则，happens-before原则，只要符合happens-before的原则，那么就不能胡乱重排，如果不符合这些规则的话，那就可以自己排序

程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作

锁定规则：一个unLock操作先行发生于后面对同一个锁额lock操作

volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作 ，volatile变量写，再是读，必须保证是先写，再读

传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C

线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作

线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生

线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行

对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始

上面这8条原则的意思很显而易见，就是程序中的代码如果满足这个条件，就一定会按照这个规则来保证指令的顺序。

但是如果没满足上面的规则，那么就可能会出现指令重排，就这个意思。这8条原则是避免说出现乱七八糟扰乱秩序的指令重排，要求是这几个重要的场景下，比如是按照顺序来，但是8条规则之外，可以随意重排指令。

volatile保证有序性，因为volatile要求的是，volatile前面的代码一定不能指令重排到volatile变量操作后面，volatile后面的代码也不能指令重排到volatile前面。

### 11.volatile的底层实现原理：lock指令以及内存屏障

（1）lock指令

对volatile修饰的变量，执行写操作的话，JVM会发送一条lock前缀指令给CPU，CPU在计算完之后会立即将这个值写回主内存，同时因为有MESI缓存一致性协议，所以各个CPU都会对总线进行嗅探，自己本地缓存中的数据是否被别人修改

如果发现别人修改了某个缓存的数据，那么CPU就会将自己本地缓存的数据过期掉，然后这个CPU上执行的线程在读取那个变量的时候，就会从主内存重新加载最新的数据了

lock前缀指令 + MESI缓存一致性协议

（2）内存屏障：禁止重排序

```html
Load1：
int localVar = this.variable
Load2：
int localVar = this.variable2 
```

LoadLoad屏障：Load1；LoadLoad；Load2，确保Load1数据的装载先于Load2后所有装载指令，他的意思，Load1对应的代码和Load2对应的代码，是不能指令重排的

```html
Store1：
this.variable = 1
StoreStore屏障
Store2：
this.variable2 = 2
```

StoreStore屏障：Store1；StoreStore；Store2，确保Store1的数据一定刷回主存，对其他cpu可见，先于Store2以及后续指令

volatile的作用是什么呢？

```html
volatile variable = 1
this.variable = 2 => store操作
int localVariable = this.variable => load操作
```

对于volatile修改变量的读写操作，都会加入内存屏障

每个volatile写操作前面，加StoreStore屏障，禁止上面的普通写和他重排；每个volatile写操作后面，加StoreLoad屏障，禁止跟下面的volatile读/写重排

每个volatile读操作后面，加LoadLoad屏障，禁止下面的普通读和voaltile读重排；每个volatile读操作后面，加LoadStore屏障，禁止下面的普通写和volatile读重排

### 12.syncrhonized

加锁，syncrhonized，一旦说某个线程加了一把锁之后，就会保证，其他的线程没法去读取和修改这个变量的值了，同一时间，只有一个线程可以读这个数据以及修改这个数据，别的线程都会卡在尝试获取锁那儿

修饰普通方法

直接synchronized修饰一个普通的方法，那么就是对当前这个对象实例在加锁，访问同一个对象实例的synchronized方法，同一时间只有一个线程可以做到

```html
synchronized(myObject) {
}
```

但是如果是两个线程，分别进入不同的对象的synchronized方法或者代码片段，这个没事，因为是在不同的对象上加锁。

其实synchronized一个代码片段，有更加常见的一种写法，就是用this，其实意思就是基于当前这个对象实例来加锁：

```html
synchronized(this) {
}
```

修饰静态方法

synchronized一个静态方法，就是对这个类的Class对象加锁，每个类都对应了一个Class对象，那么对同一个类的synchronized静态方法，同一时间只能有一个线程加锁进入其中

```html
synchronized(MyObject.class) {
}
```

### 13.synchronized底层原理

synchronized底层的原理，是跟jvm指令和monitor有关系的

你如果用到了synchronized关键字，在底层编译后的jvm指令中，会有monitorenter和monitorexit两个指令

```html
monitorenter
// 代码对应的指令
monitorexit
```

每个对象都有一个关联的monitor，比如一个对象实例就有一个monitor，一个类的Class对象也有一个monitor，如果要对这个对象加锁，那么必须获取这个对象关联的monitor的lock锁

monitor里面有一个计数器，从0开始的。如果一个线程要获取monitor的锁，就看看他的计数器是不是0，如果是0的话，那么说明没人获取锁，他就可以获取锁了，然后对计数器加1

这个monitor的锁是支持重入加锁的，如果一个线程第一次synchronized那里，获取到了myObject对象的monitor的锁，计数器加1，然后第二次synchronized那里，会再次获取myObject对象的monitor的锁，这个就是重入加锁了，然后计数器会再次加1，变成2

这个时候，其他的线程在第一次synchronized那里，会发现说myObject对象的monitor锁的计数器是大于0的，意味着被别人加锁了，然后此时线程就会进入block阻塞状态，什么都干不了，就是等着获取锁

接着如果出了synchronized修饰的代码片段的范围，就会有一个monitorexit的指令，在底层。此时获取锁的线程就会对那个对象的monitor的计数器减1，如果有多次重入加锁就会对应多次减1，直到最后，计数器是0

然后后面block住阻塞的线程，会再次尝试获取锁，但是只有一个线程可以获取到锁

### 14.wait与notify

wait与sleep的区别：前者释放锁，后者不释放锁

wait()，必须是有人notify唤醒他

wait(timeout)，阻塞一段时间，然后自己唤醒，继续争抢锁

wait与notify，必须在synchronized代码块中使用，因为必须是拥有monitor lock的线程才可以执行wait与notify操作

因此wait与notify，必须与synchornized一起，对同一个对象进行使用，这样他们对应的monitor才是一样的

notify()与notifyall()：前者就唤醒block状态的一个线程，后者唤醒block状态的所有线程

### 15.AtomicInteger

判断此时此刻是否是某个值，如果是，则修改，如果不是则重新查询一个最新的值，再次执行判断，这个操作叫做CAS，Compare and Set

Atomic原子类底层核心的原理就是CAS，无锁化，乐观锁，每次尝试修改的时候，就对比一下，有没有人修改过这个值，没有人修改，自己就修改，如果有人修改过，就重新查出来最新的值，再次重复那个过程

### 16.Unsafe类

Unsafe类是在JDK底层的一个类，而且的话人家限制好了，不允许你去实例化他以及使用他里面的方法的，首先人家的构造函数是私有化，不能自己手动去实例化他，其次，如果用Unsafe.getUnsafe()方法来获取一个实例

是不行的，在那个源码里，他会判断一下，如果当前是属于我们的用户的应用系统，识别到有我们的那个类加载器以后，就会报错，不让我们来获取实例

JDK源码里面，JDK自己内部来使用，不是对外的

Unsafe，封装了一些不安全的操作，指针相关的一些操作，就是比较底层了，主要就是Atomic原子类底层大量的运用了Unsafe

（1）volatile value

（2）Unsafe：核心类，负责执行CAS操作

（3）API接口：Atomic原子类的各种使用方式

### 17.AtomicInteger源码剖析

无限重复循环以及CAS操作

拿你刚刚获取到的那个l的值，他认为当前的value的值

去跟底层当前目前AtomicInteger对象实例中的value的值去进行比较，这就是compare的过程,如果是一样的话,就会set的过程，也就是将value的值给设置为：l（之前拿到的值） + 1（递增的值）

如果l（获取到的值），跟AtomicInteger + valueOffset获取到的当前的值，不一样的话，此时compareAndSwapInt方法就会返回false，while循环里拿到的是false的话，就会自动进入下一轮的循环

如果是成功的话，会返回一个l的值，是递增1之前的一个旧的值，所以会在外层方法中加1返回，告诉你当前累加1之后最新的值

底层CPU指令是如何实现CAS语义的

最最底层，用了一个native方法，不是java写的，走的是底层的c代码，可以通过发送一些cpu的指令，来确保说CAS的那个过程，绝对是原子的，具体是怎么来实现呢？以前的cpu会通过一些指令来锁掉某一小块的内存，后来会做了一些优化，他可以保证仅仅只有一个线程在同一时间可以对某块小内存中的数据，做CAS的操作

compare -> set，这是一系列的步骤，在执行这个步骤的时候，是每个线程都是原子的，有一个线程在执行CAS一系列的比较和设置的过程中，其他的线程是不能来执行的

cpu指令来实现

cpu会通过一些轻量级的锁小块内存的机制来实现

保证整个并发的性能要好的多

### 18.Atomic原子类体系的CAS语义存在的三大缺点分析

1、ABA问题：如果某个值一开始是A，后来变成了B，然后又变成了A，你本来期望的是值如果是第一个A才会设置新值，结果第二个A一比较也ok，也设置了新值，跟期望是不符合的。所以atomic包里有AtomicStampedReference类，就是会比较两个值的引用是否一致，如果一致，才会设置新值

假设一开始变量i = 1，你先获取这个i的值是1，然后累加了1，变成了2，但是在此期间，别的线程将i -> 1 -> 2 -> 3 -> 1， 这个期间，这个值是被人改过的，只不过最后将这个值改成了跟你最早看到的值一样的值，结果你后来去compareAndSet的时候，会发现这个i还是1，就将它设置成了2，就设置成功了

说实话，用AtomicInteger，常见的是计数，所以说一般是不断累加的，所以ABA问题比较少见

2、无限循环问题：大家看源码就知道Atomic类设置值的时候会进入一个无限循环，只要不成功，就不停循环再次尝试，这个在高并发修改一个值的时候其实挺常见的，比如你用AtomicInteger在内存里搞一个原子变量，然后高并发下，多线程频繁修改，其实可能会导致这个compareAndSet()里要循环N次才设置成功，所以还是要考虑到的。

JDK 1.8引入的LongAdder来解决，是一个重点，分段CAS思路

3、多变量原子问题：一般的AtomicInteger，只能保证一个变量的原子性，但是如果多个变量呢？你可以用AtomicReference，这个是封装自定义对象的，多个变量可以放一个自定义对象里，然后他会检查这个对象的引用是不是一个。

### 19.LongAdder

Java 8提供的一个对AtomicLong改进后的一个类，LongAdder

大量线程并发更新一个原子类的时候，天然的一个问题就是自旋，会导致并发性能还是有待提升，比synchronized当然好很多了

分段迁移，某一个线程如果对一个Cell更新的时候，发现说出现了很难更新他的值，出现了多次自旋的一个问题，如果他CAS失败了，自动迁移段，他会去尝试更新别的Cell的值，这样的话就可以让一个线程不会盲目的等待一个cell的值

### 20.AtomicStampedReference

ABA问题，反复修改几次，重新修改回了最早的值，你以为没改过，其实已经中间修改过多次了，AtomicStampedReference

一开始stamp = 0

获取到一个Applications对象（01），还有一个stamp = 0邮戳

尝试CAS，发现stamp = 0与实际的stamp = 2不符合，CAS操作失败

再次获取到Applications对象（01），此时stamp = 2，再次尝试CAS操作成功

此时别的线程，反复的修改了几次这个Applications对象，比如Applications对象（02），stamp = 1；又修改回了Applications对象（01），但是此时stamp = 2

### 21.AQS

AQS，AbstractQueuedSynchronizer，抽象队列同步器

ReentractLock、ReadWriteReentractLock，锁API底层都是基于AQS来实现的，一般我们自己不直接使用，但是是属于java并发包里的底层的API，专门支撑各种java并发类的底层的逻辑实现

### 22.ReentrantLock

```html
public ReentrantLock() {
        sync = new NonfairSync();
}
```

默认的构造函数这里，创建了一个Sync，NonfairSync，看起来是一个非常关键的组件，很可能是底层专门用于加锁和释放锁的核心组件

```html
public void lock() {
        sync.lock();
}
```

ReentrantLock在进行加锁的时候，他其实是直接基于底层的Sync来实现的lock操作，但是如果是这样子的话，ReentractLock这个类其实就是比较外层的一个薄薄的封装的一个类了，Sync就是ReentrantLock底层的核心组件

Sync：关键组件

abstract static class Sync extends AbstractQueuedSynchronizer

Sync是 一个抽象的静态内部类，子类？AQS：AbstractQueuedSynchronizer，这个东西，抽象队列同步器，是java并发包各种并发工具（锁、同步器）的底层的基础性的组件，核心的，主要是依赖于他

AQS里关键的一些东西，一个是Node（自定义数据结构，可以组成一个双向链表，也就是所谓的一个队列），state（核心变量，加锁、释放锁都是基于state来完成的）

Sync就是AQS（子类实现，多线程同步组件的意思）

NonfairSync是Sync的一个子类，覆盖重写了几个方法，没什么特别的东西在里面，大概代表了一个Sync的具体实现

ReentractLock -> synchronized（可重入加锁的）-> AQS -> NonfairSync（非公平的同步组件）

### 23.AQS底层

AQS底层加锁、释放锁，都是大量的基于CAS的操作来实现的，底层是基于 NonfairSync的 lock操作来实现加锁的

```html
final void lock() {
    if (compareAndSetState(0, 1))
        setExclusiveOwnerThread(Thread.currentThread());
    else
        acquire(1);
}
```

if (compareAndSetState(0, 1))：AQS里有一个核心的变量，state，代表了锁的状态；看一下state是否是0？如果是0的话，代表没人加过锁，此时我就可以加锁，把这个state设置为1

CAS可以无锁化的保证一个数值修改的原子性

compareAndSetState(0, 1)：相当于是在尝试加锁，底层原来是基于Unsafe来实现的，JDK内部使用的API，指针操作，基于cpu指令实现原子性的CAS，Atomic原子类底层也是基于Unsafe来实现的CAS操作

return unsafe.compareAndSwapInt(this, stateOffset, expect, update);

这行代码可以保证说，在一个原子操作中，如果发现值是我们期望的这个expect值，说明符合要求，没人修改过，此时可以将这个值设置为update，state如果是0的话，就修改为1，代表加锁成功了

这个操作是CAS原子性的

如果加锁成功了，compareAndSetState(0, 1)返回的是true，此时就说明加锁成功，他需要设置一下自己是当前加锁的线程

setExclusiveOwnerThread(Thread.currentThread());

设置当前线程自己是加了一个独占锁的线程，标识出来自己是加锁的线程

CAS -> AQS的state变量 -> exclusiveOwnerThread（当前加锁线程），Lock API性能比较好，CAS无锁化，乐观锁的思路，反复的比较

获取不到锁，处于等待状态的线程，会封装为一个Node，而且有指针，最后多个处于阻塞等待状态的线程可以封装为一个Node双向链表，JDK集合源码，就可以作为一个队列来实现了

### 24.公平锁

如果说你希望每个人过来都要按照顺序排队来加锁，公平锁，每个人先来后到，先来的人先加锁，公平性的，公平锁

```html
if (c == 0) {
                if (!hasQueuedPredecessors() &&
                    compareAndSetState(0, acquires)) {
                    setExclusiveOwnerThread(current);
                    return true;
                }
            }
```

公平锁的核心，就是一行代码，每次加锁的时候，都要先判断一下，如果前面没有排队等待的线程的话，我就尝试加锁，否则是不能尝试加锁的

h != t，如果h != t，说明head和tail不一样，如果一样代表了队列里有人在排队

如果说head的下一个节点是null，说明没人在排队，因为有一个是null，所以此时也是返回true；或者是s，也就是排在队头的节点，队头节点的线程如果不是当前线程，所以此时也是返回true

此时就是，会判断出来当前有人在排队，所以返回true

公平锁，任何一个线程过来会先判断一下，当前是否有人在排队，而且是不是自己在排队，如果不是的话，说明有别人在排队，此时自己不能尝试加锁，直接入队阻塞等待

先来后到的顺序，后来的人一定是排到队列里去等待

### 25.ReentrantLock

ReentrantLock，锁API，是如何基于AQS来实现的，读写锁，可以加读锁，也可以加写锁

但是，读锁和写锁是互斥的，也就是说，你加了读锁之后，就不能加写锁；如果加了写锁，就不能加读锁

但是如果有人加了读锁之后，别人可以同时加读锁

如果你有一份数据，有人读，有人写，如果你全部都是用synchronized的话，会导致如果多个人读，也是要串行化，一个接一个的读

我们希望的效果是多个人可以同时来读，如果使用读锁和写锁分开的方式，就可以让多个人来读数据，多个人可以同时加读锁

如果有人在读数据，就不能有人写数据，读锁 -> 写锁 -> 互斥

如果有人在写数据，别人不能写数据，写锁 -> 写锁 -> 互斥；如果有人在写数据，别人也不能读数据，写锁 -> 读锁 > 互斥

基于AQS的state二进制高低16位判断实现写锁的可重入加锁

写锁的可重入加锁，state的二进制高低16位的判断了

如果是int类型的state不是0的话，那么他的二进制数值，32位，低16位一定不是0，如果低16位不是0的话，就代表他是加过写锁的

c != 0，w == 0，c肯定不是0，但是低16位是0，说明有人加了读锁，没有人加写锁，此时你要加写锁，而且你还不是之前加锁的那个线程

c != 0，w != 0，有人加过锁，之前加的是写锁，但是当前线程不是之前加锁的线程，此时也不让你加写锁，同一个时间，只能有一个线程加写锁，如果线程1比如加了写锁，线程2也要加写锁，是不行的

c != 0，w != 0，之前有人加过写锁，而且加写锁的是你自己

如果加写锁的人是你自己，说明你就是在可重入的加写锁，将state += 1

c =1

w = 1，代表了c，int，32位，低16位的值，代表了写锁的可重入加锁次数

读写锁互斥：基于AQS的state二进制高低16位完成互斥判断

exclusiveCount(c)：获取的是state的低16位，代表写锁的状态值，如果不等于0，说明有人加了写锁，而且还不是你加的那个写锁，此时是不能加读锁的，在这里就形成了一个基于state的二进制高低16

写锁 -> 读锁的互斥

基于CAS实现多线程并发同时只有一个可以加读锁

int r = sharedCount(c);：拿到state的高16位的值，代表了读锁的加锁次数

### 26.ThreadLocal

ThreadLocal，线程本地副本

多线程并发安全问题主要出现在哪里呢？多个线程并发访问同一个共享数据的时候，才会有问题，java内存模型，并发修改同一个数据的时候，可能会导致数据错乱，必须要加一些并发同步机制

volatile可见性 -> 原子性，Atomic数值累加的原子性

共享数据吗？给每个线程拷贝一个线程自己本地的变量副本，每个线程就直接操作自己的本地副本就ok了，然后就跟其他的线程就没有冲突了

Long orderId，Long txid，数值，读一个数值 + 修改这个数值，每个线程就更新自己本地的数值就可以了，Long requestId，做成共享的，直接就每个线程自己维护一个副本，读取和更新自己的副本就可以了

避免多个线程并发的访问同一个共享的数据

源码分析

JDK里面的Thread类，内部有一个ThreadLocalMap内部类，代表了一个map，每个Thread线程对象自己内部就有一个核心的数据结构是map

这个map只能是某个线程自己内部可以使用的一份数据，是不是就是代表了线程本地的副本。一个Thread可以放多个ThreadLocal对应的本地变量副本

```html
Thread {
    ThreadLocalMap {
        ThreadLocal（requestId）: 1L,
        ThreadLocal（txid）：1L
    }
}
```

### 27.锁优化策略

标志位修改等可见性场景优先使用volatile

如果仅仅只是有一些线程会来写一个变量，标志位，另外一个线程是来读取这个标志位的值，那么此时优先使用volatile

能不用锁尽量别用锁，就会比较麻烦，可能会导致锁争用和冲突，如果没弄好的，锁的问题，可能会导致系统的吞吐量、性能会大幅度的降低

数值递增场景优先使用Atomic原子类

多线程并发访问一些共享数据，先分析和判断一下，那个变量是不是有人读，有人写，直接就是volatile就可以了。如果大家都要写，再判断一下，仅仅只是简单的数值累加或者变更，数值的一些操作

建议大家可以用 Atomic原子类，CAS机制，无锁化，并发性要比synchronized要好不少的，相当于简单的更新数值，都要一个一个线程依次加锁进入，修改，再释放锁，这个过程其实是并发性还是要差一些的

数据允许多副本场景优先使用ThreadLocal

如果你不需要多个线程共享读写一个数据的话，可以让每个线程保持一个本地变量的副本的话，那么你其实可以搞一个ThreadLocal，让每个线程都维护一个变量的副本，每个线程就操作自己本地的副本就可以了

txid，requestId，每个线程就把自己的那个值放到自己本地副本里，后续自己来修改和读取就可以了，跟其他的线程之间就没有什么冲突了

读多写少需要加锁的场景优先使用读写锁

多线程并发访问一块共享数据，就需要加锁了，优先考虑读写锁，synchronized重量级

读多写少的场景，读写锁分离，读锁 -> 大量的线程并发的读，写锁 -> 写数据其他人不能写同时来写，也不能有人来读

服务注册表的读写锁分离

读多，写少，每隔30秒拉拉取增量注册表，发送心跳，大多数的操作都是走读锁，并发的来实现读，synchronized的话，大量的读都是串行化，一个一个的读，导致服务注册中心的并发的性能很差

尽可能减少线程对锁占用的时间

读写锁用不了，synchronized锁，读写锁，加锁，有一个核心点，尽量保证你加锁的时间是很短的，不要在加锁之后，执行一些磁盘文件读写、网络IO读写，导致锁占用的时间过于长

一般 来说，我们建议，加锁，尽量就是操作一下内存里的数据就可以了，不要在锁里面去执行一些耗时的一些操作，比如说执行数据库操作，SQL，或者是别的一些东西，可能会导致占用锁的时间

就会导致线程并发的吞吐量大幅度的下降，并发能力就很弱，性能很差

尽可能减少线程对数据加锁的粒度

（1）分段加锁，实践，尽可能的减少一个线程占用锁的时间

（2）尽可能减少你对数据加锁的粒度

比方说，你手上有一份数据，里面包含了多个子数据，你加锁，可以对一整块完整的大数据来加锁，别人只要访问这一大块数据，都会有锁的争用的问题。你也可以选择降低加锁的粒度，你仅仅对大块数据里的部分子数据加锁

如果别的线程去请求这个大块数据里，其他的子数据的话，就不会跟你的锁产生冲突

更少的数据，或者是对更少的代码进行加锁

尽可能对不同功能分离锁的使用

就是如果可能的话，比如说你有一个锁，你看看能不能按照这个功能的不同，拆分为两把锁，在使用不同的功能的时候，可以用不同的锁，这样降低线程竞争锁的冲突。阻塞队列，人家在实现源码的时候，就使用了两把锁

队头是一把锁，队列尾巴是一把锁，你从队列尾巴插进去是加的一把锁，从队头消费数据使用的是另外一把锁，入队和出队的操作，就不会因为锁产生冲突了

避免在循环中频繁的加锁以及释放锁

如果是正常的代码流程，尽量避免在for循环里频繁的加锁释放锁

尽量减少高并发场景中线程对锁的争用

读写锁主要是解决了大量的读请求不会串行化，读请求可以并发起来

降低锁竞争的频率

系统刚启动的时候，会有一个线程来填充各级缓存的数据

此后的30秒内，大家全部都是读缓存数据的，不会涉及到任何加锁的行为

在这个过程中，如果有人更新注册表的数据的话，一方面会对注册表本身加写锁，另外一方面对缓存加一个锁，那么会过期掉readWriteMap的缓存。此时所有加的锁，是不会对高频的读请求有任何的锁的冲突和影响的

在写数据的期间，读数据不涉及任何读写锁的冲突，直接读的cache数据

有一个后台的线程，可能会过30秒之后，对缓存加个锁，然后同步两个缓存的数据，在这个过程中，实际上来说也是不会对高频的读操作施加任何的影响的

只有在此时，会有线程感知到缓存数据是null，重新填充数据，重新填充数据的时候，会涉及到重新从服务注册表查数据，然后加读锁，此时就是一个线程加了一个读锁，而且是很快的行为，大量的降低了频繁的读操作，可能频繁的跟写操作，读写锁冲突的问题

大幅度的降低了读写锁的互斥冲突的问题。避免了说高频的读操作，对服务注册表频繁的读取，频繁的加读锁，导致跟服务注册表的写锁频繁的互斥和冲突

各种锁的运用，加了两级缓存，尽可能的把读写锁的互斥和冲突，降低到了最低

为什么是两级缓存？不是一级缓存呢？

### 28.32位Java虚拟机中的long和double变量写操作为何不是原子的？

原子性这块，特例，32位虚拟机里的long/double类型的变量的简单赋值写操作，不是原子的，long i = 30，double c = 45.0，在32位虚拟机里就不是原子的，因为long和double是64位的

0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000

如果多个线程同时并发的执行long i = 30，long是64位的，就会导致有的线程在修改i的高32位，有的线程在修改i的低32位，多线程并发给long类型的变量进行赋值操作，在32位的虚拟机下，是有问题的

就可能会导致多线程给long i = 30赋值之后，导致i的值不是30，可能是-3333344429，乱码一样的数字，就是因为高低32位赋值错了，就导致二进制数字转换为十进制之后是一个很奇怪的数字

如果对变量加上了volatile，就可以保证在32位java虚拟机里面，对long/double变量的赋值写是原子的了

### 29.可见性涉及的底层硬件概念：寄存器、高速缓存、写缓冲器

每个处理器都有自己的寄存器（register），所以多个处理器各自运行一个线程的时候，可能导致某个变量给放到寄存器里去，接着就会导致各个线程没法看到其他处理器寄存器里的变量的值修改了

可见性的第一个问题，首先，就有可能在寄存器的级别，导致变量副本的更新，无法让其他处理器看到

然后一个处理器运行的线程对变量的写操作都是针对写缓冲来的（store buffer）并不是直接更新主内存，所以很可能导致一个线程更新了变量，但是仅仅是在写缓冲区里罢了，没有更新到主内存里去

这个时候，其他处理器的线程是没法读到他的写缓冲区的变量值的，所以此时就是会有可见性的问题，这是第二个可见性发生的场景

然后即使这个时候一个处理器的线程更新了写缓冲区之后，将更新同步到了自己的高速缓存里（cache，或者是主内存），然后还把这个更新通知给了其他的处理器，但是其他处理器可能就是把这个更新放到无效队列里去，没有更新他的高速缓存

此时其他处理器的线程从高速缓存里读数据的时候，读到的还是过时的旧值

可见性发生的问题

如果要实现可见性的话，其中一个方法就是通过MESI协议，这个MESI协议实际上有很多种不同的时间，因为他不过就是一个协议罢了，具体的实现机制要靠具体底层的系统如何实现

根据具体底层硬件的不同，MESI协议的实现是有区别的

比如说MESI协议有一种实现，就是一个处理器将另外一个处理器的高速缓存中的更新后的数据拿到自己的高速缓存中来更新一下，这样大家的缓存不就实现同步了，然后各个处理器的线程看到的数据就一样了

为了实现MESI协议，有两个配套的专业机制要给大家说一下：flush处理器缓存、refresh处理器缓存。

flush处理器缓存，他的意思就是把自己更新的值刷新到高速缓存里去（或者是主内存），因为必须要刷到高速缓存（或者是主内存）里，才有可能在后续通过一些特殊的机制让其他的处理器从自己的高速缓存（或者是主内存）里读取到更新的值

除了flush以外，他还会发送一个消息到总线（bus），通知其他处理器，某个变量的值被他给修改了

refresh处理器缓存，他的意思就是说，处理器中的线程在读取一个变量的值的时候，如果发现其他处理器的线程更新了变量的值，必须从其他处理器的高速缓存（或者是主内存）里，读取这个最新的值，更新到自己的高速缓存中

所以说，为了保证可见性，在底层是通过MESI协议、flush处理器缓存和refresh处理器缓存，这一整套机制来保障的

要记住，flush和refresh，这两个操作，flush是强制刷新数据到高速缓存（主内存），不要仅仅停留在写缓冲器里面；refresh，是从总线嗅探发现某个变量被修改，必须强制从其他处理器的高速缓存（或者主内存）加载变量的最新值到自己的高速缓存里去

内存屏障的使用，在底层硬件级别的原理，其实就是在执行flush和refresh，MESI协议是如何与内存屏障搭配使用的（flush、refresh）

volatile boolean isRunning = true;

isRunning = false; => 写volatile变量，就会通过执行一个内存屏障，在底层会触发flush处理器缓存的操作；while(isRunning) {}，读volatile变量，也会通过执行一个内存屏障，在底层触发refresh操作

之前给大家讲过那个volatile关键字的作用，对一个变量加了volatile修饰之后，对这个变量的写操作，会执行flush处理器缓存，把数据刷到高速缓存（或者是主内存）中，然后对这个变量的读操作，会执行refresh处理器缓存，从其他处理器的高速缓存（或者是主内存）中，读取最新的值

当然跟我们之前说的有一点点不一样，因为之前说的是写volatile变量的时候，一个是强制刷主内存，一个是过期掉其他处理器的高速缓存中的数据；读volatile变量的时候，会发现高速缓存中的值过期，然后强制从主内存加载最新值

其实这个东西吧，你没发现么，效果是一样的，他其实本质都是让一个线程写了volatie变量之后，另外一个变量立马可以读到volatile变量的值，只不过MESI协议的底层具体实现，根据cpu等硬件的不同，有多种不同的实现方式罢了

### 30.乱序执行机制

指令不一定说是拿到了一个指令立马可以执行的，比如有的指令是要进行网络通信、磁盘读写，获取锁，很多种，有的指令不是立马就绪可以执行的，为了提升效率，在现代处理器里面都是走的指令的乱序执行机制

把编译好的指令一条一条读取到处理器里，但是哪个指令先就绪可以执行，就先执行，不是按照代码顺序来的。每个指令的结果放到一个重排序处理器中，重排序处理器把各个指令的结果按照代码顺序应用到主内存或者写缓冲器里

这就导致处理器可能压根儿就是乱序在执行我们代码编译后的指令

### 31.synchronized锁同时对原子性、可见性以及有序性的保证

原子性、可见性、有序性，三块东西，都重新从比较细节和底层的层面，都在硬件的级别去给大家说了一下，到底是怎么回事，为什么会发生这个问题，从底层的层面来说了一下，以及大体上有没有什么办法可以来解决这些问题

原子性，基本的赋值写操作都是可以保证原子性的，复杂的操作是无法保证原子性的

可见性，MESI协议、flush、refresh，配合起来，才可以解决可见性

有序性，三个层次，最后一个层次有4种重排（LoadLoad、StoreStore、LoadStore、StoreLoad）

synchronized关键字，同时可以保证原子性、可见性以及有序性的

原子性的层面而言，他加了以后，有一个加锁和释放锁的机制，加锁了之后，同一段代码就只有他可以执行了

可见性，可以保证可见性的，他会通过加入一些内存屏障，他在同步代码块对变量做的写操作，都会在释放锁的时候，全部强制执行flush操作，在进入同步代码块的时候，对变量的读操作，全部会强制执行refresh的操作

更新的数据，别的线程只要进入代码块，就一定可以读到的

有序性，synchronized关键字，他会通过加各种各样的内存屏障，来保证说，解决LoadLoad、StoreStore等等重排序

### 32.Store内存屏障

按照可见性来划分的话，内存屏障可以分为Load屏障和Store屏障。

Load屏障的作用是执行refresh处理器缓存的操作，说白了就是对别的处理器更新过的变量，从其他处理器的高速缓存（或者主内存）加载数据到自己的高速缓存来，确保自己看到的是最新的数据。

Store屏障的作用是执行flush处理器缓存的操作，说白了就是把自己当前处理器更新的变量的值，都刷新到高速缓存（或者主内存）里去

在monitorexit指令之后，会有一个Store屏障，让线程把自己在同步代码块里修改的变量的值都执行flush处理器缓存的操作，刷到高速缓存（或者主内存）里去；然后在monitorenter指令之后会加一个Load屏障，执行refresh处理器缓存的操作，把别的处理器修改过的最新值加载到自己高速缓存里来

所以说通过Load屏障和Store屏障，就可以让synchronized保证可见性。

按照有序性保障来划分的话，还可以分为Acquire屏障和Release屏障。

在monitorenter指令之后，Load屏障之后，会加一个Acquire屏障，这个屏障的作用是禁止读操作和读写操作之间发生指令重排序。在monitorexit指令之前，会加一个Release屏障，这个屏障的作用是禁止写操作和读写操作之间发生重排序。

所以说，通过 Acquire屏障和Release屏障，就可以让synchronzied保证有序性，只有synchronized内部的指令可以重排序，但是绝对不会跟外部的指令发生重排序。

synchronized：

（1）原子性：加锁和释放锁，ObjectMonitor

（2）可见性：加了Load屏障和Store屏障，释放锁flush数据，加锁会refresh数据

（3）有序性：Acquire屏障和Release屏障，保证同步代码块内部的指令可以重排，但是同步代码块内部的指令和外面的指令是不能重排的

在volatile变量写操作的前面会加入一个Release屏障，然后在之后会加入一个Store屏障，这样就可以保证volatile写跟Release屏障之前的任何读写操作都不会指令重排，然后Store屏障保证了，写完数据之后，立马会执行flush处理器缓存的操作

在volatile变量读操作的前面会加入一个Load屏障，这样就可以保证对这个变量的读取时，如果被别的处理器修改过了，必须得从其他处理器的高速缓存（或者主内存）中加载到自己本地高速缓存里，保证读到的是最新数据；在之后会加入一个Acquire屏障，禁止volatile读操作之后的任何读写操作会跟volatile读指令重排序

那个Acquire屏障其实就是LoadLoad屏障 + LoadStore屏障，Release屏障其实就是StoreLoad屏障 + StoreStore屏障

### 33.深入分析缓存一致性协议

因为有高速缓存的存在，所以就导致各个处理器可能对一个变量会在自己的高速缓存里有自己的副本，这样一个处理器修改了变量值，别的处理器是看不到的，所以就是为了这个问题引入了缓存一致性协议（MESI协议）

MESI协议规定：对一个共享变量的读操作可以是多个处理器并发执行的，但是如果是对一个共享变量的写操作，只有一个处理器可以执行，其实也会通过排他锁的机制保证就一个处理器能写

之前说过那个cache entry的flag代表了缓存数据的状态，MESI协议中划分为：

（1）invalid：无效的，标记为I，这个意思就是当前cache entry无效，里面的数据不能使用

（2）shared：共享的，标记为S，这个意思是当前cache entry有效，而且里面的数据在各个处理器中都有各自的副本，但是这些副本的值跟主内存的值是一样的，各个处理器就是并发的在读而已

（3）exclusive：独占的，标记为E，这个意思就是当前处理器对这个数据独占了，只有他可以有这个副本，其他的处理器都不能包含这个副本

（4）modified：修改过的，标记为M，只能有一个处理器对共享数据更新，所以只有更新数据的处理器的cache entry，才是exclusive状态，表明当前线程更新了这个数据，这个副本的数据跟主内存是不一样的

MESI协议规定了一组消息，就说各个处理器在操作内存数据的时候，都会往总线发送消息，而且各个处理器还会不停的从总线嗅探最新的消息，通过这个总线的消息传递来保证各个处理器的协作

下面来详细的图解MESI协议的工作原理，处理器0读取某个变量的数据时，首先会根据index、tag和offset从高速缓存的拉链散列表读取数据，如果发现状态为I，也就是无效的，此时就会发送read消息到总线

接着主内存会返回对应的数据给处理器0，处理器0就会把数据放到高速缓存里，同时cache entry的flag状态是S

在处理器0对一个数据进行更新的时候，如果数据状态是S，则此时就需要发送一个invalidate消息到总线，尝试让其他的处理器的高速缓存的cache entry全部变为I，以获得数据的独占锁。

其他的处理器1会从总线嗅探到invalidate消息，此时就会把自己的cache entry设置为I，也就是过期掉自己本地的缓存，然后就是返回invalidate ack消息到总线，传递回处理器0，处理器0必须收到所有处理器返回的ack消息

接着处理器0就会将cache entry先设置为E，独占这条数据，在独占期间，别的处理器就不能修改数据了，因为别的处理器此时发出invalidate消息，这个处理器0是不会返回invalidate ack消息的，除非他先修改完再说

接着处理器0就是修改这条数据，接着将数据设置为M，也有可能是把数据此时强制写回到主内存中，具体看底层硬件实现

然后其他处理器此时这条数据的状态都是I了，那如果要读的话，全部都需要重新发送read消息，从主内存（或者是其他处理器）来加载，这个具体怎么实现要看底层的硬件了，都有可能的

这套机制其实就是缓存一致性在硬件缓存模型下的完整的执行原理

写缓冲器和无效队列

MESI协议如果每次写数据的时候都要发送invalidate消息等待所有处理器返回ack，然后获取独占锁后才能写数据，那可能就会导致性能很差了，因为这个对共享变量的写操作，实际上在硬件级别变成串行的了

所以为了解决这个问题，硬件层面引入了写缓冲器和无效队列

写缓冲器的作用是，一个处理器写数据的时候，直接把数据写入缓冲器，同时发送invalidate消息，然后就认为写操作完成了，接着就干别的事儿了，不会阻塞在这里。接着这个处理器如果之后收到其他处理器的ack消息之后

才会把写缓冲器中的写结果拿出来，通过对cache entry设置为E加独占锁，同时修改数据，然后设置为M

其实写缓冲器的作用，就是处理器写数据的时候直接写入缓冲器，不需要同步阻塞等待其他处理器的invalidate ack返回，这就大大提升了硬件层面的执行效率了

包括查询数据的时候，会先从写缓冲器里查，因为有可能刚修改的值在这里，然后才会从高速缓存里查，这个就是存储转发

引入无效队列，就是说其他处理器在接收到了invalidate消息之后，不需要立马过期本地缓存，直接把消息放入无效队列，就返回ack给那个写处理器了，这就进一步加速了性能，然后之后从无效队列里取出来消息，过期本地缓存即可

通过引入写缓冲器和无效队列，一个处理器要写数据的话，这个性能其实很高的，他直接写数据到写缓冲器，发送一个validate消息出去，就立马返回，执行别的操作了；其他处理器收到invalidate消息之后直接放入无效队列，立马就返回invalidate ack

### 34.内存屏障在硬件层面的实现原理

可见性问题：

Store屏障 + Load屏障

如果加了Store屏障之后，就会强制性要求你对一个写操作必须阻塞等待到其他的处理器返回invalidate ack之后，对数据加锁，然后修改数据到高速缓存中，必须在写数据之后，强制执行flush操作

他的效果，要求一个写操作必须刷到高速缓存（或者主内存），不能停留在写缓冲里

如果加了Load屏障之后，在从高速缓存中读取数据的时候，如果发现无效队列里有一个invalidate消息，此时会立马强制根据那个invalidate消息把自己本地高速缓存的数据，设置为I（过期），然后就可以强制从其他处理器的高速缓存中加载最新的值了

这就是refresh操作

为了解决有序性问题

内存屏障，Acquire屏障，Release屏障，但是都是由基础的StoreStore屏障,StoreLoad屏障，可以避免指令重排序的效果

StoreStore屏障，会强制让写数据的操作全部按照顺序写入写缓冲器里，他不会让你第一个写到写缓冲器里去，第二个写直接修改高速缓存了

resource = loadResource();

StoreStore屏障

loaded = true;

StoreLoad屏障，他会强制先将写缓冲器里的数据写入高速缓存中，接着读数据的时候强制清空无效队列，对里面的validate消息全部过期掉高速缓存中的条目，然后强制从主内存里重新加载数据

a = 1; // 强制要求必须直接写入高速缓存，不能停留在写缓冲器里，清空写缓冲器里的这条数据

int b = c;

### 35.Java虚拟机对锁的优化：锁消除、锁粗化、偏向锁、自旋锁

从JDk 1.6开始，JVM就对synchronized锁进行了很多的优化

synchronized说是锁，但是他的底层加锁的方式 可能不同，偏向锁的方式来加锁，自旋锁的方式来加锁，轻量级锁的方式来加锁

这些东西本身你只要了解一个概念就可以了，JDK 1.6开始对synchronized关键字做过哪些优化，有哪些加锁的方式，效果是什么，作用是什么，在实际的开发和使用中，根本就不需要你去过多的care一些东西

synchronized(this) {

}

（1）锁消除

锁消除是JIT编译器对synchronized锁做的优化，在编译的时候，JIT会通过逃逸分析技术，来分析synchronized锁对象，是不是只可能被一个线程来加锁，没有其他的线程来竞争加锁，这个时候编译就不用加入monitorenter和monitorexit的指令

这就是，仅仅一个线程争用锁的时候，就可以消除这个锁了，提升这段代码的执行的效率，因为可能就只有一个线程会来加锁，不涉及到多个线程竞争锁

（2）锁粗化

synchronized(this) {

}

synchronized(this) {

}

synchronized(this) {

}

这个意思就是，JIT编译器如果发现有代码里连续多次加锁释放锁的代码，会给合并为一个锁，就是锁粗化，把一个锁给搞粗了，避免频繁多次加锁释放锁

（3）偏向锁

这个意思就是说，monitorenter和monitorexit是要使用CAS操作加锁和释放锁的，开销较大，因此如果发现大概率只有一个线程会主要竞争一个锁，那么会给这个锁维护一个偏好（Bias），后面他加锁和释放锁，基于Bias来执行，不需要通过CAS

性能会提升很多

但是如果有偏好之外的线程来竞争锁，就要收回之前分配的偏好

可能只有一个线程会来竞争一个锁，但是也有可能会有其他的线程来竞争这个锁，但是其他线程唉竞争锁的概率很小

如果有其他的线程来竞争这个锁，此时就会收回之前那个线程分配的那个Bias偏好

（4）轻量级锁

如果偏向锁没能成功实现，就是因为不同线程竞争锁太频繁了，此时就会尝试采用轻量级锁的方式来加锁，就是将对象头的Mark Word里有一个轻量级锁指针，尝试指向持有锁的线程，然后判断一下是不是自己加的锁

如果是自己加的锁，那就执行代码就好了

如果不是自己加的锁，那就是加锁失败，说明有其他人加了锁，这个时候就是升级为重量级锁

（5）适应性锁

这是JIT编译器对锁做的另外一个优化，如果各个线程持有锁的时间很短，那么一个线程竞争锁不到，就会暂停，发生上下文切换，让其他线程来执行。但是其他线程很快释放锁了，然后暂停的线程再次被唤醒

也就是说在这种情况下，线程会频繁的上下文切换，导致开销过大

所以对这种线程持有锁时间很短的情况，是可以采取忙等策略的，也就是一个线程没竞争到锁，进入一个while循环不停等待，不会暂停不会发生线程上下文切换，等到机会获取锁就继续执行好了

一直追问我，什么自旋锁，不是什么事儿，当然，如果要站在jvm的底层层面，去说清楚的话，确实是比较复杂的，但是我觉得起码目前为止，暂时也没必要，各种锁底层是如何来实现的，完全可以等到以后jvm那块都讲过之后

再回过头来深入jvm底层的原理来剖析：偏向锁、自旋锁、轻量级锁，jvm层面的概念，栈侦，Load Record，不一定能听懂，基础的知识没有铺垫好，需要通过调节jvm的一些参数来优化底层synchronized里的各种加锁方式的使用

这样可以大幅度减少线程上下文的切换，而这种自旋等待获取锁的方式，就是所谓自旋锁，就是不断的自旋尝试获取锁

如果一个线程持有锁的时间很长，那么其他线程获取不到锁，就会暂停，发生上下文切换，让其他线程来执行，这种自己暂停获取锁的方式，就是所谓的重量级锁

这个根据不同情况自动调整的过程，就是适应锁的意思

### 36.CountDownLatch：同步等待多个线程完成任务的并发组件

如果你的一个线程启动了多个线程来执行一些任务，此时你的这个线程需要同步阻塞等待那些线程都执行完毕了，才可以继续往下走，此时可以用CountDownLatch

```html
protected int tryAcquireShared(int acquires) {
            return (getState() == 0) ? 1 : -1;
        }

```

刚开始的时候，state = 2，此时调用这个方法，state != 0，所以会返回-1

此时会将当前线程放入AQS的等待队列中，入队去等待

此时的话，main线程会直接通过park操作挂起，阻塞住，不能干任何事情了，就等待别人把他从队列里来唤醒

（1）state != 0

（2）将main线程封装为一个node，加入AQS的等待队列

（3）调用LockSupport.park()操作，挂起main线程

countDown()：其实就是把AQS的state给减1，走CAS来设置state的值；如果CAS成功了，就看看当前的state的值是不是0，如果是0就返回true，如果不是0，就返回false

第一个线程把state - 1，此时state = 1，此时这个返回的是false，相当于别的什么都不干了，就是仅仅把state的值给减1就可以了

如果是第二个线程再次来coutnDown操作，state = 0，如果此时state是0的话，就会触发一段逻辑，肯定会从AQS的等待队列中，唤醒之前等待的main线程，让他开始往下去执行，不要继续阻塞了

（1）await()，触发了一个线程入队阻塞等待

（2）countDown()，如果state == 0，唤醒队列里等待的所有的线程

（3）所有线程被唤醒，发现state == 0，就从await()方法里退出

### 37.CyclicBarrier：将工作任务给多线程分而治之的并发组件

如果所有线程都执行完之后，最开始传递进去的那个线程会通过Condition.signalAll()，唤醒在队列里排队的所有的线程，那些线程都会依次来尝试获取lock锁，因为那些线程此时被唤醒过后，都会进入Lock锁的AQS锁等待队列里去

### 38.Semaphore：等待指定数量的线程完成任务的并发组件

state，一开始是你构造Semaphore的时候初始化传递的一个0

直接就将state - permits = 0 - 1 = -1 = remaining

此时通过state的操作，发现remaining = -1，小于0 的，此时就会将当前线程封装为一个节点加入AQS等待队列里去，而且会park挂起线程

### 39.Exchange：支持两个线程之间进行数据交换的并发组件

### 40.HashMap为什么在高并发下会出现死循环的问题？

第一步：多线程并发操作的时候，会导致一个环形链表

两个线程此时同时进来要插入一个元素，如果此时同时触发了resize的过程，同时进行hashMap的扩容

JDK 1.7的hashmap扩容的源码，两个线程同时执行的时候，是如何导致环形链表的。某一个线程会把自己的newTable赋值给table，作为hashMap内部的数组

此时我要是来get一个值，get(k5)，k5的hash取模算法会定位到那个环形链表的位置，遍历环形链表，而且因为环形链表里可能没有k5的值，所以会导致在环形链表中无法找到对应的值来返回

间接的就会导致死循环那个环形链表，不停在里面遍历，无法退出，导致cpu直接100%，线上系统的各个线程可能都会被这个get操作给卡死

丢失数据：如果是线程1的newTable赋值给了hashmap里的table，采用了线程1的newTable之后，就会导致说，此时会导致<k3,v3>这条数据就永久丢失了，可能会被垃圾回收掉，同学

hashmap不是线程安全的，导致一些数据的问题，死循环、丢失数据，也是一个问题

JDK 1.8以后的hashmap，重构了这个算法，采用了红黑树的数据结构，优化了数组扩容的过程，他就不会再有死循环的问题

### 41.ConcurrentHashMap

ConcurrentHashMap，分段加锁，把一份数据拆分为多个segment，对每个段设置一把小锁，put操作，仅仅只是锁掉你的那个数据一个segment而已，锁一部分的数据，其他的线程操作其他segmetn的数据，跟你是没有竞争的

segment01，加锁；segment02，segment03。多个线程并发操作hashmap的效率

JDK 1.8的ConcurrentHashMap，优化了，锁粒度细化，他是就一个Node[]数组，正常会扩容的，但是他的锁粒度是针对的数组里的每个元素，每个元素的处理会加一把锁，不同的元素就会有不同的锁

大幅度的提升了多线程并发写ConcurrentHashMap的性能，降低了锁的冲突

### 42.CopyOnWriteArrayList：线程安全的List数据结构

CopyOnWriteArrayList，写时复制机制的ArrayList，可以保证线程并发的安全性

增删改的时候，都是先复制一个数组出来，对新的复制数组进行修改，最后将修改好的新数组设置为底层数组

写数据的时候一定要CopyOnWrite，如何解决读写并发的问题，写数据的时候，如何安全的读数据，ConcurrentHashMap里面是直接操作一个数组的，对数组读写全部是走的volatile的操作

CopyOnWrite机制，写副本数组，跟读就没关系了，只要写完成之后，走一个volatile写，设置最新的数组，自然读操作就会读到最新数组的元素了，只有一个线程可以写，但是写的同时可以允许大量的线程来并发读

CopyOnWriteArrayList：弱一致性

多个线程并发的读写list，中间一定是有一段时间，是复制数组被修改好了，还没设置给array；但是此时其他线程读到的都是老数组的数据，这个过程中，多个线程看到的数据是不一致的，人家修改了数据没有立马被人读到

弱一致性 -> 最终一致性

优点：读和写不互斥的，写和写互斥，同一时间就一个人可以写，但是写的同时可以允许其他所有人来读；读和读也是并发的；读写锁机制还要好；他也不涉及到Unsafe.getObjectVolatile

使用场景：多线程并发安全性，可以选用他；尽可能是读多写少的场景，大量的读是不被影响的；可能有一个线程刚刚发起了写，此时别的线程读到的还是旧的数据，也有这种可能，还好

ArrayList，synchronized(list)，ReadWriteLock来操作这个ArrayList

缺点：空间换时间，写的时候，经常内存里会出现复制出来的一模一样的副本，对内存消耗过大，副本机制保证了保证读写并发优化，大量的并发读不需要锁互斥，list如果很大，可能你要考虑在线上运行的时候，可能经常

内存占用会是list大小的几倍

### 43.ConcurrentLinkedQueue：线程安全的链表结构无界队列

ConcurrentLinkedQueue，队列，无界，队列的大小是不限制的

基于CAS实现的线程并发安全性，非阻塞

LinkedQueue，底层一定是基于链表来实现的，所以一定会有Node类数据结构，Node指向对方串成一个链表，单向的，双向的，head和tail两个指针都是指向了链表中的头节点和尾节点

if (p.casNext(null, newNode)) {

这是一个CAS操作，就是把空节点的next指针指向了新的节点，同一时间只有一个线程可以执行成功这个操作

ConcurrentLinkedQueue.offer，保证多线程并发安全的奥秘，就在于一行代码：

if (p.casNext(null, newNode)) {

上面那行代码保证了是通过CAS操作把元素入队，同一时间只有一个线程可以入队成功的，其他的线程就会入队失败，此时会不断的for循环，变换指针，尝试让指针往后挪动到最后一个尾部节点

再次尝试用CAS将自己入队，加入队尾

上面的过程会通过for循环保证不断的尝试直到成功为止

poll，从队头出队

if (item != null && p.casItem(item, null)) {

基于CAS操作，将队列的头部元素的item值设置为null，因为是CAS操作，出队的操作只能是同一时间就一个线程可以成功

通过一堆指针的变化，找到队列的头结点，通过CAS保证只有一个线程可以执行出队的操作，队列头部Node的item设置为null，原来的head Node给脱离出队列让jvm gc掉，head指针指向最新的头部的null节点

最后就是返回一个“张三”值，队列头部的值

同一时间多个线程都执行poll操作的时候，会如何？

模拟两个线程同时来执行poll出队的操作，线程1执行成功了CAS操作，开始出队；对于线程2来说，线程2通过指针的变化指向了最新的一个头节点，再次尝试用CAS操作进行出队

多个线程同时出队的时候，通过CAS + for，可以保证线程并发安全性，一个头节点同一个时间只能被一个线程出队，出队了之后，别的线程出队失败，再次尝试对最新的一个头节点尝试出队

线程安全的无界内存队列，ConcurrentLinkedQueue，给大家分析清楚了，入队+出队都是基于CAS来做的，保证同一时间只有一个线程可以入队或者是出队，其他线程如果入队或者出队失败了，会通过for循环再次尝试入队或者是出队

整个过程没有任何锁，全部是CAS，非阻塞的无界队列，多线程并发操作的性能会比用锁的那种机制会更好

### 44.线程安全的有界队列：LinkedBlockingQueue

无界队列：ConcurretedLinkedQueue，边界，大小限制，他就是一个单向的链表，无限制的往里面去存放数据，再从队列里获取数据

不停的疯狂的往无界队列里塞入数据的话，可能会导致内存溢出

提了一个所谓的面试题，使用无界队列的线程池，在远程服务异常情况下，会有什么问题。不停的可以往线程池里扔任务，搞一个线程请求远程服务，就导致请求卡死，一直阻塞无法请求通

就会导致不停的增长线程，最后线程超多，一直到把内存撑爆

有界队列：LinkedBlockingQueue，链表，有界，有大小限制，如果超过了限制，你往队列里塞数据就会被阻塞住，就不让你往队列里塞数据了，好处就在于说可以限制内存队列的大小，避免说内存队列无限制的增长，最后撑爆内存

### 45.队列满时入队线程是如何阻塞以及被唤醒的呢？

blocking queue，阻塞，队列满需要阻塞线程不让放了，队列空需要阻塞线程不让获取

如果队列满了以后，是如何阻塞线程的；后续如果有人从队列里take掉了一个元素，队列不满了，是如何唤醒那个线程的

队列满了以后，直接会调用put锁对应的condition进行阻塞等待的操作，当前线程被挂起，释放锁，进入condition等待队列

只要发现队列不满了，比如说有人take掉了一个元素，此时是不是就可以唤醒处于阻塞的线程重新获取put锁再去执行入队的操作

假设count = 10，此时getAndDecrement()，获取到的c = 10，但是count = 9

此时c = capacity，也就代表的意思是什么呢？本次take之前队列是满的，但是本次take之后队列肯定不是满的了。主要队列之前是满的，很可能有人在put的时候处于阻塞的状态，此时就可以把处于put阻塞状态的线程唤醒

你在执行put操作的时候，每次执行完了put操作之后，c + 1（代表了当前的count），小于capacity的话，就说明当前的队列没满，此时就可以也是执行一下put锁等待对了的signal方法尝试唤醒一个阻塞的线程

### 46.那么如果队列是空的又如何阻塞出队线程以及唤醒呢？

如果队列是空的，此时线程如何被阻塞，后续如果队列有数据了是如何唤醒的

如果有人put了一个数据到队列里去，很明显应该唤醒take阻塞的线程，通知人家说现在可以获取数据了

c = 0，count = 1

如果此时c是0的话，代表的意思就是说，之前队列是空的，如果队列是空的，就可能有线程执行take操作被阻塞住了，此时就尝试唤醒take阻塞的线程

每次take完了一条数据，你都需要判断一下，c > 1，c == 2，我本次take之前，队列里起码有2条数据，那么我本次take完了之后，队列里起码还有一条数据，只要队列里起码还有1条数据的话

我此时就可以尝试对notEmpty换新一个take阻塞的线程

### 47.服务注册中心集群的三层队列异步同步机制

基于无界CAS无锁化队列实现第一层队列

ConcurrentLinkedQueue，无界队列，纯CAS实现的无锁机制，多线程并发的性能很高

LinkedBlockingQueue，有界队列，基于两把独占锁实现的，多线程并发性能并不是很好，但是可以实现指定大小的有界的限制，避免内存无限制的膨胀，而且还可以针对有界这个特点，实现写入和获取的阻塞

第一层队列一定是高并发的入队的，所以用LinkedBlockingQueue未必合适

第一层队列是有界还是无界的，如果做成有界的，限制大小，万一有点什么意外，可能会导致在有界队列这里直接打满队列，会导致整个服务注册中心实例所有线程全部阻塞在这里，假死的状态

对于这种情况而言，用无界队列实现第一层队列，更加合适一些，这个是对eureka做的一点改进

使用第二层队列基于时间间隔和数量生成同步batch

LinkedBlockingQueue，一方面是独占锁，有界的，对第二个队列的入队和出队的操作都是少数的后台线程在执行，并不是大量线程高并发在写入的

有界的特性，避免说在异常情况下的过度内存消耗和膨胀，是合理的，是可行的

参照eureka，按照时间来设置一个interval，每隔500ms打包成一个batch

实现第三层队列用于存放以及发送同步batch

每个AbstractRequest会占用多少字节的内存，假设大概来说一个request，占用30个字节

平均一个batch包含100个request，3000个字节，3kb

10000 -> 30mb

### 48.基于数组实现的有界队列：ArrayBlockingQueue

### 49.优先级队列和延迟队列

优先级队列：PriorityBlockingQueue，你可以给队列里的每个元素都安排一个优先级，他的话内部会基于二叉树的数据结构来存储，根据优先级对队列中的元素进行排序，你出队的时候是按照优先级的大小来出队的

延迟队列：DelayQueue，你可以给这个压入队列的每个元素设置一个delay延迟时间，压入队列一个元素，delay time是500ms，那么必须等500ms过后这个元素才能从队列里出队的方法里获取到

java并发包下面的，我个人是不打算给大家来讲解和分析的，这两个东西真的很少很少，在单机场景下，对单个虚拟机而言，内部用优先级队列和延迟队列，是非常少见的，MQ那块，消息中间件那块

MQ那块，很多技术方案都会依赖于优先级队列和延迟队列来实现的

### 50.线程池

4种，fixed（固定数量的线程），cached（线程数量不固定，无论来多少任务都是不停的创建线程来执行，如果线程空闲了一定的时间，就会释放掉这个线程），single（线程池里就只有一个线程），scheduled（提交进去的线程，会在指定的时间过后才去执行）

最最常见的线程池就是fixed线程池，线程数量是固定的，不会超过这个数量的线程，就执行你不断提交的任务

fixed线程池

fixed线程池，搞了一堆固定数量的线程，配合了一个无界队列来处理你提交的任务，最多无论什么时候，只能有你指定数量的一些线程来处理任务。如果线程池里所有的线程都在繁忙的过程中，处理任务。

此时的话呢，就只能再次提交任务的时候，把任务给压入无界队列中等待

如果线程池里的某个线程挂掉了，此时他会自己启动一个新的线程加入到线程池里去

线程池里的线程会一直存活在线程池里，等待处理新提交过来的任务，直到你关闭这个线程池

ThreadPoolExecutor才是真正代表的是线程池管理器，管理了一个线程池，内部持有一个线程池，相当于创建了一个线程池

```html
java.util.concurrent.ThreadPoolExecutor.ThreadPoolExecutor(
int corePoolSize, 
int maximumPoolSize, 
long keepAliveTime, 
TimeUnit unit, 
BlockingQueue<Runnable> workQueue
)
 
```

corePoolSize：线程池里应该有多少个线程

maximumPoolSize：如果线程池里的线程不够用了，等待队列还塞满了，此时有可能根据不同的线程池的类型，可能会增加一些线程出来，但是最多把线程数量增加到maximumPoolSize指定的数量

keepAliveTime + TimeUnit：如果你的线程数量超出了corePoolSize的话，超出corePoolSize指定数量的线程，就会在空闲keepAliveTime毫秒之后，就会自动被释放掉

workQueue：你的线程池的等待队列是什么队列

threadFactory：在线程池里创建线程的时候，你可以自己指定一个线程工厂，按照自己的方式创建线程出来

RejectedExecutionHandler：如果线程池里的线程都在执行任务，然后等待队列满了，此时增加额外线程也达到了maximumPoolSize指定的数量了，这个时候实在无法承载更多的任务了，此时就会执行这个东西

线程池工作流程

线程数量 < corePoolSize

此时都会直接创建新线程放入线程池中，同时新线程负责执行我自己这个提交过过来的任务

线程池里的线程数量 >= corePoolSize

把当前的线程给压入到等待队列里去排队

排队过后做一些额外的处理，如果线程池已经关闭了，此时要从对了里把刚加入的任务给出队，线程池是空的，此时会创建一个线程放进去，还会处理一些额外的情况

如果线程池的线程数量 >= corePoolSize，无法创建新的线程，此时尝试入队等待也失败，可能队列满了，此时就会再次尝试创建新的线程（根据maximumPoolSize来了，尝试在创建额外的线程出来）

但是，如果创建额外线程都失败了，此时就会走reject策略，拒绝你提交任务

超过corePoolSize数量的线程，在keepAliveTime的时间范围内都空闲之后，比如说60s，额外创建的线程就会被回收掉

### 51.如何通过非阻塞的高性能CAS实现安全的线程创建

他一定会反复确保如果当前线程数量已经超过了corePoolSize之后，是绝对不让你创建新的core线程放入线程池的，直接会返回false，也就是代表着addWorker方法失败了，无法创建新core线程放入线程池中

if (compareAndIncrementWorkerCount(c))

如果当前线程数量 < corePoolSize的话，就会新创建一个线程出来放入线程池，在此之前先会尝试用CAS操作递增线程数量，如果说CAS成功的话，就可以在后面去创建一个新的线程，放入线程池

假设此时corePoolSize是4，线程数量是3，可能会有不同的线程来执行任务的提交，此时通过CAS可以保证安全的递增线程数量，此时只能有一个线程是CAS递增线程数量成功的，其他线程CAS会失败

如果说CAS成功了，就代表可以继续往下走，去真的创建一个线程出来

如果CAS失败了，再次for循环，判断线程数量是否小于corePoolSize，如果是的话，再次尝试进行CAS递增

CAS安全的来递增当前线程的数量

### 52.如何基于独占锁安全的将线程放入池子中以及启动任务

如果基于加独占锁来安全的创建线程以及放入线程池里，启动这个新的线程

Worker就是工作线程，他是一个AQS的子类，本身自己就可以直接基于AQS来实现独占锁的机制，负责执行你提交的Runnable任务，是一个组件，里面保存了一个核心的state，代表了工作线程的状态

Worker内部是基于threadFactory（有一个默认的线程工厂），你也可以自己手动指定一个线程工厂，此时就会按照线程工厂的策略来创建一个线程，放在Worker的内部，代表了执行任务的工作线程

ThreadPoolExecutor，核心的数据结构，就是维护了一堆线程的线程池，如果要操作这个核心数据结构，就必须要在ThreadPoolExecutor的层面加一个独占锁，此时只能是一个线程尝试来操作核心数据结构

### 53.提交到线程池的任务是如何完成执行以及指标统计的

CAS递增线程数量 -> 创建Worker（theadFactory，AQS）-> 加入线程池（HashSet，独占锁） -> start线程 -> 执行提交的Runnable任务

Worker内部的thread线程，是通过threadFactory构造出来的，就是一个最最普通的thread，一旦启动之后，就会执行自己内部的Runnable业务逻辑，也就是我们传递进去的那个Runnable业务逻辑

只要thread start成功之后，这个提交任务到线程池的方法逻辑就结束掉了

Worker执行一个任务的时候，会通过自己的AQS机制更新一下自己的状态，相当于更新自己当前执行的线程是谁。state = 1，state = 0

### 54.核心线程满了之后如何将任务压入近乎无界的队列等待

比如说线程池里的线程数量已经是4了，再次提交第5个任务的时候，就会发现线程数量 = corePoolSize了，就不会再次创建新的线程了，入队等待的时候，走的是LinkedBLockingQueue的offer()方法

offer()方法实际上并不是阻塞的，非阻塞的

如果此时队列满，不是卡住等待人家take掉一个元素，而是直接返回false

使用的是近乎于无界的队列，所以基本上可以认为你offer入队的时候，永远是返回true，入队永远不会有满一说，入队都是成功的，可以无限的不停的入队

### 55.线程池中的工作线程如何从队列获取任务来执行

只要线程池的线程数量达到了corePoolSize之后，接下来任务都是直接入队的，无界队列，不停的入队，线程池里启动的线程是如何不停的从队列里获取任务来执行的呢

如果要允许线程的超时，两个条件满足一个就可以：配置项（allowCoreThreadTimedOut），当前线程数量超过了corePoolSize

allowCoreThreadTimedOut，默认是false，不允许core线程因为获取不到任务就超时退出；如果你设置为true的话，就会导致线程池中的core线程，如果超过一定的时间获取不到队列中的任务

就会认为是core线程空闲了一段时间，此时core线程就会允许退出，自己释放掉自己

当前线程数量是否超过了corePoolSize的数量，默认情况下是不会的，CAS保证在corePoolSize范围内的线程数量，但是cached线程池的时候，他是允许创建额外的非core线程，最多是达到maximumPoolSize的数量

超过了corePoolSize数量的额外的线程，都是允许timeout，也就是说，如果一定时间范围内没有从队列获取到任务，说明空闲了一段时间，此时就自动释放掉

fixed线程池中，暂时你可以认为，正常情况下是不会出现这个情况的，线程数量就是corePoolSize的数量

Runnable r = timed ?

workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) :

workQueue.take();

fixed线程池的线程在从队列获取任务的时候，两个条件都不会满足，此时就是做的阻塞式的方法从LinkedBlockingQueue里获取任务

### 56.面试题：无界队列线程池在远程服务异常时内存飙升怎么办

最后可能会导致JVM OOM，系统崩溃

如果要解决这个问题从哪个角度来切入，你可以考虑自己定制线程池，使用有界队列，不要使用无界队列，可以限制内存空间的使用，避免系统崩溃；无界队列，直接支撑前端的请求，不允许有界阻塞，到最后大不了就是系统崩溃

远程服务异常的话，可能会导致另外一个问题，你的线程不停的异常报错和崩溃退出

如果你提交到线程池的任务报错了，抛了异常出来，在线程池执行的过程中，Worker层面接收到一个异常，会直接throw抛出去

只要有一个Worker异常挂掉，此时就会把这个worker从线程池里给挪出去，然后判断一下，如果当前线程数量 < corePoolSize，就会重新创建一个Worker和线程放入线程池中，自己再搞一个补位

### 57.线程池关闭的过程中会涉及到什么？

关闭这个线程池，在整个系统都关闭的时候会来关闭这个线程池

Worker本身是一个AQS，执行任务的时候，state = 1；执行完一个任务，state = 0

尝试将Worker的state = 1，如果成功了，就说明这个Worker当前的state = 0，说明这个Worker当前是空闲的状态，没有执行任何一个任务，此时就可以中断掉这个Worker，实际上来说Worker内部的线程就会退出

正在执行任务的Worker，tryLock失败了，state = 1，不要去中断他了，让他执行完这个任务再说

任务队列里还有一些任务，等待那些任务会被执行完毕，把队列里的任务都执行完毕，如果队列里的任务都执行完毕了之后，再次执行getTask()的时候会有问题

### 58.不限制线程数量的线程池：cached线程池

提交任务到cached线程池时会先触发线程的创建吗？

corePoolSize：0

maximumPoolSize：Integer.MAX_VALUE

keepAliveTime：60s

workQueue：SynchronousQueue

刚开始线程数量是0，此时是不是小于corePoolSize呢？当然不是了，corePoolSize = 0，所以此时第一种情况的条件是不会满足的

以上述方法构造出来的线程池，他在提交任务的时候是不会上来就直接创建一个线程的，解决了一个疑问：cached线程池，他是不会上来直接创建线程的，会走第二种情况，将任务压入队列中

第一个任务到cached线程池的入队失败以及非core线程执行

SynchronousQueue，入队

刚开始提交第一个任务的时候，SynchronousQueue的源码反复盘查了两遍，基本上可以确认说没有线程在等待获取任务的时候，入队直接是返回false的，不让你入队成功。必须要有人在等待获取任务，才能入队成功

就导致直接创建一个非core线程，Worker，core是true还是false，只不过是用来决定是跟corePoolSize对比还是maximumPoolSize对比，几乎可以认为永远可以不停的添加非core线程

直接就会把你的第一个线程给他执行完毕

cached线程池，提交第一个任务的时候，就是直接创建一个非core线程给你执行了

### 59.线程池已有线程在执行任务时再次提交任务会如何执行呢？

SynchronousQueue的原理，TransferStack来进行数据传递

put + take的方式，才能实现他原本希望实现的一个效果，如果put的时候没有人在take，此时就会将head指针指向put操作，put线程就park挂起

再次另外一个线程来put，head指针指向最新的线程put的数据，因为没有人take，所以也是挂起

他其实是用来干两个线程之间的数据传递的同步，put + take，take的时候，如果没人put，此时你会阻塞，直到有人put，是最后一次take的人先可以获取到别人put的数据，栈的效果

put的时候，如果没人take，此时你会阻塞住，多人put，最后一个put是在栈顶，此时有人take，先获取最后一次put的数据，栈，后进先出，queue，反过来，先入先出，默认的实现是栈的实现

SynchronousQueue的offer，他是不会阻塞的，如果没有take，offer的时候直接就是返回null，就是入队失败

此时如果有已经有一个非core线程在执行任务，再来第二个任务要提交，此时会如何呢？当前线程数量 = 1，corePoolSize = 0，1 < 0？不成立，所以此时还是不会直接创建一个core线程出来

此时还是会直接创建一个非core的线程来直接执行提交的任务

### 60.非core线程执行完之后如何尝试从队列获取下一个任务

当前线程数量，一定是大于corePoolSize的，corePoolSize是0，无论你有多少个线程，都是属于非core线程，都会大于，这里的timed就是true

非core线程在从队列获取任务的时候走的是poll，非阻塞的，而且有超时时间，keepAliveTime，如果超过指定时间没获取到任务，此时就会当前线程就会自动释放掉，此时会进入SynchronousQueue的栈

他会位于栈顶，但是挂起当前线程的时间超时时间是60s，超过60s的话，就认为此次poll就失败了，null，此时就会返回一个null，此时就自动触发这个非core线程的释放了

### 61.cached线程池有线程的时候提交任务到队列又如何被执行

如果没有线程在从队列获取任务，此时无论你提交多少任务，SynchronousQueue入队都是失败的，offer都是返回false，此时都会直接创建线程来执行任务的，但是如果有线程空闲出来就会尝试从队列poll任务

如果队列为空，此时最多会等待60s空闲去poll一个任务出来，如果超过了60s没有poll到任务，此时这个线程自己就会释放掉

此时你提交任务会如何呢？

此时会通过SynchronousQueue实现一个match配对，offer入队的任务会给最后一个poll任务的线程去执行，如果有线程在poll队列的话，那么你入队都是可以成功的，会交给某个线程来执行的

### 62.不停往cached线程池提交任务时会导致CPU负载过高吗？

答案是：会，就可能会在系统高峰期导致大量的线程被创建出来，然后就是导致机器的CPU负载过高，更有甚至，就是线程太多导致内存溢出，线程太多了，导致CPU负载飙升。队列无限增长，内存飙升

### 63.cached线程池又会不会触发拒绝任务提交的机制呢？

答案是：不会

第一种fixed，是基于有限固定数量的线程处理源源不断涌入的任务，但是呢，无界队列，所以任务可以无限制的涌入和排队

第二种cached，是在需要的时候无限制的创建新的线程来处理新的任务，提交的任务几乎是不会排队的，永远能最快速度的得到执行，入队的时候先看看有没有人空闲在poll，如果有立马执行

4核8G，虚拟机，一般来说，线程池开启线程来异步处理任务，200以内，100~200的时候，线程机器的CPU负载就很高了，内存队列排队个几十万个任务，也还好，内存也没撑爆，但是如果你的线程一旦达到四五百个，线上机器的CPU负载过高的报警

### 64.如何根据系统的业务场景需求定制自己的线程池

如果你的业务场景有一些需求的话，你完全可以自己直接构造ThreadPoolExecutor的线程池，传入一些参数符合你自己的业务需求的，fixed和cached、scheduled可以满足大部分场景的线程池的使用了

fixed，但是你希望队列是有界的，此时你就可以自己定制了

corePoolSize：线程池里的线程是0，少于这个数量，他会自动创建这个数量的线程的，只要线程池里的线程是少于corePoolSize，他后面就会以take阻塞的方式从队列里获取任务，这些线程是不会释放了

任务一定会直接入队，此时你是用什么队列？无界队列，有界队列，同步队列，LinkedBlockingQueue和SynchronousQueue，如果你看一些开源项目里对线程池的定制的源码的话

有界队列的话，此时如果队列满了，入队失败，就会尝试以非core方式创建线程，直接执行任务，最多线程池里的线程不能超过maximumPoolSize，如果线程池里的线程总数一旦超过了corePoolSize之后

超出corePoolSize数量的那些线程，在从队列获取的时候是走的poll + 超时时间（keepAliveTime）的方式，也就是如果一定时间空闲获取不到任务，自动退出释放线程，维持空闲的线程数量在小于等于corePoolSize

如果等待队列也满了，而且线程数量达到了maximumPoolSize，此时会直接执行reject策略，拒绝你提交任务

创建线程的时候，可以用自己的threadFactory，定制你的线程是不是要设置为daemon，需要不需要线程组的概念