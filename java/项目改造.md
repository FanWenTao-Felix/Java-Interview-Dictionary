# 项目改造

### 1.初步改造方案

1.技术方案

eureka，ribbon + feign，zuul（默认就整合ribbon和hystrix）

2.微服务网关层

我们使用zuul来构建微服务架构的网关，起码部署两台机器，双机部署是最最起码的，这样子可以保证如果一台机器挂了，还有另外一台机器可以用，我们在生产系统里面，每个服务，最最起码是双机部署

3.业务服务层

一共将整个电商系统拆分为15个服务，每个服务部署两台机器，双机部署，避免单点故障，如果只部署一台机器，挂了整个就挂了，单点故障。

4.数据存储层

所有的服务都共用一个数据存储，也就是一个数据库，这个数据库就部署在一台机器上，刚开始是不做分库分表的，然后的话呢，就是在数据库服务器中，创建15个逻辑库，在一个MySQL中，创建15个database。

在微服务架构中，每个服务都是自己的代码仓库，每个服务都是自己的数据库，逻辑库，一个MySQL中的一个database。后面如果压力上来了，你就可以将不同的服务的数据库迁移到不同的机器上去。

5.服务注册中心

采用Eureka来作为服务注册中心，每个服务都是一个Eureka Client，会自动往Eureka Server去进行服务注册，eureka server是双机部署的，这样的话呢可以避免单点故障

6.服务调用

采用ribbon + feign进行服务调用，feign进行生命式的服务调用，ribbon进行服务调用的负载均衡

7.开发框架

每个服务的业务逻辑的开发，统一采用SSM框架（spring mvc + spring + mybatis + spring boot）

### 2.技术思考

1、服务注册中心

eureka server来做的，我们要来思考服务注册和发现的核心运行过程中，有哪些地方是需要我们来配置和调整的

1.1 eureka server的请求压力

他是双机部署，然后eureka server两台机器互相之间都会进行注册，完成eureka server集群的一个识别和构造

eureka server在线上的生产环境是否要部署很多台机器，没什么太大的必要，eureka server的设计原则，是纯粹基于自己的内存来设计的，也就是不会比如说依赖数据库，或者是网络请求，所以eureka server纯内存操作，都是很快的

每一台eureka server的机器都可以承载很多的并发请求，一台普通的4核8G的机器，部署eureka server，每秒钟接收个几百的请求都是问题不大的

你觉得eureka server的压力大吗？不大，其实各个服务都是eureka client，eureka client其实每隔30秒来一次心跳的请求，所以这个压力其实一点儿都不大，比如说你有100个服务，每隔服务部署2台机器，就有200个服务实例，30秒有200个心跳请求，每秒呢？每秒大概也就8~10个心跳请求

1.2 服务注册的时效性

spring cloud对eureka做了额外的封装，只要服务一旦启动，立马就会发出注册的请求，时效性基本在毫秒级

1.3 服务发现的时效性

服务刚启动的时候，发现其他所有服务的时效性

**服务刚启动的时候，立马发现其他所有服务的时效性是毫秒级，立马就会去发送一个请求，拿到所有的注册表**

如果说其他服务加了一台机器，此时这个服务要发现别的服务新增了一台机器，要过多久可以发现

eureka server的多级缓存的机制

比如说，现在服务A新增了一台机器，会更新到注册表中去，而且会立马过期掉原有的缓存，会立马过期掉这个ReadWriteCacheMap

很多人会天真的认为，服务A新增了一台机器，其他服务最多30秒之内一定会感知到服务A新增了一台机器，你如果考虑到eureka server内部的多级缓存的机制，你就会发现说，其实在极端情况下，服务A新增了一台机器，可能会要1分钟的时间，60s的时间，才能让其他服务感知到

每个服务拉取增量注册表的间隔，就是在初始化调度任务的代码那里看一下，默认的时间是多少，看看我们可以通过设置哪个参数来改变这个时间

int registryFetchIntervalSeconds = clientConfig.getRegistryFetchIntervalSeconds();

默认情况下，就是30秒，spring-cloud-netflix-eureka-client工程里的EurekaClientConfigBean里面，定义了所有参数的默认值，如果你要设置，就是eureka.client作为前缀，然后加上变量的名字，就可以设置自己的数值

eureka-core工程，ResponseCacheImpl，在spring-cloud-netflix-eureka-server工程里的EurekaServerConfigBean，里面对所有的参数都进行了定义，默认情况下也是30秒

如果要修改，eureka.server.responseCacheUpdateIntervalMs = 10000

**eureka服务发现是分钟级的时效性**

1.4 服务心跳的间隔

任何一个服务启动之后都会定时发送心跳的通知，通知eureka server自己还是存活的状态

心跳的间隔是多长时间

在spring-cloud-netflix-eureka-client工程里，有一个EurekaInstanceConfigBean，这个里面就定义了默认的发送心跳的时间，就是30秒

eureka.client.leaseRenewalIntervalInSeconds = 30

1.5 服务故障的自动感知的时效性

服务故障了，eureka server过了多久可以感知到服务故障了，然后将这个服务给他下线呢？

首先在eureka server中，是每隔60s去执行一次evict task，去判断一下当前所有的服务实例，是否有的服务实例出现了故障，一直没有发送心跳过来，是否要将故障的服务实例给他下线呢

eureka.server.evictionIntervalTimerInMs = 60 * 1000

按照这套evict task执行的机制，还有eureka本身的一个bug（90s * 2），很可能在极端情况下，从一个服务宕机之后，到evict task发现和判定这个服务已经故障，可能最多要差不多4分钟，最少也得2分钟~3分钟

发现故障了以后，从服务注册表中摘除，然后过期掉readWriteCacheMap缓存

加上两个缓存map的同步，以及其他服务30s一次增量拉取的成本，很可能在极端情况下，服务B是要过了将近5分钟才能感知到服务A的某台机器故障，宕机了。即使不在极端情况下，其他服务要感知到某个服务实例的故障，起码也要三分钟~四分钟

**我们一般都是按照极端情况来计算的，服务实例故障，自动感知的时效性，服务A某台机器宕机了，服务B感知服务A那台机器宕机了，时效性在5分钟以内，起码三四分钟**

引申出来的一个问题，如果说服务A某台机器宕机了，要3分钟以后服务B才能感知到，此时3分钟以内，服务A请求服务B的那台机器就会是失败的，此时怎么办呢？所以才要做超时设置（多少秒之内请求不成功就timeout），失败重试（服务A的一台机器挂了，你可以重试服务A的其他机器）

eureka.instance.leaseExpirationDurationInSeconds = 90

1.6 服务下线的感知的时效性

服务正常下线的话，会怎么样呢？这块的时效性，跟之前说的那个服务新增了一台机器，新增了一个服务实例，那么一般感知时效性在1分钟以内。**如果服务正常下线，执行DiscoveryClient的shutown()方法，此时其他服务感知到这个服务实例下线，也是在1分钟以内。**

1.7 eureka server的自我保护的稳定性

在eureka的自我保护机制的代码里，大量的运用了hard code硬编码，惨痛一点，他默认你的心跳的时间间隔是30s，一分钟就2次心跳，也就是说你压根儿就不能去修改心跳的间隔，否则就会跟eureka自我保护机制的hard code硬编码的代码出现冲突

在eureka的自我保护代码里，充斥了大量的问题和bug，在尤其是测试环境下，你会发现经常就是动不动就进入自我保护的模式，说实话自我保护模式非常不稳定，完全不适合生产环境来使用

否则如果在生产环境中，动不动进入自我保护的模式，你平时服务故障了，他都不会去进行服务的故障感知和实例摘除，那这个事情就坑了

我之前做测试的时候，包括我们的一些同学做测试的时候，经常会发现每分钟期望的心跳次数是算错了，跟我们期望的是不符合的，计算每分钟的心跳次数，如果心跳次数<期望的心跳次数，就进入自我保护机制，不让evict task摘除任何服务实例

不要用，这么垃圾的代码，写的这么不靠谱的机制，不要用，在生产环境，测试环境都直接关了

eureka.server.enableSelfPreservation = false

1.8 eureka server集群的负载均衡

如果eureka server部署集群的时候，各个服务在注册或者是发送心跳，是如何请求eureka server集群的呢？

**你的每个服务里，会配置一个eureka server列表，谁配置在第一个，所有的服务优先就是访问那个eureka server。然后如果那台eureka server宕机了，那么此时所有的服务在重试过后都会访问其他的eureka server，而且此后大家都会去访问那台eureka server。**

1.9 eureka server集群同步的时效性

你配置了多少台server机器，其中接收到请求的那条机器，就会将请求转发给其他所有的机器

将这个集群数据同步的任务，会放入一个acceptorQueue里面去，AcceptorRunner后台线程来处理，每隔10ms会执行一次业务逻辑

中间有一个打包的过程，他默认会将500ms内的请求，注册、心跳、下线，打成一个batch，再一次性将这个batch发送给其他的机器，减少网络通信的次数，减少网络通信的开销，集群同步的批量处理的机制

**eureka server集群同步的时效性，基本上是在1s内，几百毫秒都是正常的**

2、服务调用

ribbon + feign，我们主要是面向feign来做，ribbon作为feign底层依赖的这么一套机制来搞的

2.1 ribbon + eureka服务发现的时效性

eureka client感知到其他服务上线了一个新的服务实例，1分钟以内，几十秒；eureka client感知到其他服务有个服务实例宕机了，大概是5分钟以内，三四分钟

比如说购物车服务要访问库存服务，刚开始库存服务就一台机器

后来某天，库存服务进行服务扩容，新增了1台机器，此时购物车服务本地的eureka client大概是1分钟才感知到人家新增了1台机器，ribbon的PollingServerListUpdater，刚好是30秒过后去刷新本地的eureka client的注册表到ribbon内部去

ribbon感知到库存服务新增1台机器，可能又过了30秒了，1.5分钟，1分30秒，如果比较快呢，1分钟组左右

目前库存服务有2台机器，其中1台机器宕机了，此时购物车服务本地的eureka client大概也需要三四分钟，最长是5分钟时间感知到，ribbon每隔30秒刷新eureka client的注册表到ribbon内部，极限情况下，ribbon感知到库存服务某台机器宕机了，可能需要5.5分钟，正常也需要个4分钟左右

2.2 ribbon的负载均衡算法

spring cloud环境下，ZoneAwareLoadBalancer，机房感知负载均衡器，比如说如果是多机房部署的话，比如在上海和北京，各部署了一部分机器在一个机房里，你一个系统，购物车服务有10台机器，库存服务有10台机器

在北京机房里，购物车服务放了5台机器，库存服务放了5台机器；在上海机房里，购物车服务也是5台机器，库存服务也是5台机器

然后ZoneAwareLoadBalancer，比如说北京机房里的购物车服务，现在要访问库存服务，是针对库存服务两个机房的10台机器去做负载均衡吗？优先是同机房访问，北京机房的机器，尽可能就是优先访问自己北京机房里的库存服务的5台机器

那么就是在北京机房的库存服务的5台机器中进行负载均衡

对于绝对多数的中小型公司来说，没有机房的概念，抛弃掉机房的事儿不考虑的话，那么他的负载均衡的算法，就是最最基础的round robin轮询算法，每次请求下一台机器，对一个机器列表，循环往复的请求

2.3 ribbon的服务故障感知的时效性

比如库存服务有2台机器，其中一台机器故障宕机了，如果要让依赖库存服务的购物车服务感知到，可能需要几分钟的时间，三四分钟，五六分钟，都有可能

比如说在这几分钟的时间里面，ribbon内部的保存的库存服务的server list，还是2台机器，此时不断的请求过来，ribbon负载均衡算法还是会不断的将请求流量分发给库存服务已经宕机的那台机器

如果请求已经宕机的一台机器，一定是会有问题的，连接一定会连接不上去的，connect timeout异常和报错，如果说你不做任何处理的话，可能会在短时间内导致大量的请求都会失败

超时和重试的参数

```
ribbon:
    ConnectTimeout: 1000
    ReadTimeout: 1000
    OkToRetryOnAllOperations: true
    MaxAutoRetries: 1
    MaxAutoRetriesNextServer: 3
```

ConnectTimeout：连接一台机器的超时时间

ReadTimeout：向一台机器发起请求的时间

重点是通过实验来设置好MaxAutoRetries和MaxAutoRetriesNextServer这两个参数

2.4 feign的服务调用的超时

2.5 feign的服务调用的失败重试

3、服务网关

3.1 ribbon预加载

第一次请求zuul的时候是很慢的，很容易超时

```
zuul:
    ribbon:
        eager-load:
            enabled: true
```

3.2 zuul + ribbon + eureka感知服务上线和故障的时效性

zuul 和 feign在这块是类似的，ribbon + eureka，eureka client感知到服务新实例上线，大概可能要个1分钟以内，ribbon那里，1.5分钟，1分钟左右，zuul也是一样的

如果说购物车服务现在有2台机器，但是不幸的是有1台机器宕机了，此时怎么办呢？eureka client大概可能需要5分钟以内的时间才能感知到，ribbon，5.5分钟，三四分钟，四五分钟。zuul的ribbon，感知到服务实例故障了，四五分钟

在某个服务实例宕机了，zuul还是不停的将请求转发过去会怎么样呢？肯定会失败，请求不了，此时zuul默认就是整合hystrix，hystrix会感知请求失败，异常了，直接会走hystrix的降级的逻辑，是什么都没有的，所以异常直接就会抛出来

zuul会有一个统一的error filter，会将这个异常给打印出来，反馈到调用方那里去

3.3 **请求超时和重试**

如果说某个服务实例突然挂了，zuul默认情况下是无法处理这种情况的，所以在线上生产环境下是很要命的，绝对不能让zuul直接这样子裸奔，不设置任何重试的机制直接就上了。如果某个服务实例挂了，打印error日志，给前端返回一段异常的json串：

```
{
  "timestamp": 1532067539032,
  "status": 500,
  "error": "Internal Server Error",
  "exception": "com.netflix.zuul.exception.ZuulException",
  "message": "GENERAL"
} 
```

默认情况下，hystrix实际上虽然是用了，但是没有这个降级的策略，默认情况下，无论是被hystrix线程池拒绝（被限流）、请求超时（出现过几次请求超时的现象）、发生了异常（某个服务实例宕机了）

抛出异常，hystrix降级没有办法做任何处理，异常只能被打印出来，封装成一个json串法功给前端浏览器来显示，或者是将json串发送给调用zuul网关的android、ios、PC前端，抛出来的这个异常，统一都是ZuulException，其实具体是什么异常，他是不告诉你的，他只告诉你内部出现了异常和错误

不是特别好，起码针对服务实例可能会挂、或者是可能会偶尔出现网络调用超时（失败），设置个最基本的重试的策略

zuul也是用的hystrix + ribbon那套东西，所以说，超时这里要考虑hystrix和ribbon的，而且hystrix的超时要考虑ribbon的重试次数和单次超时时间

ribbon的超时，hystrix的超时，zuul默认启用了hystrix，你要考虑到hystrix的超时

hystrix是包裹了ribbno的使用的，一般来说，hystrix的超时时长必须大于ribbon的超时时长，否则如果hystrix设置了超时是1s，ribbon设置的超时时长是2s，那么ribbon其实还没超时，hystrix直接就超时了

必须是hystrix超时时长最好是远大于ribbon超时时长，超时和重试尽量都是以ribbon为主

hystrix的超时时间计算公式如下：

(ribbon.ConnectTimeout + ribbon.ReadTimeout) * (ribbon.MaxAutoRetries + 1) * (ribbon.MaxAutoRetriesNextServer + 1)

```
hystrix:
  command:
    default:
      execution:
        isolation:
          thread:
            timeoutInMilliseconds: 10000
```

如果不配置ribbon的超时时间，默认的hystrix超时时间是4000ms（4s）

4、开发框架

SSM + Spring Boot

5、数据库

MySQL，一台机器，部署一个mysql服务，在里面创建15个逻辑库，15个中心，每个中心对应一个逻辑库

在微服务的架构中，有一点是非常的重要的，服务与服务之间一般是不会共享数据库的，每个服务一般是使用自己独立的一个数据库，然后如果某个服务要访问另外一个服务的数据的话，不能直接查其他服务的数据库的，而是通过其他服务暴露出来的接口来访问

6、工程拆分

我们一共会将电商系统拆分为15个中心，15个工程，每个服务一个工程，每个服务的工程对应一个独立的git仓库，每个服务都是完全独立的，每个服务都是在eclipse里是单独的一个工程，有自己独立的代码仓库，有自己独立的集成测试机器、QA测试机器、staging测试机器、prod生产机器，独立的部署，独立的数据库（某台数据库服务器上的一个逻辑库，比如mysql中的一个database）

### 3.服务拆分

spring boot，是作为一个parent父工程引入进来，约束了很多依赖的版本

spring cloud，是作为dependency management引入进来的，约束了很多依赖的版本

所以说大家只要保证spring boot和spring cloud版本基本一致就ok了

大量的服务与服务之间的调用，都是用的那种很复杂的domain类，传递大量的无效的字段。实际上在开发的时候，如果是分布式的系统，考虑网络间通信的效率问题，你可以尽可能的减少互相之间传递的参数的数量，可以提高网络通信的效率。

### 4.引入MQ消息中间件

库存服务，这块，其实是将库存更新的消息，异步发送到一个内存队列中的，我们之前是用的内存队列，但是现在的话呢，我们没法用这个内存队列了，因为我们的库存服务和调度服务给隔离开来了，独立部署，可能都不在一台机器上

库存服务发送库存更新的消息，到MQ（消息中间件），独立部署的中间件，调度服务就去消费这个MQ中的消息，拿到了额消息之后，调度服务再来更新自己本地的库存

常见情况

1、MQ的技术选型：你要对常见的几种MQ技术都有一点点的了解，而且知道在什么情况下选用哪种MQ

2、消息队列，部署为一个集群，这个东西呢，我们会在后面给大家来讲解，这块东西在项目阶段二中不会深入去讲解，但是在面试突击课程中，剖析了一下常见的MQ技术的高可用的原理

3、消息队列，消费到重复数据怎么办

4、消息队列，发送过去的数据丢失了怎么办

5、消息队列中的数据如何按照顺序来发送与消费

6、大量的消息积压在消息队列中该怎么办呢

7、消息队列的核心原理（你如何来设计一个消息中间件）

### 5.RabbitMQ发送与接收消息

你发送的每条消息，只会被一个worker（consumer）消费到和处理，consumer有3个，rabbitmq会被一个队列中的消息按照round robin轮询的算法，依次将消息推送给每个consumer，每个consumer都会接受到1/3的消息

ack机制

如果某个consumer消费一个消息，消费到一半儿就宕机了，这个消息没处理完，就会导致消息的丢失，消息队列中的数据丢失

consumer会自动进行ack，通知rabbitmq自己消费完了一个消息，自动ack的机制，可能什么呢？某个消息还在处理中，consumer就自动通知rabbitmq这个消息处理完了。后面宕机了，消息丢了。

consumer将自动ack关闭掉，然后自己在处理完一个任务之后，确定处理完毕了，再手动执行ack机制，通知rabbitmq处理完毕。如果处理到一半的时候，宕机了，rabbitmq没有收到这个消息的ack通知，就会将消息分发给其他的consumer再次处理

durability持久化机制

如果某个消息发送到了rabbitmq中，还没有来得及推送给消费者，此时rabbitmq自己宕机了，也会导致消息会丢失

消息的持久化机制

生产者在发送消息的时候，发送到rabbitmq之后，rabbitmq会将消息持久化到磁盘上去，然后才会通知生产者说ok，此次消息发送成功了。如果rabbitmq宕机了，没有将消息持久化到磁盘上去，此时生产者写入会报错，然后你就需要不断的重试写入，直到rabbitmq恢复正常，保证消息不会丢失

消息分发的均匀性

有的消息处理很耗时，有的消息处理不耗时，此时就会导致有的consumer一直在处理很耗时的消息，接收到的消息比较少；另外的consumer处理的是不耗时的消息，消息是均匀分发的，分发给一个worker，再是下一个worker。如果有的worker一直在处理耗时的任务，就会导致其他的任务处理的可能就是不耗时的任务，一直比较空闲

除非是一个worker已经处理完了一个message，而且通知了ack以后，才会给这个worker再次分发下一个消息，如果某个worker在处理耗时的任务，还没处理完，rabbitmq就不会将其他的消息分发给这个worker了

他会直接将其他的耗时的任务分发给其他的worker

### 6.spring cloud streaming整合rabbitmq

**1、加入rabbitmq配置**

在application.yml中分别加入rabbitmq的配置：

```
rabbitmq:
    host: localhost
    port: 5672
    username: guest
    password: guest
```

**2、编写生产者**

在ServiceB中加入以下的一些依赖：

```
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-stream-rabbit</artifactId>
</dependency>
```

编写发送消息的接口

```
public interface MessageService {
    @Output(“test-queue”)
    SubscribableChannel testQueue();
}  
```

在启动类上加入注解

@EnableBinding(MessageService.class)

然后在一个Controller中发送消息

```
public class ServiceBController {
    @Autowired
    private MessageService messageService;

    public String sendMessage() {
        Message message = MessageBuilder.withPayload(“hello world”.getBytes()).build();
        messageService.testQueue().send(message);
        return “SUCCESS”;
    } 
}
```

**3、编写消费者**

在application.yml中加入消费组：

如果你不设置这个消费组，会导致ServiceA启动了两台机器，每条消息都会推送给每台机器。但是如果你将ServiceA所有机器的消费组设置为一个，那么就是在各台机器之间round robin轮询发送消息

```
spring:
    application:
        name: ServiceA
    cloud:
        stream:
            bindings:
                my-test-queue:
                    group: groupA
```

在ServiceA中加入以下的一些依赖：

```
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-stream-rabbit</artifactId>
</dependency>
```

编写消息接口

```
public interface MessageService {
    @Input(“test-queue”)
    SubscribableChannel testQueue();
}
```

在启动类上加入注解

```
@EnableBinding(MessageService.class)
@StreamListener(“test-queue”)
public void receive(byte[] message) {
    System.out.println(“接收到的消息是：” + new String(message));
}
```

只要消息可以发送，可以接受，多个消费者可以均匀的接受一部分消息，就成功了，ack机制、持久化机制、均匀分发，涉及到一些参数的设置，刚开始用mq，你不搞也无所谓，在你的系统负载很低的时候，故障很少出现的

服务B发送的消息，服务A只有一台机器会接受到这个消息

### 7.问题

1、接口调用超时重试的时候，导致了大量脏数据的产生【解决，只要给业务表增加唯一索引就可以了，保证其业务上没有脏数据的产生】

2、在spring cloud环境下，多个服务叠加在一起，会出现超乎寻常的大量的重试，我们要判断一下，这个重试是否正常

3、wms服务中，出现了很严重的mysql的deadlock死锁的问题，导致wms服务的接口响应的性能很差

4、对本地数据库的操作是纯事务，不应该将本地数据库操作跟远程接口的调用，混在一块儿，可能会出现莫名其妙的事务之间锁争用的问题，通过分布式系统，触发了一个分布式系统场景下的问题

5、对于这种插入数据的操作，我们是否要进行超时重试呢，如果要进行超时重试，就有可能会产生脏数据，那么我们就必须保证这个接口的幂等性

什么叫做分布式系统接口的幂等性，是什么意思呢？就是说，可能会有人重试你几次，发送一样的请求，你必须保证一样的请求不能重复的更新数据，否则就会导致数据出现脏数据。。。。

### 8.分布式事务技术

01、仿支付宝流量充值中心的案例背景介绍（包含10+个服务的分布式系统）
02、基于Spring Boot完成单块架构的仿支付宝流量充值中心
03、事务的基础概念以及为流量充值中心加入最基础的事务
04、深入剖析Spring事务框架的源码（@Transactional注解底层的秘密）
05、并发量和数据量持续增加时将数据库进行拆分
06、跨多个数据库的事务问题分析
07、XA规范 + 2PC + 3PC的分布式事务知识讲解
08、MySQL XA分布式事务原理
09、Java与Spring的事务支持：JDBC事务、JTA事务、全局事务以及本地事务
10、多数据源支持 + XA分布式事务来重构流量充值中心
11、Spring对XA分布式事务支持的相关源码深入剖析
12、对流量充值中心进行服务化拆分（基于Spring Cloud拆分为10+个服务）
13、服务化系统的分布式事务问题分析
14、分布式系统一致性问题、CAP理论与BASE理论
15、分布式事务的各种解决方案原理讲解：TCC补偿方案、可靠消息最终一致性方案、最大努力通知方案、Sagas方案
16、可靠消息最终一致性方案的技术架构设计
17、在流量充值中心系统中落地实践可靠消息最终一致性方案
18、对可靠消息最终一致性方案进行多个点的架构优化
19、对可靠消息最终一致性方案涉及到的MQ等相关底层原理进行深入剖析
20、TCC方案的技术选型以及各种候选框架的压测
21、TCC方案的技术架构设计
22、在流量充值中心系统中落地实践TCC方案
23、站在BAT大型互联网公司的角度对TCC方案进行多个点的架构优化
24、对tcc-transaction框架深入阅读源码
25、最大努力通知方案的技术架构设计
26、在流量充值中心中落地实践最大努力通知方案
27、对最大努力通知方案进行多个点的架构优化
28、对最大努力通知方案涉及到的MQ等底层原理进行深入剖析
29、深入学习saga分布式事务解决方案
30、saga方案的技术架构设计
31、在流量充值中心项目中落地实践saga方案
32、深入研究saga分布式事务框架的源码
33、将TCC、可靠消息最终一致性、最大努力通知、saga等几种方案结合业务场景运用在电商平台项目中，与我们复杂电商的各个业务结合来使用

### 9.Mysql事务

MySQL的默认隔离级别是Read Repeatable，就是可重复读，就是说每个事务都会开启一个自己要操作的某个数据的快照，事务期间，读到的都是这个数据的快照罢了，对一个数据的多次读都是一样的。

MySQL是如何实现Read Repeatable的，MySQL是通过MVCC机制来实现的，就是多版本并发控制，multi-version concurrency control。

innodb存储引擎，会在每行数据的最后加两个隐藏列，一个保存行的创建时间，一个保存行的删除时间，但是这儿存放的不是时间，而是事务id，事务id是mysql自己维护的自增的，全局唯一。

在一个事务内查询的时候，mysql只会查询创建时间的事务id小于等于当前事务id的行，这样可以确保这个行是在当前事务中创建，或者是之前创建的；同时一个行的删除时间的事务id要么没有定义（就是没删除），要么是必当前事务id大（在事务开启之后才被删除）；满足这两个条件的数据都会被查出来。

那么如果某个事务执行期间，别的事务更新了一条数据呢？这个很关键的一个实现，其实就是在innodb中，是插入了一行记录，然后将新插入的记录的创建时间设置为新的事务的id，同时将这条记录之前的那个版本的删除时间设置为新的事务的id。

这样的话，你的这个事务其实对某行记录的查询，始终都是查找的之前的那个快照，因为之前的那个快照的创建时间小于等于自己事务id，然后删除时间的事务id比自己事务id大，所以这个事务运行期间，会一直读取到这条数据的同一个版本。

### 10.分布式事务

分布式事务的模型，这里面有几个角色，就是AP（Application，应用程序），TM（Transaction Manager，事务管理器），RM（Resource Manager，资源管理器），CRM（Communication Resource Manager，通信资源管理器）

其实Application说白了就是我们的系统，TM的话就是一个在系统里嵌入的一个专门管理横跨多个数据库的事务的一个组件，RM的话说白了就是数据库（比如MySQL），CRM可以是消息中间件（但是也可以不用这个东西）

一个横跨多个数据库的事务，就是一个事务里，涉及了多个数据库的操作，然后要保证多个数据库中，任何一个操作失败了，其他所有库的操作全部回滚，这就是所谓的分布式事务

#### **2PC理论**

Two-Phase-Commitment-Protocol，两阶段提交协议

（1）准备阶段

prepare消息一发，各个库先在本地开个事务，然后执行好SQL，万事俱备只欠东风了，而且注意这里各个数据库会准备好随时可以提交或者是回滚，有对应的日志记录的 

然后各个数据库都返回一个响应消息给事务管理器，如果成功了就发送一个成功的消息，如果失败了就发送一个失败的消息

（2）提交阶段

TM接收到所有的数据库返回的消息都是成功，直接发送个消息通知各个数据库说提交，然后各个数据库都在自己本地提交事务呗，提交好了通知下TM，TM要是发现所有数据库的事务都提交成功了，就认为整个分布式事务成功了

否则TM直接判定这个分布式事务失败，然后TM通知所有的数据库，全部回滚，然后各个库都回滚好了以后就通知TM，TM就认为整个分布式事务都回滚了

##### 缺点 

1、同步阻塞：在阶段一里执行prepare操作会占用资源，一直到整个分布式事务完成，才会释放资源，这个过程中，如果有其他人要访问这个资源，就会被阻塞住 

2、单点故障：TM是个单点，一旦挂掉就完蛋了 

3、事务状态丢失：即使把TM做成一个双机热备的，一个TM挂了自动选举其他的TM出来，但是如果TM挂掉的同时，接收到commit消息的某个库也挂了，此时即使重新选举了其他的TM，压根儿不知道这个分布式事务当前的状态，因为不知道哪个库接收过commit消息，那个接收过commit消息的库也挂了，兄弟 

4、脑裂问题：在阶段二中，如果发生了脑裂问题，那么就会导致某些数据库没有接收到commit消息，那就完蛋了，有些库收到了commit消息，结果有些库没有收到，这咋整呢，那肯定完蛋了

#### 3PC

three-phase-commitment，三阶段提交协议，这个是针对2PC做的一个改进，主要就是为了解决2PC协议的一些问题

（1）CanCommit阶段：这个就是TM发送一个CanCommit消息给各个数据库，然后各个库返回个结果，注意一下，这里的话呢，是不会执行实际的SQL语句的，其实说白了，就是各个库看看自己网络环境啊，各方面是否ready 

（2）PreCommit阶段：如果各个库对CanCommit消息返回的都是成功，那么就进入PreCommit阶段，TM发送PreCommit消息给各个库，这个时候就相当于2PC里的阶段一，其实就会执行各个SQL语句，只是不提交罢了；如果有个库对CanCommit消息返回了失败，那么就尴尬了，TM发送abort消息给各个库，大家别玩儿了，结束这个分布式事务 

（3）DoCommit阶段：如果各个库对PreCommit阶段都返回了成功，那么发送DoCommit消息给各个库，就说提交事务吧，兄弟们，各个库如果都返回提交成功给TM，那么分布式事务成功；如果有个库对PreCommit返回的是失败，或者超时一直没返回，那么TM认为分布式事务失败，直接发abort消息给各个库，说兄弟们回滚吧，各个库回滚成功之后通知TM，分布式事务回滚

##### 跟2PC相比，主要做了下面两个改进点：

（1）引入了CanCommit阶段 

（2）在DoCommit阶段，各个库自己也有超时机制，也就是说，如果一个库收到了PreCommit自己还返回成功了，等了一会儿，如果超时时间到了，还没收到TM发送的DoCommit消息或者是abort消息，直接判定为TM可能出故障了，人家库自己颠儿颠儿的就执行DoCommit操作，提交事务了。

如果这个库接收到了PreCommit消息，说明第一阶段各个库对CanCommit都返回成功了啊，这样TM才会发送PreCommit来，那么就默认为基本上各个库的PreCommit都会成功，所以大家没接收到DoCommit，直接自己执行提交操作了

##### 3PC的缺陷

如果人家TM在DoCommit阶段发送了abort消息给各个库，结果因为脑裂问题，某个库没接收到abort消息，自己还颠儿颠儿的执行了commit操作，不是也不对么

所以啊，其实2PC也好，3PC也好，都没法完全保证分布式事务的ok的，要明白这一点，总有一些特殊情况下会出问题的

### 11.分布式事务实践

整合在spring boot + druid生产级环境中的一套单系统跨多库的分布式事务实践，使用的JTA事务管理器，加上Atomikos分布式事务的框架，Atomikos框架主要是提供了支持分布式事务的DataSource数据源的一个支持

JTA主要是提供了事务管理器，也就是分布式事务流程管控的这么一套机制

原来是单库的时候，tm其实PlatformTransactionManager，也就是单库的最基础的事务管理器，现在的话就变成了我们之前配置的那个JtaTransactionManager了，代表事务的一个对象，是JtaTransactionObject

JtaTransactionManager.doBegin()方法中去，TransactionManagerImpl中的一个方法，这个方法中主要就是获取一个CompositeTransaction的一个对象，根据当前线程，从一个map中获取出来一个Stack数据结构，从栈中弹出来一个对象

每个线程过来，都可以获取到属于这个线程他自己的代表着分布式事务的一个CompositeTransaction的对象，刚开始这个栈里肯定是没有对象的，栈是空的，所以刚开始从栈中弹出来的这个对象是null，所以会手动创建一个 

创建好了一个CompositeTransaction对象之后，会放入map中thread对应的一个Stack，将这个对象压入栈中 

mybatis的mapper组件，会找SqlSessionFactory，SqlSessionFactory一定会去找底层的数据库连接池中的DataSource

基于DataSource获取一个Connection，基于这个Connection来执行preparedStatement() SQL语句，肯定会干这个事情的

Atomikos框架，做了一个DataSource，AtomikosDataSourceBean，从这里来获取Connection的时候，一定是会获取一个Atomikos包装过的Connection

提交事务的时候，JtaTransactionManager会去执行两阶段的提交，先是发送XA PREPARE指令，然后根据情况去发送对应的XA COMMIT指令，或者是XA ROLLBACK指令

数据库连接池最最核心的是啥？其实很简单，就是从ConnectionFactory里面去获取一个Connection，放到一个ConnectionPool里面去，这个池子里面放了数据库连接，在AtomikosDataSourceBean中，我们其实设置了DruidXADataSource

人家mybatis -> mapper -> SqlSessionFactory -> DataSource -> getConnection()，AtomokosDataSOurceBean最核心的地方其实是getConnection()方法

ConnectionFactory是Atomikos框架自己做的，这个数据库连接池的工厂，其实是基于DruidXADataSource来封装的，从ConnectionFactory里获取连接，你基本可以认为是通过DruidXADataSource来获取连接

从connectionPool中获取了一个数据库连接，一看就是基于底层的DruidXADataSource在获取数据库连接，但是获取出来的数据库连接，肯定是经过Atomikos的包装的，AtomikosConnectionProxy，这个东西里面是封装了一个DruidXADataSource实际返回给他的一个数据库连接，创建了一个动态代理，动态代理中，肯定是封装了这个对应的AtomikosConnectionProxy这个东西的，AtomokosConnectionProxy其实是一个JDK动态代理中的一个InvocationHandler的这么一个概念，这个东西呢，他其实是在动态代理被调用的时候，都会由他来拦截

AtomikosDataSourceBean -> getConnection() -> mybatis底层肯定是基于这个数据源来获取数据库连接以及执行preparedStatement之类的操作 -> Connection接口的动态代理 -> AtomikosConnectionProxy（InvocationHandler）

### 12.JTA技术特点：

（1）Atomikos框架实现XA分布式事务的一些技术特点是什么呢？ 

（2）如果让你来实现一套Atomikos框架，就是第一步，你需要跟JtaTransactionManager整合起来，需要JTA结合起来 

（3）创建分布式事务的时候，创建一个代表了分布式事务的对象；在各个SQL执行的时候，必须从你的DataSource里面获取Connection，对Connection的prepareStatement()方法的调用，你需要进行拦截，去对各个库执行XA START指令，以及定义好SQL；在提交事务的时候，你就需要去对各个库执行XA PREPARE指令，如果都成功，就执行XA COMMIT指令，如果失败，就执行XA ROLLBACK指令 

（4）比较有技术含量的一些点，还是动态代理的灵活的使用，自己去实现DataSource，Connection变成了你的动态代理，Feign源码，大量的也是依赖于动态代理，动态代理这个东西，不难，JDK动态代理，Atomikos、Feign等开源框架用的都是JDK动态代理，CBLIB动态代理

### 13.分布式事务解决方案

#### （1）两阶段提交方案/XA方案

第一个阶段，一般tb主席会提前一周问一下团队里的每个人，说，大家伙，下周六我们去滑雪+烧烤，去吗？这个时候tb主席开始等待每个人的回答，如果所有人都说ok，那么就可以决定一起去这次tb。如果这个阶段里，任何一个人回答说，我有事不去了，那么tb主席就会取消这次活动。

第二个阶段，那下周六大家就一起去滑雪+烧烤了 

所以这个就是所谓的XA事务，两阶段提交，有一个事务管理器的概念，负责协调多个数据库（资源管理器）的事务，事务管理器先问问各个数据库你准备好了吗？如果每个数据库都回复ok，那么就正式提交事务，在各个数据库上执行操作；如果任何一个数据库回答不ok，那么就回滚事务。

#### （2）TCC方案

这个其实是用到了补偿的概念，分为了三个阶段： 

1）Try阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行锁定或者预留

2）Confirm阶段：这个阶段说的是在各个服务中执行实际的操作

3）Cancel阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚操作

这种方案说实话几乎很少用人使用，我们用的也比较少，但是也有使用的场景。因为这个事务回滚实际上是严重依赖于你自己写代码来回滚和补偿了，会造成补偿代码巨大，非常之恶心。

比较适合的场景：这个就是除非你是真的一致性要求太高，是你系统中核心之核心的场景，比如常见的就是资金类的场景，那你可以用TCC方案了，自己编写大量的业务逻辑，自己判断一个事务中的各个环节是否ok，不ok就执行补偿/回滚代码。

而且最好是你的各个业务执行的时间都比较短。

#### （3）本地消息表

1）A系统在自己本地一个事务里操作同时，插入一条数据到消息表

2）接着A系统将这个消息发送到MQ中去

3）B系统接收到消息之后，在一个事务里，往自己本地消息表里插入一条数据，同时执行其他的业务操作，如果这个消息已经被处理过了，那么此时这个事务会回滚，这样保证不会重复处理消息

4）B系统执行成功之后，就会更新自己本地消息表的状态以及A系统消息表的状态

5）如果B系统处理失败了，那么就不会更新消息表状态，那么此时A系统会定时扫描自己的消息表，如果有没处理的消息，会再次发送到MQ中去，让B再次处理

6）这个方案保证了最终一致性，哪怕B事务失败了，但是A会不断重发消息，直到B那边成功为止

#### （4）可靠消息最终一致性方案

这个的意思，就是干脆不要用本地的消息表了，直接基于MQ来实现事务。比如阿里的RocketMQ就支持消息事务。

大概的意思就是：

1）A系统先发送一个prepared消息到mq，如果这个prepared消息发送失败那么就直接取消操作别执行了

2）如果这个消息发送成功过了，那么接着执行本地事务，如果成功就告诉mq发送确认消息，如果失败就告诉mq回滚消息

3）如果发送了确认消息，那么此时B系统会接收到确认消息，然后执行本地的事务

4）mq会自动定时轮询所有prepared消息回调你的接口，问你，这个消息是不是本地事务处理失败了，所有没发送确认消息？那是继续重试还是回滚？一般来说这里你就可以查下数据库看之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，别确认消息发送失败了。

5）这个方案里，要是系统B的事务失败了咋办？重试咯，自动不断重试直到成功，如果实在是不行，要么就是针对重要的资金类业务进行回滚，比如B系统本地回滚后，想办法通知系统A也回滚；或者是发送报警由人工来手工回滚和补偿 

这个还是比较合适的，目前国内互联网公司大都是这么玩儿的，要不你举用RocketMQ支持的，要不你就自己基于类似ActiveMQ？RabbitMQ？自己封装一套类似的逻辑出来，总之思路就是这样子的

#### （5）最大努力通知方案 

这个方案的大致意思就是： 

1）系统A本地事务执行完之后，发送个消息到MQ

2）这里会有个专门消费MQ的最大努力通知服务，这个服务会消费MQ然后写入数据库中记录下来，或者是放入个内存队列也可以，接着调用系统B的接口

3）要是系统B执行成功就ok了；要是系统B执行失败了，那么最大努力通知服务就定时尝试重新调用系统B，反复N次，最后还是不行就放弃

#### 总结

（1）TCC方案，适合于，你的多个服务的操作都比较快 

TCC相当于是一堆同步服务调用的操作，包裹在一个事务里面，同步，关键词，人家给你发起一个请求，触发了一个复杂的TCC事务，人家要等你这个事务完成结束了，然后才能接续往下走的 

假如你的TCC事务里面涉及了10来个服务的调用，要10来秒才能结束，太不靠谱了

（2）可靠消息最终一致性的方案

这个方案，适合于那那种比较耗时的操作，通过这个消息中间件做成异步调用，发送一个消息出去，人家服务消费消息来执行业务逻辑，CAP理论，C（最终一致性），也就是说包裹在一个事务中的多个操作，其中有些操作可能在一定时间内是没执行的

可能要等过一段时间之后，然后才能去执行，最终一定会执行的，最终一致性的方案，通过MQ消息中间件保证消息的可靠性，最终来实现最终一致性的方案

（3）最大努力通知方案

跟可靠消息最终一致性方案是类似的，可靠消息最终一致性方案，会保证最终必须要让那个执行成功的，但是最大努力通知方案，不一定保证最终一定会成功，可能会失败，但是他会尽力给你去给你通知那个服务的执行 

比较适合那种不太核心一些服务调用的操作，比如说消息服务，充值好了以后发送短信，一般来说肯定是要发出去短信的，但是如果真的不小心发送失败了，发送短信失败了也无所谓的。。。

### 14.CAP理论

CAP，就是Consistency、Availability、Partition Tolerence的简称，简单来说，就是一致性、可用性、分区容忍性

（1）一致性

就是说一个分布式系统中，一旦你做了一个数据的修改，那么这个操作成功的时候，就必须是分布式系统的各个节点都是一样的

啥叫**强一致性**呢，就是说上面讲的那种就是强一致性；**弱一致性**呢，就是你更新个数据，鬼知道能不能让各个节点都更新成功；**最终一致性**，就是可能更新过后，一段时间内，数据不一致，最后过了一段时间成功了

（2）可用性

客户端往分布式系统的各个节点发送请求，都是可以获取到响应的，要不是可以写入成功，要不是可以查询成功；什么叫做不可用呢？客户端往分布式系统中的各个节点发送请求的时候，获取不到响应结果，这个时候，系统就是不可用了，写入失败，人家不让你写入，不接受你的请求

（3）分区容忍性

分区容忍性，你的分布式系统可以容忍网络分区的故障，出现上面说的那种网络分区的故障之后，分布式系统的各个节点之间无法进行通信，无所谓，整套分布式系统各个节点，各自为战，该干嘛干嘛，只不过互相之间无法通信而已

分布式系统还是在运转着，你分别给各个节点发送请求，人家还是可以给你一些响应结果的，这个就是实现了分区容忍性

（4）CP 

一般来说，CAP要么同时满足AP，要么同时满足CP，不可能同时满足CAP的，啥意思呢

假设，出现了网络分区的故障，但是因为有P，所以分布式系统继续运转，但是此时分布式系统的节点之间无法进行通信，也就无法同步数据了

此时客户端要来查询数据，也就是那个key1的数据了，此时系统实际上是处于一个不一致的状态，因为各个节点之间的数据是不一样的，如果客户端来查询key1这条数据，你要是要保证CP的话，就得返回一个特殊的结果（异常）给客户端 

任何一个节点此时不接收任何查询的请求，返回一个异常（系统当前处于不一致的状态，无法查询），这样的话呢，客户端是看不到不一致的数据的

（5）AP 

如果网络故障，数据没同步，数据处于不一致的状态下，要保证A，可用性，你两个节点都要允许任何客户端来查询，都可以查到，这样的话呢，整个系统就处于可用的状态下，但是此时就牺牲掉了C 

一会儿可以查到key1的数据，一会儿从另外一个节点去查又查不到了，这就是对客户端而言，看到了不一致的数据

（6）BASE理论 

所谓的BASE，Basicly Available、Soft State、Eventual Consistency，也就是基本可用、软状态、最终一致性 

BASE希望的是，CAP里面基本都可以同时实现，但是不要求同时全部100%完美的实现，CAP三者同时基本实现，BASE，基本可用、最终一致性

### 15.TCC分布式事务

**1、通用性TCC技术方案**

（1）主业务服务会先在本地开启一个本地事务（这个本地事务说白了，就是你的主业务服务是不是也可能会干点儿什么事儿）

（2）主业务服务向业务活动管理器申请启动一个分布式事务活动，主业务服务向业务活动管理器注册各个从业务活动

（3）接着主业务服务负责调用各个从业务服务的try接口

（4）如果所有从业务服务的try接口都调用成功的话，那么主业务服务就提交本地事务，然后通知业务活动管理器调用各个从业务服务的confirm接口

（5）如果有某个服务的try接口调用失败的话，那么主业务服务回滚本地事务，然后通知业务活动管理器调用各个从业务服务的cancel接口

（6）如果主业务服务触发了confirm操作，但是如果confirm过程中有失败，那么也会让业务活动管理器通知各个从业务服务cancel

（7）最后分布式事务结束

**2、异步确保型TCC技术方案**

在主业务服务和从业务服务之间加了一个可靠消息服务，但是这个可靠消息服务可不是在请求什么MQ之类的东西，而是将消息放在数据库里的

大致来说呢，就是主业务服务的try、confirm和canel操作都调用可靠消息服务，然后可靠消息服务在try阶段插入一条消息到本地数据库；接着主业务服务执行confirm操作，可靠消息服务就是根据之前的消息，调用从业务服务实际的业务接口；如果要是这个调用失败的话，那么主业务服务发起cancel，可靠消息服务删除自己本地的消息即可

**3、补偿性TCC解决方案**

这个其实是跟通用型的TCC方案类似的，只不过从业务服务就提供俩接口就ok了，Do和Compensate，就是执行接口和补偿接口，这种方案的好处就是折中一下了，不需要从业务服务改造出来一个T接口，就是锁定资源的接口，只需要加一个补偿接口，如果业务逻辑执行失败之后，进行补偿 

这样就可以少做一个接口了，但是因为没有做资源的一个锁定，那么大家需要自己注意类似资金转账的余额检查之类的事儿了，还有就是补偿的时候，因为你没做资源锁定，所以要注意一下补偿机制是否一定会成功

Do接口，Compensate接口，不要try接口，不要锁定资源，直接执行业务逻辑，如果有失败就调用Compensate接口，补偿接口，回滚刚才的操作

### 16.bytetcc

（1）将对外发布的服务，直接相关的业务逻辑定义在controller里面，我们现在也是这样子来搞的，这里的方法加transactional注解

（2）这个controller里的方法相当于是try逻辑，锁定资源

（3）在controler要配置compensable注解，这个注解就指定了每个接口的confirm和cancel的逻辑，try-confirm-cancel配对组合好了

（4）bytetcc框架会负责在适当的时机来调用和执行try，然后自动去调用confirm和cancel，这里一大堆关于tcc事务的控制，都是人家框架自己搞定的

### 17.tcc分布式事务 

（1）资金服务：转账

（2）订单服务：创建充值订单

（3）抽奖服务：增加抽奖机会

（4）积分服务：增加积分

（5）流量券服务：使用流量券，赠送流量券

订单服务： 

（1）try：新建一个订单，状态设置为“交易中”

（2）confirm：修改订单的状态为“交易成功”

（3）cancel：修改订单的状态为“交易失败”

抽奖服务改造成tcc模式 

（1）try：锁定资源，预分配资源，你可以这样子，给抽奖表加一个锁定抽奖次数字段，给冻结抽奖次数的字段增加1

（2）confirm：将锁定抽奖次数字段减去1，给正式的抽奖次数字段增加1

（3）cancel：将锁定抽奖次数字段给减去1 

积分服务的tcc事务： 

（1）try：可以搞一个锁定积分字段，在锁定积分字段里先增加一些积分

（2）confirm：扣减掉锁定积分字段的积分，然后给正式的积分字段增加一些积分

（3）cancel：扣减掉锁定积分字段里的积分

使用流量券 

（1）try：他这个业务操作其实非常的简单，只是修改这么一个状态，你可以设计一个中间的这么一个状态，比如说“修改中”，-1

（2）confirm：正式给修改成2，“已使用”

（3）cancel：修改成1，“未使用”

流量券的状态：-1（修改中），0（不可用），1（可用，但是未使用），2（已使用）

赠送流量券 

（1）try：新建一个流量券，但是状态是0，不可用

（2）confirm：状态修改为1，未使用

（3）cancel：删除这个赠送的流量券

### 18.bytetcc框架的源码

（1）LocalXADataSource，bytecc框架的，来封装了一下底层的数据源

（2）@Compensable注解，以及tcc三个接口

（3）@Import(SpringCloudConfiguration.class)

（4）@ImportResource({ "classpath:bytetcc-supports-springcloud.xml" }) 

LocalXADataSource，封装底层的数据源，我认为他主要是要捕获到在一个分布式事务中，到底对哪些数据库执行了哪些SQL操作呢

LocalXADataSource主要是看到分布式事务中执行的各个SQL语句，记录一下状态，插入一些自己的数据到底层的各个库的bytejta表中，记录分布式事务的状态

@Compensable，我们大概可以想象到，tcc（try、confirm、cancel），三个接口，这个注解也不是核心的入口，他就是将每个接口定义成tcc三个接口

@Import和@ImportResource里面一定是搞了一大堆bytetcc自己的一些bean以及组件，在系统启动的时候这里的bean和组件就会开始工作，比如说，在服务里，你既然加了@Compensable注解之后，肯定得有组件去扫描和识别这个注解，然后将每个接口封装成tcc三个接口

CompensableFeignBeanPostProcessor，实现了一个BeanPostProcessor接口，就是说他是要对每个bean在实例化好了之后来处理一下每个bean，他是专门处理feign里面的FeignInvocationHandler这个bean的

你只要加了这个@Transactional注解，就可以看到TransactionInterceptor的东西就会运行，他会拦截掉这个请求，这个东西会走一套流程： 

（1）begin，启动一个事务 => TransactionManager

（2）执行目标方法内部的业务逻辑

（3）根据目标方法执行的结果，是否成功，或者是报错，来选择commit / rollback => TransactionManager



正常的一个分布式事务的机制和流程应该是如下的：

（1）流量充值服务的try -> 资金服务的try -> 账单服务的try

（2）如果try全部成功了，那么流量充值服务内部的bytetcc框架会自动去调用资金服务内部的bytetcc框架

（3）那么资金服务的confirm接口被调用了之后，如何触发账单服务的confirm接口呢？但是我们设想的是，bytetcc这种框架，如果你要做企业级的tcc分布式事务的框架，你必须得支持一点，资金服务之前调用过账单服务的try接口，还成功了，所以应该记录好，自己是调用过账单服务的这个事儿的

（4）然后当资金服务的confirm接口被调用的时候，是被谁调用呢？是bytetcc框架的controller去执行了资金服务的confirm逻辑，那么我们可以想象一下，bytetcc框架在执行了自己本地的confirm逻辑之后，是不是可以根据自己之前调用过哪些其他的服务的try，来判断，自己现在执行了confirm之后，其实也需要去调用之前自己try成功过的其他服务的confirm接口

（5）如果try阶段有任何一个服务报错了，try失败了

（6）那么就需要流量充值服务去调用资金服务的cancel接口，资金服务也需要触发账单服务的cancel接口

每个服务的try，是可以通过spring cloud的feign去调用其他服务的try接口的 

每个服务的confirm和cacnel，都是只要执行自己本地的confirm和cancel的逻辑就可以了，不需要你去触发其他服务的confirm和cancel，为啥呢？因为这都是bytetcc事务框架在底层都把事儿给你做了

### 19.分布式事务

可靠消息最终一致性方案涉及到4个组件： 

（1）上游服务：发送MQ消息通知下游服务执行某个操作 

（2）可靠消息服务：协调上下游服务的消息传递，确保数据一致性，可以认为这个所谓的可靠消息服务是我们自己开发的，也是一个spring cloud的服务，只不过这个服务是通用的，是所有服务所有系统都基于这个可靠消息服务来实现可靠消息最终一致性的方案。“可靠消息”四个字，这一切都是基于可靠消息服务来做的，方案设计，消息如何保持可靠性 

（3）MQ消息中间件：这个一般是RocketMQ或者是RabbitMQ 

（4）下游服务：就是那个要被调用的服务

所谓的分布式事务，上游服务他要执行一个本地的数据库操作，下游服务也要执行一个本地的数据库操作，现在尽量就是希望是说上游服务和下游服务的数据库操作要么同时完成，要么同时不完成

具体的执行流程如下所示 

（1）上游服务发送一个待确认消息给可靠消息服务 

（2）可靠消息服务将这个待确认的消息保存到自己本地数据库里，保存起来，但是不发给MQ，这个时候消息的状态是“待确认” 

（3）上游服务操作本地数据库 

（4）上游服务根据自己操作本地数据库的结果，来通知可靠消息服务，可以确认发送消息了，或者是删除消息

操作完本地数据库之后，会有两个结果，第一个结果是操作失败了，第二个结果是操作成功了，如果本地数据库操作失败了，本地操作会回滚，回滚之后，上游服务就要通知可靠消息服务删除消息；如果本地数据库操作成功了，那么此时本地事务就提交了，接着就可以通知可靠消息服务发送消息 

（5）可靠消息服务将这个消息的状态修改为“已发送”，并且将消息发送到MQ中间件里去

这个环节是必须包裹在一个事务里的，如果发送MQ失败报错，那么可靠消息服务更新本地数据库里的消息状态为“已发送”的操作也必须回滚，反之如果本地数据库里的消息状态为“已发送”，那么必须成功投递消息到MQ里去

如果更新数据库里的消息状态报错了，那么消息根本不会投递到MQ里去；如果更新数据库里的消息状态成功了，但是事务还没提交，然后将消息投递到MQ里去报错了，此时事务管理器会感知到这个异常，然后会直接回滚掉整个事务，更新数据库里消息状态的操作也会回滚掉的

（6）下游服务从MQ里监听到一条消息 

（7）下游服务根据消息，在自己本地操作数据库

（8）下游服务对本地数据库操作完成之后，对MQ进行ack操作，确认这个消息处理成功

（9）下游服务对MQ进行ack之后，再给可靠消息服务发送个请求，通知该服务说，ok，我这里处理完毕了，可靠消息服务收到通知之后，将消息的状态修改为“已完成”



（1）上游服务发送一个待确认消息给可靠消息服务

如果这个环节失败了，也就是上游服务就没法发送一个待确认消息给可靠消息服务，那么无所谓啊，从头儿开始就失败了，数据没有任何不一致吧

（2）可靠消息服务将这个待确认的消息保存到自己本地数据库里，保存起来，但是不发给MQ，这个时候消息的状态是“待确认”

如果这个环节失败了，也就是说可靠消息服务没有成功的将消息保存在本地数据库，比如自己本地数据库插入报错了，那也没事啊，因为肯定会返回给上游服务一个失败的消息的，此时上游服务就不会继续往下执行了，对吧

（3）上游服务操作本地数据库

如果这个环节失败了，上游服务执行本地数据库操作以及发送确认消息这个绑定在一块儿的事儿没干成功，那么也无所谓啊，因为这个时候，本地操作也会回滚，然后也不会继续发送确认消息给可靠消息服务了 

如果本地事务回滚了之后，正常来说就会发送消息通知可靠消息服务，删除那条消息

（4）上游服务通知可靠消息服务，可以确认发送消息了

如果这个环节失败了，那么尴尬死了，上游服务的本地数据库操作都成功了，结果没发个消息给可靠消息服务，什么鬼！那就不会通知下游服务了，直接数据不一致 

问题就大了，也就是说，上游服务通过spring cloud feign调用可靠消息服务，通知确认/删除消息的时候，服务调用失败，异常，因为此时会导致可靠消息服务的数据库里，留着一条状态为“待确认”的消息

（5）可靠消息服务将这个消息的状态修改为“已发送”，并且将消息发送到MQ中间件里去，这个环节是必须包裹在一个事务里的

如果发送MQ失败报错，那么可靠消息服务更新本地数据库里的消息状态为“已发送”的操作也必须回滚，反之如果本地数据库里的消息状态为“已发送”，那么必须成功投递消息到MQ里去 

如果这个环节失败了，更加尴尬了，上游服务的本地数据库操作成功了，而且也成功的通知了可靠消息服务了，结果可靠消息服务将消息发送到MQ里去居然失败了，那还是没能成功把消息发出去啊，兄弟！ 

数据不一致了 

分成两种情况来说，如果上游服务操作本地数据库失败了，会通知可靠消息服务去删除消息，此时可靠消息服务删除消息的操作失败了，会导致有问题，有一个消息“待确认”始终停留在可靠消息服务的数据库里 

如果上游服务操作本地数据库成功了，会通知可靠消息服务去确认和投递消息，但是确认消息+投递消息（绑定在一起），现在一起失败了，消息还是“待确认”的状态，而且没有投递到MQ里去，此时也是不对的

（6）下游服务从MQ里监听到一条消息

下游服务如果压根儿不知道为啥没有从MQ里消费到消息，那没关系，现在的MQ都很强大的，后面我们给大家讲解，这个MQ如何可靠的投递消息。如果你没消费成功，人家MQ一定会确认给你重试投递的 

因为你是手动ack的，只要MQ没有收到ack的通知，会重新投递消息的 

（7）下游服务根据消息，在自己本地操作数据库

如果下游服务操作自己本地数据库失败了，那么本地事务就会回滚咯，然后就不会发送ack给MQ，那么MQ会继续不断的重试的，所以也没关系的，这是现在的MQ中间件都支持的功能

（8）下游服务对本地数据库操作完成之后，对MQ进行ack操作，确认这个消息处理成功 

如果下游服务处理完了本地操作，给MQ发送ack的时候失败了，这个。。。你就得考虑下怎么处理了，其实我们可以考虑将下游服务的本地数据库操作和MQ的ack操作包裹在一个事务里，这样的话呢，如果MQ ack操作报错了，本地事务直接回滚 

这样的话，MQ后续会再次重发消息的，所以没关系 

rabbitmq也好，kafka也好，都是这样子，只要你关闭了自动ack以后，你迟迟的没有进行手动ack，人家会进行消息的重发，成功的手动ack之后，然后才会让mq将这条消息给删除掉不再重新投递 

kafka，手动提交offset，关闭自动提交offset，变成手动提交offset 

rabbimq，关闭自动ack，手动进行ack，就可以了 

就算不用rabbitmq手动ack重新投递消息的机制，只要依靠可靠消息服务的完善，我们其实也可以把这块做的很完善，可靠消息服务本身最重要的一点就是去重新投递消息 

（9）下游服务对MQ进行ack之后，再给可靠消息服务发送个请求，通知该服务说，ok，我这里处理完毕了，可靠消息服务收到通知之后，将消息的状态修改为“已完成” 

如果人家下游服务都ok了，MQ也ack了，结果下游服务发送给可靠消息服务通知，以及修改消息状态为“已完成”的时候，居然出错了，那么就麻烦了，无论是怎么回事，总之在可靠消息服务的数据库里，那个消息的状态是“已发送”，但是不是“已完成”！ 

这样整个系统状态是有问题的！ 

可靠消息服务的数据库里的消息的状态一直是“已发送”，所以这个状态是不对的

如果4和5两个步骤失败了，会呈现出的一个场景就是说，消息在可靠消息服务的数据库里的状态是“待确认”，一直是待确认，从来不会改变了 

（1）可靠消息服务得开一个后台线程，专门扫描那些数据库里处于“待确认”状态的消息，同时该消息的创建时间到现在已经超过了比如10分钟了，这个10分钟是你自己设定的一个超时阈值，一般来说，用个几分钟，或者10分钟都行 

（2）发现那种一直处于“待确认”状态的消息，还超过了一定的时间，就认为是超时了 

（3）对超时的消息，需要回调上游服务特意提供的查询这个操作状态的一个接口，然后上游服务自己判断一下这个操作是否执行，举个例子，比如说上游服务本来应该是将订单状态修改为“交易成功”的，此时回调过去以后，上游服务就得自己去判断下那个操作执行了没有，查下那个订单的状态，对吧

 （4）如果操作是还没执行，那么证明是3那个步骤失败了，就是上游服务的本地数据库操作失败了，所以导致消息一直处于“待确认”的状态，此时可靠消息服务需要将这条消息给删除即可 

（5）如果操作是已经执行了，那么说明是4或者5失败了，要不是上游服务没通知到可靠消息服务，要不是可靠消息服务自己没成功投递出去消息，此时可靠消息服务就是再次尝试用一个事务来更新本地消息状态为“已发送”，同时尝试再次发送消息给MQ 

通过上述机制，可以解决4和5失败的情况 

4和5失败结果是一样的，都是导致一条消息状态一直是“待确认” 

（1）如果是上游服务的本地数据库操作失败了，然后发送删除消息的通知给可靠消息服务，结果人家可靠消息服务的4和5失败了

（2）可靠消息服务后台线程过了一段时间，会扫描出来这条待确认的消息是超时了

（3）可靠消息服务会回调上游服务的一个接口，上游服务判断了一下发现说，当时步骤3的本地事务是失败的

（4）可靠消息服务得到了这个回调通知之后，就会将消息给删除 

如果步骤3是成功的 

（1）如果是上游服务的本地数据库操作成功了，然后发送确认消息的通知给可靠消息服务，结果人家可靠消息服务的4和5失败了

（2）可靠消息服务后台线程过了一段时间，会扫描出来这条待确认的消息是超时了

（3）可靠消息服务会回调上游服务的一个接口，上游服务判断了一下发现说，当时步骤3的本地事务是成功的

（4）可靠消息服务得到了这个回调通知之后，就会将消息状态再次改为“已发送”，同时将消息再次投递到MQ里去 

如果说可靠消息服务再次尝试删除消息，或者投递消息，还是失败了呢？因为消息的状态只要一直停留在“待确认”的状态，后台线程会不停的扫描到这条消息，然后再次发送请求去回调，然后再次尝试重新处理

所以6789出问题，其实消息的状态都是“已发送”，如果是8和9出问题，那么下游数据库的操作已经执行成功了 

（1）可以开一个后台线程，专门监控“已发送”的消息，如果超过了10分钟，那么就需要判定为超时了

（2）这个时候也是需要可靠消息服务再次重新投递消息给到MQ，让下游服务再次去消费

（3）下游服务的接口一定要保证幂等性，数据库操作只能成功的执行一次 

如果是6和7出问题，那么下游服务的本地数据库操作还没执行，此时重新投递了一条消息，那么正常情况下，就会执行数据库操作，反过来通知可靠消息服务变更消息状态为“已完成”，但是如果是8和9出问题，那么实际上此时下游服务的本地数据库操作已经执行成功了，只不过消息状态还是“已发送”而已 

所以此时如果你的可靠消息服务再次投递消息，导致下游服务再次执行了数据库操作，就会导致数据可能出现异常，所以此时需要依靠幂等性的保证，下游服务的本地数据库操作只能成功的执行一次，如果再次重复执行，要通过幂等性保证机制来实现 

但是这里有个问题啊，下游服务是可能接收到多次这个消息的，所以下游服务必须有一个幂等性的保证，保证他的本地数据库操作一旦成功之后，只能执行一次，第二次如果再来执行必须报错，要保证幂等性 

保证幂等性之后，我们可以想象一下，如果是678出了问题，然后反复重试了几次，接着可靠消息服务又重新投递了消息，那么下游服务有幂等性保证，所以就还好，不会造成数据错乱的 

那么如果是9出了问题呢，就是没有通知可靠消息服务更新消息的状态为“已完成”，那么通过重新投递，数据库操作不会再次执行了，但是反而会再次通知一次可靠消息服务，这个消息的状态是“已完成” 

6和7出问题，那么通过上面那套机制，就可以保证下游服务重新执行数据库操作，以及通知变更消息状态为“已完成”；如果是8和9出问题，那么可以通过上面那套机制，保证，数据库操作仅仅执行过一次，而且还重新通知消息状态变更为“已完成” 

通过这套机制，就可以保证整个可靠消息最终一致性，上游服务和下游服务的数据最终一定是一致的，说实在的，这个方案比TCC麻烦多了，要借助MQ什么的，还要考虑各种超时和重试 

而且你考虑一下，你有可能在一个10多个服务链式调用的核心链路场景下，单纯用这个方案来做分布式事务吗，那就成了每个服务都是发送消息出去了，极其的麻烦，麻烦的不行，让人痛苦想死，每个服务都要弄一个回调查询操作是否执行的接口之类的，还要跟MQ重度耦合 

而且从好好的同步服务链式调用，变成了MQ异步调用，时效性，问题排查都极为麻烦，所以这个方案，可靠消息最终一致性方案一般会跟TCC方案结合在一起，来实现复杂场景下的分布式事务 

在我们公司里实践的时候，一般都是会将TCC事务和可靠消息最终一致性方案结合起来使用，对于服务调用链的分布式事务，可以用tcc事务来保证，但是对于一些耗时可以做成异步化的服务的调用，同时也要包裹在事务里的话呢，可以使用可靠消息最终一致性的方案来做，基于MQ来做异步化，但是因为中间加了一个可靠消息服务，可以保证上游服务执行成功了以后，一定会保证下游服务也会执行成功

基于RocketMQ来实现这套方案了 

他的大概思路如下 

（1）上游服务发送一个prepare消息，可以认为是一个待确认消息到RocketMQ

（2）RocketMQ会在自己内部保存这条消息，然后返回一个状态给上游服务

（3）接着上游服务执行本地事务

（4）如果失败了就发送rollback给RocketMQ，RocketMQ会删除掉那条消息，如果成功就发送commit给RocketMQ

（5）rocketmq会根据状态来处理消息，如果是rollback就删除消息，如果是commit就将消息标识为可以被下游服务消费

（6）下游服务消息消息

（7）接着下游服务会消费这个消息，执行本地事务

（8）下游服务然后返回ack给rocketmq，如果消费失败，或者是本地事务执行失败，或者ack发送失败，那么rocketmq都会有自己的重试策略，重发消息

（9）rocketmq如果能够成功的收到ack消息就会将消息删除 

但是如果上游服务本地事务执行失败，也没发送rollback或者commit给rocketmq呢？那rocketmq会有异样的啊，一个检查消息状态超时的机制，发现消息超时了，就回调上游服务的一个接口，上游服务自己负责检查这个操作是否执行，如果没有就要执行，然后发送commit或者rollback给rocketmq；如果执行过了，那就执行发送commit给rocketmq好了，这个回调的机制是一样的 

那下游服务只要自己保证了幂等性就可以了

**1、下游服务的ack** 

其实用rabbitmq在这里还是比较简单的，主要就是用一个ack机制就好了，因为ack之后，就可以保证只有你处理完之后才会ack掉这条消息，然后的话呢，如果你没ack，rabbitmq会给你重新投递消息 

其他的没什么特殊的了 

如果是自动ack，那么rabbitmq只要一旦投递出去这个消息给消费者，立即就删除这条消息了，要是你消费者没处理成功，这条消息就丢了，这种的话主要是针对的那种对消息少量丢失不太敏感的，然后要求高吞吐量的场景 

但是可以使用手动ack的方式只有当你手动ack的时候，rabbitmq才会认为说你已经消费成功了，就会删除那条消息

**2、上游服务的confirm** 

有一种可能，在5这个步骤里，发送rabbitmq消息，看起来是成功了，其实消息并没有成功的投递到rabbitmq里去，是有这种可能的，同时你以为成功了，没有报错，消息可能在网络传递中丢掉了 

你本地的消息状态也变更为“已发送了”，结果消息并没有投递到mq里去，下游的6789四个步骤没有执行

操作状态回查以及待确认消息重投

这块的话呢，我们要用spring boot结合调度来做，我们要做一个后台自动调度每隔5分钟运行一次的一个后台线程，自动来扫描，启动调度相关的一些配置 

@EnableScheduling

@Component

@Scheduled(fixedRate = 1 * 60 * 1000) 

去查询处于待确认状态，同时创建时间超过10分钟的消息，认为是超时

tcc事务的底层源码和底层原理 

其实就是个上游服务，下游服务，以及一个最大努力通知服务，核心的点就在于这个重试的规则的制定，你要支持几种规则： 

（1）失败了之后，按照一定的间隔，连续重试指定的次数，比如失败之后，每隔5分钟重试一次，连续重试10次

（2）失败了之后，每次间隔都增加个5分钟，连续重试5次，举个例子啊，第一次重试是5分钟以后，第二次是10分钟以后，第三次是15分钟以后，第四次是20分钟以后，第五次是25分钟以后

为啥要支持上游服务自己在发送消息的时候定义重试规则呢？因为很简单啊，不同的业务场景可能重试的要求不同，得由上游服务来定义，有些人觉得应该这样，有些人觉得应该那样，那你自己定义不就得了 

最大努力通知服务，如果一次请求没成功，那么就将消息存到数据库里去，然后记录下来他的重试规则，以及上一次重试的时间，是第几次重试，然后搞一个后台线程不停的扫描，每次扫出来就根据规则去重新调用下游服务